---
title: 中文分词
date: 2018-5-17 17:40:01
tags: [NLP]
mathjax: true
categories: NLP
---
##基本思路

&emsp;&emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有三个要素：A是状态转移概率分布矩阵，简单说就是在任一时刻从一个隐含状态到另一个隐含状态的转移概率构成的矩阵；B是观测概率分布矩阵，其实就是在任一时刻给定隐含状态s生成观测状态o的条件概率$P(o|s)$构成的矩阵；$\pi$是初始概率矩阵，也就是在初始状态下各隐含状态的概率。而一般的HMM模型有三个基本问题：1. 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，计算$P(O|\lambda)$，这是评估问题。2. 给定观测序列$O = \{o_1, o_2, \dots, o_t\}$，求解模型$\lambda = (A, B, \pi$，使得$P(O|\lambda)$尽可能大，这是学习问题，若给定隐含状态序列S可以考虑用maximum likelihood来解决，若隐含状态序列则可以用Baum-Welch算法解决，不过这并不是本文重点。3. 给定 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，求使得$P(S|O)$最大的隐含状态序列$S = \{s_1, s_2, \dots, s_t\}$，这被称为解码问题或预测问题。对于分词这个任务来说，主要涉及到的是第三个问题。
&emsp;&emsp;jieba分词的源码就提供了解决这个问题的一个很好的范例。将隐含状态集合定义为$\{S, B, M, E\}$，S的含义是单字，B的含义是词头，M的含义是词中，E的含义是词尾。在```jieba/finaseg/prob_start.py```中定义了初始概率$\pi$，在```jieba/finaseg/prob_trans.py```中定义了状态转移概率$A$，在```jieba/finaseg/prob_emit.py```中定义了状态观测概率分布$B$，在用基于统计的方法获得以上这些之后就用Viterbi算法求一条使得$P(S|O)$最大的路径就好。（关于Viterbi算法还是在另篇文章中再说。）
&emsp;&emsp;考虑一下，此时如果我们不知道B，该如何定义要求解的函数。可以试着模仿Viterbi的想法，用$\delta_i(s)$表示到第i个字时状态为s时的最优值，则$\delta_{i+1}(s') = max\{\delta_i(s)a_{ss'}P(s|i),  s\in\{S, B, M, E\}\}$，其中$a_{ss'}$是转移概率，$P(s|i)$表示第i个字状态是s的概率（这样定义是有着一定数学原理的，具体推导也借鉴了Viterbi算法原本的定义，核心思想是极大似然）。转移概率可通过统计的方法得到，那么$P(s|i)$呢？影响到这个概率的因素很多，不妨将这个问题转化为一个seq2seq的问题，输入一个序列，输出各个位置的4-tag标注。中文中通过前后文语境都能作为序列标注的依据，从而考虑使用Bi-directional的LSTM来进行这个任务。只要将输出接一层softmax就可以将结果当作概率使用。
