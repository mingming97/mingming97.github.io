<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ljm&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://mingming97.github.io/"/>
  <updated>2018-06-12T11:39:56.652Z</updated>
  <id>https://mingming97.github.io/</id>
  
  <author>
    <name>ljm</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>word2vec理解</title>
    <link href="https://mingming97.github.io/2018/06/08/Word2Vec-basic/"/>
    <id>https://mingming97.github.io/2018/06/08/Word2Vec-basic/</id>
    <published>2018-06-08T02:14:01.000Z</published>
    <updated>2018-06-12T11:39:56.652Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文需要对n-gram模型在评估语句上的应用有一定了解（n-gram还有一些其他应用，如模糊匹配、特征工程等），可以看作对<a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="external">word2vec 中的数学原理详解</a>这篇博文的一些思考。</p>
<p>&emsp;&emsp;首先一定要建立起word2vec的基本认识，在NLP的一些任务中，我们需要将自然语言进行处理并发现其中的规律。但是机器没有办法直接理解自然语言，这就需要我们将其抽象化，换成一种机器能理解的表示方式。这就是word2vec的目的，将训练集中的词语表示成n维向量空间中的一个向量，而这个向量要对解决的问题有着尽可能好的特征表示。由此可以看出，word2vec与要解决的问题（也就是要训练的语言模型）是紧密相关的，所以一般情况下他们是一起进行训练的。例如keras中的Embedding层，官方解释是将字典下标转换为固定大小的向量。事实上这就是word2vec的一个过程，暂且不提keras是如何做的，要明确的一点是，Embedding层中的参数也是要随着训练的模型一起更新的，这有助于更好地理解之后的Skip-gram和CBOW模型。</p>
<h2 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h2><p>&emsp;&emsp;先看一下通用的语言模型，如下图所示。下面对这个图做出一些解释，这是一个三层语言模型，首先看输入，$w_{t-n+1},\dots,w_{t-2}, w_{t-1}$等是$w_t$之前的n-1个词，index的含义是他们在词典中的下标；再看输出，第i个输出是$P(w_t=i|context)$，其中context表示$w_t$的上下文，在这里表示输入的之前的n-1个词。所以这个模型就是计算了已知前n-1个词，而第n个词为i的条件概率。按照论文中的解释，这个网络有三层，如下：</p>
<ul>
<li><p>输入层（Input Layer）：负责将词向量输入，也就是$C(w_{t-n+1}),\dots,C(w_{t-2}),C(w_{t-1})$，每个用m维向量表示，共n-1个首尾相接，所以形成了一个m(n-1)维的向量。</p>
</li>
<li><p>隐含层（Hidden Layer）：设之前得到的m(n-1)维的向量为X，则隐含层的输出$Z = tanh(WX + p)$。其中W为该层权重，也就是要训练的参数，p是bias。输出使用tanh作为activation function，所以不用做smoothing。</p>
</li>
<li><p>输出层（Outpu Layer）：输出层的结点数与词典的大小相同，代表下一个词是词典中某一个词的概率。按照图中所说就是$O = softmax(UZ + q)$。其中U为该层权重，q是bias，softmax是activation function。</p>
</li>
</ul>
<p>  <img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/network.png" alt=""></p>
<p>  &emsp;&emsp;之后要介绍的网络都建立在这个三层模型的基础上，只是对一些流程做出了改动。</p>
<h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><p>&emsp;&emsp;这个模型要解决的问题其实就是在知道当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的情况下，预测$w_t$。也就是要计算$P(w_t|context(w_t))$。大体如下图所示：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/CBOW-Architecture.jpg" alt=""></p>
<p>&emsp;&emsp;对于其模型而言，去掉了之前的Hidden Layer，增加了一个Projection Layer，之前是将所有向量拼接，而现在是将所有向量求和，减少了参数数目，大体的模型如下。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/CBOWmodel.png" alt=""></p>
<ul>
<li><p>输入层：$Context(w)$中2c个词的词向量。</p>
</li>
<li><p>投影层：输入的2c个词向量的累加，即$x_w = \displaystyle\sum_{i=1}^{2c}v(Context(w)_i)$。</p>
</li>
<li><p>输出层：是一棵以词频为权值的Huffman树，这其中每一个叶子节点都代表一个单词，每一个非叶子节点都可以视作一次二分类。</p>
</li>
</ul>
<p>&emsp;&emsp;如果我们将其看作多分类问题，由于词典非常庞大，softmax的计算时间会线性增长，开销太大。所以此处采用了一种叫hierarchical softmax的技术，很形象，将输出层变成了一棵二叉树，并对每个非叶子节点做logisitic回归，目标节点就是我们训练集中的真实的词，也就是从根节点到目标层对应的路径就是每次分类正确的结果。</p>
<p>  &emsp;&emsp;如何更新参数呢。对于单个非叶子节点而言，它被分为正类的概率是$\sigma(x_w^T\theta)= \displaystyle\frac{1}{1+e^{-x_w^T\theta}}$，分为负类的概率是$1-\sigma(x_w^T\theta)$。在word2vec的源码中，将哈夫曼编码为1（向左走）视为负类，0（向右走）视为正类。不妨设到目标叶子节点共有s层，其中从根节点到目标叶子节点之前的非叶子节点的参数为$\theta_1,\theta_2,\dots,\theta_{s-1}$，目标叶子节点的哈夫曼编码为$d_2,d_3,\dots,d_s$，则可以得到</p>
<script type="math/tex; mode=display">
p(d_j|x_w, \theta_{j-1})=
\begin{cases}
\sigma(x_w^T\theta_{j-1}), &d_j = 0;\\
1-\sigma(x_w^T\theta_{j-1}), &d_j = 1,
\end{cases}</script><p>写成整体表达式就是</p>
<script type="math/tex; mode=display">
p(d_j|x_w, \theta_{j-1}) = \left[\sigma(x_w^T\theta_{j-1})\right]^{1-d_j}\cdot\left[1-\sigma(x_w^T\theta_{j-1})\right]^{d_j}</script><p>以上是一个结点的情况，如要求$p(w|Context(w))$将每个节点概率相乘即可，也就是</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\displaystyle\prod_{j=2}^{s}p(d_j|x_w, \theta_{j-1})</script><p>为了方便计算取对数，表示如下</p>
<script type="math/tex; mode=display">
\begin{align}
L &= \displaystyle\sum_{w\in C}log\prod_{j=2}^{s_w}\left[p(d_j^w|x_w, \theta_{j-1}^w)\right] \\
&=\displaystyle\sum_{w \in C}\sum_{j=2}^{s_w}\left\{(1-d_j^w)log\left[\sigma(x_w^T\theta_{j-1}^w)\right]+d_j^wlog\left[1-\sigma(x_w^T\theta_{j-1}^w)\right]\right\}
\end{align}</script><p>以上就是CBOW的目标函数，注意d和$\theta$都有上标的原因是表示他们是$w$相关的。由于我们的目标是将这个函数最大化，所以要用Gradient Ascent。对于我们的整个模型，需要训练的参数有$\theta_{j-1}^w,x_w$，分别对他们求梯度，由于是加法，又由于对一个固定的w和j，对$\theta_j^w$求偏导有贡献的项仅仅有花括号中的w和j相同的对应项，而对$x_w$而言有贡献的是$j\in[2, s_w]$部分的花括号中的内容的偏导和，为方便计算，将花括号中的内容记作T。</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial T}{\partial\theta_{j-1}^w} &= \displaystyle\frac{\partial}{\partial\theta_{j-1}^w}\left\{(1-d_j^w)log\left[\sigma(x_w^T\theta_{j-1}^w)\right]+d_j^wlog\left[1-\sigma(x_w^T\theta_{j-1}^w)\right]\right\}\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))\displaystyle\frac{\partial x_w^T\theta_{j-1}^w}{\partial\theta_{j-1}^w} + d_j^w\cdot(-\sigma(x_w^T\theta_{j-1}^w))\displaystyle\frac{\partial x_w^T\theta_{j-1}^w}{\partial\theta_{j-1}^w}\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))x_w - d_j^w\sigma(x_w^T\theta_{j-1}^w)x_w\ \ \ \ (矩阵求导分母布局)\\
&=\left[(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]x_w
\end{align}</script><p>对于$\displaystyle\frac{\partial T}{\partial x_w}$，与上面的过程类似，可以得到</p>
<script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial x_w} =\left[(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]\theta_{j-1}^w</script><p>显然，对于参数$\theta_{j-1}^w$的更新，很容易，只需要</p>
<script type="math/tex; mode=display">
\theta_{j-1}^w\leftarrow\theta_{j-1}^w+\eta\left[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]x_w</script><p>而对于$x_w$，它是通过v(w)累加来的，所以我们真正应该更新的是v(w)，word2vec的做法如下</p>
<script type="math/tex; mode=display">
v(w)\leftarrow v(w) + \eta\displaystyle\sum_{j=2}^{s_w}\left[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]\theta_{j-1}^w,\ \ \ \ w\in Context(w)</script><p>这样做当然有一定道理，因为我们仅仅求出了他们的一个集体梯度，所以要将这个集体梯度反馈到每一个个体上。</p>
<p>综上对于一个训练样本$(Context(w), w)$，CBOW伪代码如下</p>
<script type="math/tex; mode=display">
e\leftarrow0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ \ x_w\leftarrow\displaystyle\sum_{w\in Context(w)} v(w)\\
for \ j \leftarrow2\ to\ s_w\ do\\
\ \ q \leftarrow \sigma(x_w^T\theta_{j-1}^w)\\
\ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^w-q)\\
\ \ e\leftarrow e + g\theta_{j-1}^w\\
\ \ \ \ \ \ \ \ \ \theta_{j-1}^w\leftarrow \theta_{j-1}^w+gx_w\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ for \ u \in Context(w) do\\
\ \ \ \ \ \ v(u) \leftarrow v(u) + e</script><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><p>&emsp;&emsp;与CBOW相反，这个模型要解决的问题是在已知词$w_t$的情况下，预测词$w_{t-2}, w_{t-1},w_{t+1},w_{t+2}$。也就是要计算$P(Context(w_t)|w_t)$，如下图所示：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/Skip-Gram-Architecture-26.jpg" alt=""></p>
<p>&emsp;&emsp;对于其模型，与CBOW模型类似，为了与其对比，保留了projection layer，如下：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/SKIPGRAMmodel.png" alt=""></p>
<ul>
<li><p>输入层：词向量$v(w)$</p>
</li>
<li><p>投影层：为了与CBOW保持类似保留的，无实际作用。</p>
</li>
<li><p>输出层：一棵Huffman树。与CBOW类似，也是为了利用Hierarchical softmax，所输出的一棵以单词为叶子节点，非叶子节点作为二分类的单元的哈夫曼树。</p>
</li>
</ul>
<p>如何表示$P(Context(w)|w)$呢，可以将其拆解如下：</p>
<script type="math/tex; mode=display">
p(Context(w)|w) = \displaystyle\prod_{u \in Context(w)}p(u|w)</script><p>此处的$p(u|w)$可以借助CBOW的表示方式，表示为：</p>
<script type="math/tex; mode=display">
p(u|w) = \displaystyle\prod_{j=2}^{s}p(d_j|v_w, \theta_{j-1})</script><p>此处的$\theta,d_j$在总体表示中，同样是与语料中的$u$是相关的，将p写作二分类的形式后再取对数（与CBOW中操作相似），得到总体的目标函数</p>
<script type="math/tex; mode=display">
\begin{align}
L&= \displaystyle\sum_{w\in C}log\prod_{u\in Context(w)}\prod_{j=2}^{s_u}\left[p(d_j^u|v_w,\theta_{j-1}^u)\right]\\
&= \displaystyle\sum_{w \in C}\sum_{u\in Context(w)}\sum_{j=2}^{s_u} \left\{(1-d_j^u)log\left[\sigma(v_w^T\theta_{j-1}^u)\right]+d_j^ulog\left[1-\sigma(v_w^T\theta_{j-1}^u)\right]\right\}
\end{align}</script><p>出于与CBOW中同样的考虑，可将后面花括号中的内容记为T，与CBOW中推导过程完全相同，得到下面的结果：</p>
<script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial\theta_{j-1}^u} =\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]v_w</script><script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial v_w}=\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]\theta_{j-1}^u</script><p> 参数更新也是类似的（此处也是要求目标函数最大值）</p>
<script type="math/tex; mode=display">
\theta_{j-1}^u \leftarrow \theta_{j-1}^u + \eta\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]v_w</script><script type="math/tex; mode=display">
v_w\leftarrow v_w + \eta\displaystyle\sum_{u\in Context(w)}\sum_{j=2}^{s_u}\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]\theta_{j-1}^u</script><p>综上对于一个训练样本$(w, Context(w))$，Skip-gram伪代码如下</p>
<script type="math/tex; mode=display">
e\leftarrow 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
for\ u \in Context(w)\ do \\
for \ j \leftarrow2\ to\ s_u\ do\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q \leftarrow \sigma\left[v(w)^T\theta_{j-1}^u\right]\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^u-q)\\
\ \ \ \ \ \ \ \ e\leftarrow e + g\theta_{j-1}^u\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta_{j-1}^u\leftarrow \theta_{j-1}^u+gv(w)\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
v(w) \leftarrow v(w)+e\ \ \ \ \ \ \ \ \ \ \ \</script><p>但是在word2vec中，真实的做法是每处理一个$Context(w)$中的词，就进行一次更新，也就是如下的步骤：</p>
<script type="math/tex; mode=display">
for\ u \in Context(w)\ do \\
e\leftarrow 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
for \ j \leftarrow2\ to\ s_u\ do\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q \leftarrow \sigma\left[v(w)^T\theta_{j-1}^u\right]\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^u-q)\\
\ \ \ \ \ \ \ \ e\leftarrow e + g\theta_{j-1}^u\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta_{j-1}^u\leftarrow \theta_{j-1}^u+gv(w)\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
v(w) \leftarrow v(w)+e\ \ \ \\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\</script><p>&emsp;&emsp;以上两种就是基于Hierarchical softmax的CBOW和Skip-gram模型，还有基于Negative Sampling的两种模型，在这里暂且不提。如果想得知更多细节，可以在源码中获得，我个人只看过java版本的，没看过纯C版本的，尽管语言不同，但我想在一些处理细节上是相同的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文需要对n-gram模型在评估语句上的应用有一定了解（n-gram还有一些其他应用，如模糊匹配、特征工程等），可以看作对&lt;a href=&quot;https://blog.csdn.net/itplus/article/details/37969519&quot; 
    
    </summary>
    
      <category term="NLP" scheme="https://mingming97.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://mingming97.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>中文分词</title>
    <link href="https://mingming97.github.io/2018/05/17/chinese-split-word/"/>
    <id>https://mingming97.github.io/2018/05/17/chinese-split-word/</id>
    <published>2018-05-17T09:40:01.000Z</published>
    <updated>2018-05-18T09:51:04.149Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p>&emsp;&emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有三个要素：A是状态转移概率分布矩阵，简单说就是在任一时刻从一个隐含状态到另一个隐含状态的转移概率构成的矩阵；B是观测概率分布矩阵，其实就是在任一时刻给定隐含状态s生成观测状态o的条件概率$P(o|s)$构成的矩阵；$\pi$是初始概率矩阵，也就是在初始状态下各隐含状态的概率。而一般的HMM模型有三个基本问题：1. 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，计算$P(O|\lambda)$，这是评估问题。2. 给定观测序列$O = \{o_1, o_2, \dots, o_t\}$，求解模型$\lambda = (A, B, \pi$，使得$P(O|\lambda)$尽可能大，这是学习问题，若给定隐含状态序列S可以考虑用maximum likelihood来解决，若隐含状态序列则可以用Baum-Welch算法解决，不过这并不是本文重点。3. 给定 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，求使得$P(S|O)$最大的隐含状态序列$S = \{s_1, s_2, \dots, s_t\}$，这被称为解码问题或预测问题。对于分词这个任务来说，主要涉及到的是第三个问题。<br>&emsp;&emsp;jieba分词的源码就提供了解决这个问题的一个很好的范例。将隐含状态集合定义为$\{S, B, M, E\}$，S的含义是单字，B的含义是词头，M的含义是词中，E的含义是词尾。在 <code>jieba/finaseg/prob_start.py</code>中定义了初始概率$\pi$，在 <code>jieba/finaseg/prob_trans.py</code>中定义了状态转移概率$A$，在<code>jieba/finaseg/prob_emit.py</code>中定义了状态观测概率分布$B$，在用基于统计的方法获得以上这些之后就用Viterbi算法求一条使得$P(S|O)$最大的路径就好。（关于Viterbi算法还是在另篇文章中再说。）<br>&emsp;&emsp;考虑一下，此时如果我们不知道B，该如何定义要求解的函数。可以试着模仿Viterbi的想法，用$\delta_i(s)$表示到第i个字时状态为s时的最优值，则$\delta_{i+1}(s’) = max\{\delta_i(s)a_{ss’}P(s|i),  s\in\{S, B, M, E\}\}$，其中$a_{ss’}$是转移概率，$P(s|i)$表示第i个字状态是s的概率（这样定义是有着一定数学原理的，具体推导也借鉴了Viterbi算法原本的定义，核心思想是极大似然）。转移概率可通过统计的方法得到，那么$P(s|i)$呢？影响到这个概率的因素很多，不妨将这个问题转化为一个seq2seq的问题，输入一个序列，输出各个位置的4-tag标注。中文中通过前后文语境都能作为序列标注的依据，从而考虑使用Bi-directional的LSTM来进行这个任务。只要将输出接一层softmax就可以将结果当作概率使用。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>&emsp;&emsp;有一个经典的亚研院的语料库就是4-tag标注的，大概长这个样子。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/msrtrain.png" alt=""></p>
<p>&emsp;&emsp;首先要明确我们训练数据的lstm的输入，应该是batch_size * sentence_len的一个tensor。以标点符号为分隔，汉语中单句话一般没有太长，所以此处统一每个句向量的长度为32。获得句向量的方式很简单，将文章中所有出现的字做成一个字典，将每个字用其在字典中对应的下位置表示，不足的长度用0补齐，就得到了句向量的表示。而对于tag，则可以使用one-hot的编码，注意要为补足句向量的0留一个编码位置，所以一共有5类tag。因此一句话中tag为s的字将被编码为[1, 0, 0, 0, 0]，tag为b的将被编码为[0, 1, 0, 0, 0]，以此类推。<br>&emsp;&emsp;明确了以上过程，就可以开始细节上的处理。注意到每行前面有很多“/s，”/s，‘/s，’/s一类的东西，我们的数据是不需要这类东西的，可以用正则处理掉。处理掉之后单看每行，可以方便的使用python的re.findall提取出来一个字和标签的元组组成的列表。再将二者分别处理，分别得到句子和标签。<br>&emsp;&emsp;通过标签得到独热编码的过程值得记录一下。面对的问题就是给出了一个类别的列表，如何得到独热编码。可以利用numpy的花式索引方便进行。代码如下：（虽然花式索引返回的是数据的拷贝，但是使用花式索引进行赋值却是在原数组上进行操作的）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">data = np.array(data)	<span class="comment"># 获得numpy的数组</span></div><div class="line">one_hot = np.zeros((len(data), <span class="number">5</span>), dtype=np.float64)	<span class="comment"># 获得len(data)个长度为5的0向量</span></div><div class="line">one_hot[range(len(data)), data] = <span class="number">1</span>		<span class="comment"># 二维花式索引 可以实现任意位置的操作</span></div><div class="line">default = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>], dtype=np.float64)	<span class="comment"># 定义默认向量</span></div><div class="line">default = np.tile(default, (<span class="number">32</span> - len(one_hot), <span class="number">1</span>))	<span class="comment"># 将默认向量在列方向上重复32-len(one_hot)次</span></div><div class="line">one_hot = np.concatenate((one_hot, default), axis = <span class="number">0</span>)	<span class="comment"># 将默认部分与有效部分拼接保证长度与句向量相同</span></div></pre></td></tr></table></figure></p>
<h2 id="模型定义及训练"><a href="#模型定义及训练" class="headerlink" title="模型定义及训练"></a>模型定义及训练</h2><p>&emsp;&emsp;接下类就是定义一个双向LSTM的过程，为了简单起见，使用了keras进行，keras中还有embedding层，正好适用于我们生成的句向量。keras封装度比较高，所以代码较短，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">inputs = Input(shape=(sent_len,), dtype=<span class="string">'int32'</span>)</div><div class="line">embedding = Embedding(len(charsets) + <span class="number">1</span>, word_size, input_length=sent_len, mask_zero=<span class="keyword">True</span>)(inputs)</div><div class="line">bilstm_layer = Bidirectional(LSTM(<span class="number">64</span>, return_sequences=<span class="keyword">True</span>), merge_mode=<span class="string">'sum'</span>)(embedding)</div><div class="line">output = TimeDistributed(Dense(<span class="number">5</span>, activation=<span class="string">'softmax'</span>))(bilstm_layer)</div><div class="line">model = Model(input=inputs, output=output) model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">json.dump(model.to_json(), open(os.path.join(filedir, model_name), <span class="string">"w"</span>))</div><div class="line">model.fit(x, y, batch_size=batch_size, epochs=epochs)</div><div class="line">model.save_weights(os.path.join(filedir, model_weights_name))</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;训练好了模型，输入一句话，就能给出各个位置对应各种标签的概率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sentence_embeddings, all_sentences, sentence_len = data.get_sent_embeddings(charsets, test_sentence)</div><div class="line">result = model.predict(sentence_embeddings, verbose=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<h2 id="计算概率最大路径"><a href="#计算概率最大路径" class="headerlink" title="计算概率最大路径"></a>计算概率最大路径</h2><p>&emsp;&emsp;根据之前对$\delta_i(s)$的定义，使用dp就可以计算概率最大的路径了。此处转移概率用的是相等概率，dp算法的具体实现上利用了python字典的性质，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(nodes)</span>:</span></div><div class="line">    path = &#123;<span class="string">'b'</span>: nodes[<span class="number">0</span>][<span class="string">'b'</span>], <span class="string">'s'</span>: nodes[<span class="number">0</span>][<span class="string">'s'</span>]&#125;</div><div class="line">    <span class="keyword">for</span> layer_num <span class="keyword">in</span> range(<span class="number">1</span>, len(nodes)):</div><div class="line">        old_path = path.copy()</div><div class="line">        path = &#123;&#125;</div><div class="line">        <span class="keyword">for</span> new_tag <span class="keyword">in</span> nodes[layer_num].keys():</div><div class="line">            tmp = &#123;&#125;</div><div class="line">            <span class="keyword">if</span> layer_num == len(nodes) - <span class="number">1</span>:</div><div class="line">                <span class="keyword">if</span> new_tag <span class="keyword">in</span> [<span class="string">"m"</span>, <span class="string">"b"</span>]:</div><div class="line">                    <span class="keyword">continue</span></div><div class="line">            <span class="keyword">for</span> old_path_tag <span class="keyword">in</span> old_path.keys():</div><div class="line">                <span class="keyword">if</span> old_path_tag[<span class="number">-1</span>]+new_tag <span class="keyword">in</span> transpose_matrix.keys():</div><div class="line">                    tmp[old_path_tag+new_tag] = old_path[old_path_tag] + \</div><div class="line">                                                nodes[layer_num][new_tag] + \</div><div class="line">                                                transpose_matrix[old_path_tag[<span class="number">-1</span>]+new_tag]</div><div class="line">            k = np.argmax(list(tmp.values()))</div><div class="line">            path[list(tmp.keys())[k]] = list(tmp.values())[k]</div><div class="line">    <span class="keyword">return</span> list(path.keys())[np.argmax(list(path.values()))]</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过以上几个步骤，就完成了对一句话4-tag的标注。自己试了几句话，效果还不错，见图：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/result1.png" alt=""></p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/result2.png" alt=""></p>
<p>完整代码可以参照我的<a href="https://github.com/mingming97/Chinese-Word-Split" target="_blank" rel="external">Github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本思路&quot;&gt;&lt;a href=&quot;#基本思路&quot; class=&quot;headerlink&quot; title=&quot;基本思路&quot;&gt;&lt;/a&gt;基本思路&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有
    
    </summary>
    
      <category term="NLP" scheme="https://mingming97.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://mingming97.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>hexo配置LaTeX公式</title>
    <link href="https://mingming97.github.io/2018/05/15/hexo-latex/"/>
    <id>https://mingming97.github.io/2018/05/15/hexo-latex/</id>
    <published>2018-05-15T13:12:18.000Z</published>
    <updated>2018-05-18T10:05:51.895Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎 <code>hexo-renderer-marked</code>更换为<code>hexo-renderer-kramed</code>来渲染markdown。<br>&emsp;&emsp;首先要将之前的<code>hexo-renderer-marked</code>卸载，并安装<code>hexo-renderer-kramed</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm uninstall hexo-renderer-marked --save</div><div class="line">npm install hexo-renderer-kramed --save</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;在这之后建议在hexo的根目录下找到<code>package.json</code>文件，用文本编辑器打开它，删除字符串<code>hexo-renderer-marked</code>所在的一行并保存。之所以不直接卸载<code>hexo-renderer-marked</code>，是因为其他重要包极有可能在卸载该包的同时被删除。<br>&emsp;&emsp;不要忘了行内公式的转义字符，打开<code>./node_modules/kramed/lib/rules</code>，并修改<code>inline.js</code>文件的11和20行，分别修改为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</div><div class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;每次在写文章前，要在YAML font-matter中添加<code>mathjax: true</code>，这样便能确保启动mathjax引擎进行渲染了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎 &lt;code&gt;hexo-renderer-marked&lt;/code&gt;更换为&lt;code&gt;hexo-renderer-kramed&lt;/code&gt;来渲染markdown。&lt;
    
    </summary>
    
      <category term="hexo" scheme="https://mingming97.github.io/categories/hexo/"/>
    
    
      <category term="LaTeX" scheme="https://mingming97.github.io/tags/LaTeX/"/>
    
      <category term="hexo" scheme="https://mingming97.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>LIS的O(nlogn)实现原理</title>
    <link href="https://mingming97.github.io/2017/08/30/LIS-O-nlogn/"/>
    <id>https://mingming97.github.io/2017/08/30/LIS-O-nlogn/</id>
    <published>2017-08-30T08:07:39.000Z</published>
    <updated>2018-05-17T16:08:41.703Z</updated>
    
    <content type="html"><![CDATA[<p>　　朴素的LIS的<strong>O(n²)</strong>算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为<strong>d[i] = max{d[j] | j &lt; i &amp;&amp; a[j] &lt; a[i]} + 1</strong>。显然，对于一个i下的两个不同决策<strong>j，k (j,k &lt; i)，若a[j] &lt; a[k]，d[j] &gt;= d[k]，而a[i] &gt; a[k]时，k显然没有j决策更优</strong>。通过这种思想，我们发现决策有一定的单调性，从而其中可以用二分查找来降低时间复杂度。</p>
<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>　　我们可以维护一个决策序列B，用<strong>B[i]</strong>表示子序列长度为i时的最小末尾(a中的值)。可以看出如果<strong>i &gt; j</strong>，一定有<strong>B[i] &gt;= B[j]</strong>。原因很简单，看B的定义就可以知道。由上可知B是单调的序列，对于其中的的值可以进行二分查找。</p>
<p>　　所以可以按顺序枚举<strong>a[i]</strong>，如果<strong>a[i]</strong>的值比B中的最大值要大，则将<strong>a[i]</strong>放入<strong>B[i]</strong>的尾部，最大长度加1; 否则对B进行二分查找以找到<strong>a[i]</strong>该放入的位置，<strong>pos = min{j | B[j] &gt; a[i]}</strong>，用a[i]代替B[pos]，使a[i]成为子序列长度为pos时的最小值。大致过程如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (a[i] &gt; B[len<span class="number">-1</span>]) &#123;</div><div class="line">        B[len] = a[i];</div><div class="line">        len++;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">        pos = binarySearch(len, a[i]);</div><div class="line">        B[pos] = a[i];</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>完整代码请戳：<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/LIS-nlogn.cpp" target="_blank" rel="external">ljm’s github</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　朴素的LIS的&lt;strong&gt;O(n²)&lt;/strong&gt;算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为&lt;strong&gt;d[i] = max{d[j] | j &amp;lt; i &amp;amp;&amp;amp; a[j] &amp;lt; a[i]} +
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="dp" scheme="https://mingming97.github.io/tags/dp/"/>
    
  </entry>
  
  <entry>
    <title>并查集</title>
    <link href="https://mingming97.github.io/2017/08/28/Union-Find/"/>
    <id>https://mingming97.github.io/2017/08/28/Union-Find/</id>
    <published>2017-08-28T09:41:39.000Z</published>
    <updated>2018-05-17T16:08:39.771Z</updated>
    
    <content type="html"><![CDATA[<p>　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最快呢。并查集是一种轻量级的数据结构，其本质是一种集合，支持不相交集合的“并”和“查”的操作。由于在特定状况下，可以通过某些特殊处理将这两种操作的时间复杂度降至很低，所以在某些算法，例如tarjan求lca的实现中，也引入了这一数据结构。</p>
<h2 id="基本实现"><a href="#基本实现" class="headerlink" title="基本实现"></a>基本实现</h2><p>　　其实并查集的本质就是一个划分，商集中的每一个元素（集合）内的元素都可以建一棵树，放在一起就变成了森林。对于任意两个个元素，只需要看他们的树根是否相等，就可以判断他们是否属于同一集合；至于两个集合的合并，只需要将一棵树的树根指向另一棵树的树根。那么，可以用<strong>pre[i]</strong>记录i这个结点的父节点，初始化时只需令<strong>pre[i] = i</strong>，意味着每一个结点都属于一个单独的集合，也就是每一个节点都是树根。如果想查找树根怎么办呢？ 按照定义<strong>pre[i]</strong>是i的父亲，<strong>pre[pre[i]]</strong>就是i结点父亲的父亲，按照这个方式不断向上找直至<strong>pre[i]</strong>等于i，就找到了树根，可以使用递归来实现。如果想合并两个集合怎么办呢，只需要找到两个元素所在树的树根，将一个树根指向另一个，就完成了。</p>
<h2 id="时间复杂度优化"><a href="#时间复杂度优化" class="headerlink" title="时间复杂度优化"></a>时间复杂度优化</h2><h3 id="启发式合并"><a href="#启发式合并" class="headerlink" title="启发式合并"></a>启发式合并</h3><p>　　为了解决合并时树退化成链的情况，在合并时我们可以根据两棵树的深度合并，将最大深度小的向最大深度大的合并。如果两棵树的深度一样，则随便选一个作为根，并将根的最大深度+1。这样做的话在n次操作后，任何一棵树（一个集合）的深度最大不会超过<strong>[logn]+1</strong>，从而使查找的时间复杂度降为$O(logn)$。代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> rx = find(x), ry = find(y);</div><div class="line">    <span class="keyword">if</span> (rx != ry) &#123;</div><div class="line">        <span class="keyword">if</span> (rank[rx] == rank[ry]) &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">            rank[rx]++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (rank[rx] &lt; rank[ry]) &#123;</div><div class="line">            pre[rx] = ry;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="路径压缩"><a href="#路径压缩" class="headerlink" title="路径压缩"></a>路径压缩</h3><p>　　一般情况下我们只需要知道知道一个元素所在的树的根就可以，所以可以在查找元素的过程中，把路径上的所有子节点直接指向根节点，这样就可以将查找的复杂度降至O(1)。代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> x == pre[x] ? x : pre[x] = find(pre[x]);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="应用时的问题"><a href="#应用时的问题" class="headerlink" title="应用时的问题"></a>应用时的问题</h2><p>　　在具体问题中，两棵树是否可以按秩合并，以及是否可以路径压缩是需要按照题的要求来定的，本文只是提出了大体框架及基本实现（可移步<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/union-find.cpp" target="_blank" rel="external">我的github</a>查看）。具体节点信息的维护或是节点合并顺序等还需要自己判断。
　  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="并查集" scheme="https://mingming97.github.io/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/"/>
    
      <category term="数据结构" scheme="https://mingming97.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
