<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ljm&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://mingming97.github.io/"/>
  <updated>2018-05-18T09:51:04.149Z</updated>
  <id>https://mingming97.github.io/</id>
  
  <author>
    <name>ljm</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>中文分词</title>
    <link href="https://mingming97.github.io/2018/05/17/chinese-split-word/"/>
    <id>https://mingming97.github.io/2018/05/17/chinese-split-word/</id>
    <published>2018-05-17T09:40:01.000Z</published>
    <updated>2018-05-18T09:51:04.149Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p>&emsp;&emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有三个要素：A是状态转移概率分布矩阵，简单说就是在任一时刻从一个隐含状态到另一个隐含状态的转移概率构成的矩阵；B是观测概率分布矩阵，其实就是在任一时刻给定隐含状态s生成观测状态o的条件概率$P(o|s)$构成的矩阵；$\pi$是初始概率矩阵，也就是在初始状态下各隐含状态的概率。而一般的HMM模型有三个基本问题：1. 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，计算$P(O|\lambda)$，这是评估问题。2. 给定观测序列$O = \{o_1, o_2, \dots, o_t\}$，求解模型$\lambda = (A, B, \pi$，使得$P(O|\lambda)$尽可能大，这是学习问题，若给定隐含状态序列S可以考虑用maximum likelihood来解决，若隐含状态序列则可以用Baum-Welch算法解决，不过这并不是本文重点。3. 给定 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，求使得$P(S|O)$最大的隐含状态序列$S = \{s_1, s_2, \dots, s_t\}$，这被称为解码问题或预测问题。对于分词这个任务来说，主要涉及到的是第三个问题。<br>&emsp;&emsp;jieba分词的源码就提供了解决这个问题的一个很好的范例。将隐含状态集合定义为$\{S, B, M, E\}$，S的含义是单字，B的含义是词头，M的含义是词中，E的含义是词尾。在 <code>jieba/finaseg/prob_start.py</code>中定义了初始概率$\pi$，在 <code>jieba/finaseg/prob_trans.py</code>中定义了状态转移概率$A$，在<code>jieba/finaseg/prob_emit.py</code>中定义了状态观测概率分布$B$，在用基于统计的方法获得以上这些之后就用Viterbi算法求一条使得$P(S|O)$最大的路径就好。（关于Viterbi算法还是在另篇文章中再说。）<br>&emsp;&emsp;考虑一下，此时如果我们不知道B，该如何定义要求解的函数。可以试着模仿Viterbi的想法，用$\delta_i(s)$表示到第i个字时状态为s时的最优值，则$\delta_{i+1}(s’) = max\{\delta_i(s)a_{ss’}P(s|i),  s\in\{S, B, M, E\}\}$，其中$a_{ss’}$是转移概率，$P(s|i)$表示第i个字状态是s的概率（这样定义是有着一定数学原理的，具体推导也借鉴了Viterbi算法原本的定义，核心思想是极大似然）。转移概率可通过统计的方法得到，那么$P(s|i)$呢？影响到这个概率的因素很多，不妨将这个问题转化为一个seq2seq的问题，输入一个序列，输出各个位置的4-tag标注。中文中通过前后文语境都能作为序列标注的依据，从而考虑使用Bi-directional的LSTM来进行这个任务。只要将输出接一层softmax就可以将结果当作概率使用。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>&emsp;&emsp;有一个经典的亚研院的语料库就是4-tag标注的，大概长这个样子。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/msrtrain.png" alt=""></p>
<p>&emsp;&emsp;首先要明确我们训练数据的lstm的输入，应该是batch_size * sentence_len的一个tensor。以标点符号为分隔，汉语中单句话一般没有太长，所以此处统一每个句向量的长度为32。获得句向量的方式很简单，将文章中所有出现的字做成一个字典，将每个字用其在字典中对应的下位置表示，不足的长度用0补齐，就得到了句向量的表示。而对于tag，则可以使用one-hot的编码，注意要为补足句向量的0留一个编码位置，所以一共有5类tag。因此一句话中tag为s的字将被编码为[1, 0, 0, 0, 0]，tag为b的将被编码为[0, 1, 0, 0, 0]，以此类推。<br>&emsp;&emsp;明确了以上过程，就可以开始细节上的处理。注意到每行前面有很多“/s，”/s，‘/s，’/s一类的东西，我们的数据是不需要这类东西的，可以用正则处理掉。处理掉之后单看每行，可以方便的使用python的re.findall提取出来一个字和标签的元组组成的列表。再将二者分别处理，分别得到句子和标签。<br>&emsp;&emsp;通过标签得到独热编码的过程值得记录一下。面对的问题就是给出了一个类别的列表，如何得到独热编码。可以利用numpy的花式索引方便进行。代码如下：（虽然花式索引返回的是数据的拷贝，但是使用花式索引进行赋值却是在原数组上进行操作的）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">data = np.array(data)	<span class="comment"># 获得numpy的数组</span></div><div class="line">one_hot = np.zeros((len(data), <span class="number">5</span>), dtype=np.float64)	<span class="comment"># 获得len(data)个长度为5的0向量</span></div><div class="line">one_hot[range(len(data)), data] = <span class="number">1</span>		<span class="comment"># 二维花式索引 可以实现任意位置的操作</span></div><div class="line">default = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>], dtype=np.float64)	<span class="comment"># 定义默认向量</span></div><div class="line">default = np.tile(default, (<span class="number">32</span> - len(one_hot), <span class="number">1</span>))	<span class="comment"># 将默认向量在列方向上重复32-len(one_hot)次</span></div><div class="line">one_hot = np.concatenate((one_hot, default), axis = <span class="number">0</span>)	<span class="comment"># 将默认部分与有效部分拼接保证长度与句向量相同</span></div></pre></td></tr></table></figure></p>
<h2 id="模型定义及训练"><a href="#模型定义及训练" class="headerlink" title="模型定义及训练"></a>模型定义及训练</h2><p>&emsp;&emsp;接下类就是定义一个双向LSTM的过程，为了简单起见，使用了keras进行，keras中还有embedding层，正好适用于我们生成的句向量。keras封装度比较高，所以代码较短，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">inputs = Input(shape=(sent_len,), dtype=<span class="string">'int32'</span>)</div><div class="line">embedding = Embedding(len(charsets) + <span class="number">1</span>, word_size, input_length=sent_len, mask_zero=<span class="keyword">True</span>)(inputs)</div><div class="line">bilstm_layer = Bidirectional(LSTM(<span class="number">64</span>, return_sequences=<span class="keyword">True</span>), merge_mode=<span class="string">'sum'</span>)(embedding)</div><div class="line">output = TimeDistributed(Dense(<span class="number">5</span>, activation=<span class="string">'softmax'</span>))(bilstm_layer)</div><div class="line">model = Model(input=inputs, output=output) model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">json.dump(model.to_json(), open(os.path.join(filedir, model_name), <span class="string">"w"</span>))</div><div class="line">model.fit(x, y, batch_size=batch_size, epochs=epochs)</div><div class="line">model.save_weights(os.path.join(filedir, model_weights_name))</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;训练好了模型，输入一句话，就能给出各个位置对应各种标签的概率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sentence_embeddings, all_sentences, sentence_len = data.get_sent_embeddings(charsets, test_sentence)</div><div class="line">result = model.predict(sentence_embeddings, verbose=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<h2 id="计算概率最大路径"><a href="#计算概率最大路径" class="headerlink" title="计算概率最大路径"></a>计算概率最大路径</h2><p>&emsp;&emsp;根据之前对$\delta_i(s)$的定义，使用dp就可以计算概率最大的路径了。此处转移概率用的是相等概率，dp算法的具体实现上利用了python字典的性质，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(nodes)</span>:</span></div><div class="line">    path = &#123;<span class="string">'b'</span>: nodes[<span class="number">0</span>][<span class="string">'b'</span>], <span class="string">'s'</span>: nodes[<span class="number">0</span>][<span class="string">'s'</span>]&#125;</div><div class="line">    <span class="keyword">for</span> layer_num <span class="keyword">in</span> range(<span class="number">1</span>, len(nodes)):</div><div class="line">        old_path = path.copy()</div><div class="line">        path = &#123;&#125;</div><div class="line">        <span class="keyword">for</span> new_tag <span class="keyword">in</span> nodes[layer_num].keys():</div><div class="line">            tmp = &#123;&#125;</div><div class="line">            <span class="keyword">if</span> layer_num == len(nodes) - <span class="number">1</span>:</div><div class="line">                <span class="keyword">if</span> new_tag <span class="keyword">in</span> [<span class="string">"m"</span>, <span class="string">"b"</span>]:</div><div class="line">                    <span class="keyword">continue</span></div><div class="line">            <span class="keyword">for</span> old_path_tag <span class="keyword">in</span> old_path.keys():</div><div class="line">                <span class="keyword">if</span> old_path_tag[<span class="number">-1</span>]+new_tag <span class="keyword">in</span> transpose_matrix.keys():</div><div class="line">                    tmp[old_path_tag+new_tag] = old_path[old_path_tag] + \</div><div class="line">                                                nodes[layer_num][new_tag] + \</div><div class="line">                                                transpose_matrix[old_path_tag[<span class="number">-1</span>]+new_tag]</div><div class="line">            k = np.argmax(list(tmp.values()))</div><div class="line">            path[list(tmp.keys())[k]] = list(tmp.values())[k]</div><div class="line">    <span class="keyword">return</span> list(path.keys())[np.argmax(list(path.values()))]</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过以上几个步骤，就完成了对一句话4-tag的标注。自己试了几句话，效果还不错，见图：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/result1.png" alt=""></p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/result2.png" alt=""></p>
<p>完整代码可以参照我的<a href="https://github.com/mingming97/Chinese-Word-Split" target="_blank" rel="external">Github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本思路&quot;&gt;&lt;a href=&quot;#基本思路&quot; class=&quot;headerlink&quot; title=&quot;基本思路&quot;&gt;&lt;/a&gt;基本思路&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有
    
    </summary>
    
      <category term="NLP" scheme="https://mingming97.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://mingming97.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>hexo配置LaTeX公式</title>
    <link href="https://mingming97.github.io/2018/05/15/hexo-latex/"/>
    <id>https://mingming97.github.io/2018/05/15/hexo-latex/</id>
    <published>2018-05-15T13:12:18.000Z</published>
    <updated>2018-05-17T16:15:55.269Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&amp;emsp;&amp;emsp;首先要将之前的```hexo-renderer-marked```卸载，并安装```hexo-renderer-kramed```。</div></pre></td></tr></table></figure></p>
<p>npm uninstall hexo-renderer-marked —save<br>npm install hexo-renderer-kramed —save<br><code>&amp;emsp;&amp;emsp;在这之后建议在hexo的根目录下找到</code>package.json<code>文件，用文本编辑器打开它，删除字符串`hexo-renderer-marked`所在的一行并保存。之所以不直接卸载`hexo-renderer-marked`，是因为其他重要包极有可能在卸载该包的同时被删除。 
&amp;emsp;&amp;emsp;不要忘了行内公式的转义字符，打开</code>./node_modules/kramed/lib/rules<code>，并修改</code>inline.js<code>文件的11和20行，分别修改为</code>escape: /^\([`<em>[]()#$+-.!_&gt;])/,<code>和</code>em: /^\</em>((?:**|[\s\S])+?)*(?!*)/,<code>。
&amp;emsp;&amp;emsp;每次在写文章前，要在YAML font-matter中添加</code>mathjax: true```，这样便能确保启动mathjax引擎进行渲染了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line
    
    </summary>
    
      <category term="hexo" scheme="https://mingming97.github.io/categories/hexo/"/>
    
    
      <category term="LaTeX" scheme="https://mingming97.github.io/tags/LaTeX/"/>
    
      <category term="hexo" scheme="https://mingming97.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>LIS的O(nlogn)实现原理</title>
    <link href="https://mingming97.github.io/2017/08/30/LIS-O-nlogn/"/>
    <id>https://mingming97.github.io/2017/08/30/LIS-O-nlogn/</id>
    <published>2017-08-30T08:07:39.000Z</published>
    <updated>2018-05-17T16:08:41.703Z</updated>
    
    <content type="html"><![CDATA[<p>　　朴素的LIS的<strong>O(n²)</strong>算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为<strong>d[i] = max{d[j] | j &lt; i &amp;&amp; a[j] &lt; a[i]} + 1</strong>。显然，对于一个i下的两个不同决策<strong>j，k (j,k &lt; i)，若a[j] &lt; a[k]，d[j] &gt;= d[k]，而a[i] &gt; a[k]时，k显然没有j决策更优</strong>。通过这种思想，我们发现决策有一定的单调性，从而其中可以用二分查找来降低时间复杂度。</p>
<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>　　我们可以维护一个决策序列B，用<strong>B[i]</strong>表示子序列长度为i时的最小末尾(a中的值)。可以看出如果<strong>i &gt; j</strong>，一定有<strong>B[i] &gt;= B[j]</strong>。原因很简单，看B的定义就可以知道。由上可知B是单调的序列，对于其中的的值可以进行二分查找。</p>
<p>　　所以可以按顺序枚举<strong>a[i]</strong>，如果<strong>a[i]</strong>的值比B中的最大值要大，则将<strong>a[i]</strong>放入<strong>B[i]</strong>的尾部，最大长度加1; 否则对B进行二分查找以找到<strong>a[i]</strong>该放入的位置，<strong>pos = min{j | B[j] &gt; a[i]}</strong>，用a[i]代替B[pos]，使a[i]成为子序列长度为pos时的最小值。大致过程如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (a[i] &gt; B[len<span class="number">-1</span>]) &#123;</div><div class="line">        B[len] = a[i];</div><div class="line">        len++;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">        pos = binarySearch(len, a[i]);</div><div class="line">        B[pos] = a[i];</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>完整代码请戳：<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/LIS-nlogn.cpp" target="_blank" rel="external">ljm’s github</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　朴素的LIS的&lt;strong&gt;O(n²)&lt;/strong&gt;算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为&lt;strong&gt;d[i] = max{d[j] | j &amp;lt; i &amp;amp;&amp;amp; a[j] &amp;lt; a[i]} +
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="dp" scheme="https://mingming97.github.io/tags/dp/"/>
    
  </entry>
  
  <entry>
    <title>并查集</title>
    <link href="https://mingming97.github.io/2017/08/28/Union-Find/"/>
    <id>https://mingming97.github.io/2017/08/28/Union-Find/</id>
    <published>2017-08-28T09:41:39.000Z</published>
    <updated>2018-05-17T16:08:39.771Z</updated>
    
    <content type="html"><![CDATA[<p>　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最快呢。并查集是一种轻量级的数据结构，其本质是一种集合，支持不相交集合的“并”和“查”的操作。由于在特定状况下，可以通过某些特殊处理将这两种操作的时间复杂度降至很低，所以在某些算法，例如tarjan求lca的实现中，也引入了这一数据结构。</p>
<h2 id="基本实现"><a href="#基本实现" class="headerlink" title="基本实现"></a>基本实现</h2><p>　　其实并查集的本质就是一个划分，商集中的每一个元素（集合）内的元素都可以建一棵树，放在一起就变成了森林。对于任意两个个元素，只需要看他们的树根是否相等，就可以判断他们是否属于同一集合；至于两个集合的合并，只需要将一棵树的树根指向另一棵树的树根。那么，可以用<strong>pre[i]</strong>记录i这个结点的父节点，初始化时只需令<strong>pre[i] = i</strong>，意味着每一个结点都属于一个单独的集合，也就是每一个节点都是树根。如果想查找树根怎么办呢？ 按照定义<strong>pre[i]</strong>是i的父亲，<strong>pre[pre[i]]</strong>就是i结点父亲的父亲，按照这个方式不断向上找直至<strong>pre[i]</strong>等于i，就找到了树根，可以使用递归来实现。如果想合并两个集合怎么办呢，只需要找到两个元素所在树的树根，将一个树根指向另一个，就完成了。</p>
<h2 id="时间复杂度优化"><a href="#时间复杂度优化" class="headerlink" title="时间复杂度优化"></a>时间复杂度优化</h2><h3 id="启发式合并"><a href="#启发式合并" class="headerlink" title="启发式合并"></a>启发式合并</h3><p>　　为了解决合并时树退化成链的情况，在合并时我们可以根据两棵树的深度合并，将最大深度小的向最大深度大的合并。如果两棵树的深度一样，则随便选一个作为根，并将根的最大深度+1。这样做的话在n次操作后，任何一棵树（一个集合）的深度最大不会超过<strong>[logn]+1</strong>，从而使查找的时间复杂度降为$O(logn)$。代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> rx = find(x), ry = find(y);</div><div class="line">    <span class="keyword">if</span> (rx != ry) &#123;</div><div class="line">        <span class="keyword">if</span> (rank[rx] == rank[ry]) &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">            rank[rx]++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (rank[rx] &lt; rank[ry]) &#123;</div><div class="line">            pre[rx] = ry;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="路径压缩"><a href="#路径压缩" class="headerlink" title="路径压缩"></a>路径压缩</h3><p>　　一般情况下我们只需要知道知道一个元素所在的树的根就可以，所以可以在查找元素的过程中，把路径上的所有子节点直接指向根节点，这样就可以将查找的复杂度降至O(1)。代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> x == pre[x] ? x : pre[x] = find(pre[x]);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="应用时的问题"><a href="#应用时的问题" class="headerlink" title="应用时的问题"></a>应用时的问题</h2><p>　　在具体问题中，两棵树是否可以按秩合并，以及是否可以路径压缩是需要按照题的要求来定的，本文只是提出了大体框架及基本实现（可移步<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/union-find.cpp" target="_blank" rel="external">我的github</a>查看）。具体节点信息的维护或是节点合并顺序等还需要自己判断。
　  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="并查集" scheme="https://mingming97.github.io/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/"/>
    
      <category term="数据结构" scheme="https://mingming97.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
