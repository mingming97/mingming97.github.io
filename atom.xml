<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ljm&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://mingming97.github.io/"/>
  <updated>2018-10-28T06:58:07.967Z</updated>
  <id>https://mingming97.github.io/</id>
  
  <author>
    <name>ljm</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SSD源码阅读及原理详解</title>
    <link href="https://mingming97.github.io/2018/10/21/SSD/"/>
    <id>https://mingming97.github.io/2018/10/21/SSD/</id>
    <published>2018-10-21T07:08:00.000Z</published>
    <updated>2018-10-28T06:58:07.967Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文阅读的版本是tensorflow对SSD的实现，相对而言阅读难度要远低于caffe版本的实现，源代码可见<a href="https://github.com/balancap/SSD-Tensorflow/" target="_blank" rel="external">balancap/SSD-Tensorflow</a>。</p>
<h2 id="一、思路"><a href="#一、思路" class="headerlink" title="一、思路"></a>一、思路</h2><p>&emsp;&emsp;SSD的网络结构在论文中清晰可见，如图所示。具体是使用了VGG的基础结构，保留了前五层，将fc6和fc7改为了带孔卷积层，而且去掉了池化层和dropout，在不增加参数的条件下指数级扩大了感受野，而且没有因为池化导致丢失太多的信息；后面再额外增加了3个卷积层和一个average pooling层，目的是使用不同层的feature map来检测不同尺度的物体。之后从前面的卷积层中提取了conv4_3，从后面新增的卷积层中提取了conv7，conv8_2，conv9_2，conv10_2，conv11_2来作为检测使用的特征图，在这些不同尺度特征图上提取不同数目的default boxes，对其进行分类和边框回归得到物体框和类别，最后进行nms来进行筛选。简而言之，SSD预测的目标就是以一张图中的所有提取出的anchor boxes为窗口，检测其中是否有物体，如果有，预测它的类别并对其位置进行精修，没有物体则将其分类为背景。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/structure.png" alt=""></p>
<p>&emsp;&emsp;通俗一点的思路如下面的两个图所示，SSD所做的其实就是将feature map用额外的两个卷积层去卷积得到分类评分和边框回归的偏移，其中k表示从该层feature的每个anchor处提取的不同default boxes的个数，这些词具体是什么可以在后面的代码细节中看到。其他的一些细节，例如数据增广，mining hard examples等，也都在代码中有体现。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/ssdstruct.jpg" alt=""></p>
<p>下面是提取结果的卷积层的放大图。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/extrafeature.jpg" alt=""></p>
<p>&emsp;&emsp;每个feature map可以分两条路，分别得到分类结果和回归结果，再通过已有的ground truth box及其类别得到每个default box的分类和边框偏移，就可以计算loss，进行训练了。</p>
<h2 id="二、default-boxes提取"><a href="#二、default-boxes提取" class="headerlink" title="二、default boxes提取"></a>二、default boxes提取</h2><p>&emsp;&emsp;default boxes的选取与faster rcnn中的anchor有一些类似，就是按照不同的scale和ratio生成k个boxes，看下面的图就能大概了解其思想</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/defaultboxes1.jpg" alt=""></p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/defaultboxes2.jpg" alt=""></p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/defaultboxes3.jpg" alt=""></p>
<ul>
<li><p>scale：scale指的是所检测的框的大小相对于原图的比例。比例低的可以框出图中的小物体，比例高的可以框出图中的大物体。深层次的feature map适合检测大物体，所以此处使用了一个线性关系来设置各个feature map所检测的scale。公式如下</p>
<script type="math/tex; mode=display">
s_k = s_{min} + \displaystyle\frac{s_{max}-s_{min}}{m-1}(k-1),k\in[1,m]</script><p>其中m是特征图的个数，实际取的时候为5，因为conv4_3层是单独设置的大小。$s_k$是第k个特征图的scale，$s_{min}$和$s_{max}$表示scale的最小值和最大值，在原论文中分别取0.2和0.9，而第一个特征图的scale一般设为$s_{min}$的一半，为0.1，所以对于300$\times$300的图片，最小的比例为300*0.1=30，之后每个对应feature map所检测的default boxes的大小都是300*$s_k$。在caffe源码中的计算是先给出了$s_k$的增长步长，也就是$\displaystyle \lfloor\frac{\lfloor s_{max}\times100\rfloor-\lfloor s_{min}\times100\rfloor}{m-1}\rfloor=17$，由此可以得到5个值分别为20，37，54，71，88（后面还会得到另一个虚拟值是88+17=105）。这些比例乘图片大小再除以100，就能得到各个特征图的大小分别为60，111，162，213，264。再结合最小比例，可以得到default boxes的实际尺度分别为30，60，111，162，213，264。</p>
</li>
<li><p>aspect ratio：aspect ratio指的是default boxes的横纵比，一般有$\displaystyle a_r \in \{1,2,3,\frac{1}{2},\frac{1}{3}\}$，对于特定的横纵比，会使用</p>
<script type="math/tex; mode=display">
w_k^a=s_k\sqrt{a_r}\ \ \ \ \ \ h_k^a=\displaystyle\frac{s_k}{\sqrt{a_r}}</script><p>来计算真正的宽度和高度（此处$s_k$也是指真实的大小，也就是上文中的30，60，111，…）。默认情况下，每个特征图会有一个$a_r=1$的default box，除此之外还会设置一个$s_k^{\ ‘}=\sqrt{s_k s_{k+1}}$，$a_r=1$的default box，也就是说每个特征图中都会设置两个大小不同的正方形的default box，此处最后一个特征图就需要用到之前的虚拟值105（对应的实际尺度是315）。然而在实现时，使用的比例是可以自己选择的，理论上每个feature map都应该有6个default boxes，但是实际实现中某些层只使用了4个default box，没有使用长宽比为$3$和$\frac{1}{3}$的default box。</p>
</li>
<li><p>default box中心：default box的中心在计算的时候需要恢复为原图的相对坐标，所以每个中心设置为$\displaystyle\left(\frac{i+0.5}{|f_k|},\frac{j+0.5}{|f_k|}\right)\ \ i,j \in [0, |f_k|)$，其中$|f_k|$表示第k个feature map的大小。</p>
</li>
</ul>
<p>综上可以得到如下表格</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">feature map</th>
<th style="text-align:center">feature map size</th>
<th style="text-align:center">min_size($\textbf{s}_\textbf{k}$)</th>
<th style="text-align:center">max_size($\textbf{s}_\textbf{k+1}$)</th>
<th style="text-align:center">aspect_ratio</th>
<th style="text-align:center">step</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">conv4_3</td>
<td style="text-align:center">38$\times$38</td>
<td style="text-align:center">30</td>
<td style="text-align:center">60</td>
<td style="text-align:center">1,2,$\frac{1}{2}$</td>
<td style="text-align:center">8</td>
</tr>
<tr>
<td style="text-align:center">conv7</td>
<td style="text-align:center">19$\times$19</td>
<td style="text-align:center">60</td>
<td style="text-align:center">111</td>
<td style="text-align:center">1,2,3,$\frac{1}{2}$,$\frac{1}{3}$</td>
<td style="text-align:center">16</td>
</tr>
<tr>
<td style="text-align:center">conv8_2</td>
<td style="text-align:center">10$\times$10</td>
<td style="text-align:center">111</td>
<td style="text-align:center">162</td>
<td style="text-align:center">1,2,3,$\frac{1}{2}$,$\frac{1}{3}$</td>
<td style="text-align:center">32</td>
</tr>
<tr>
<td style="text-align:center">conv9_2</td>
<td style="text-align:center">5$\times$5</td>
<td style="text-align:center">162</td>
<td style="text-align:center">213</td>
<td style="text-align:center">1,2,3,$\frac{1}{2}$,$\frac{1}{3}$</td>
<td style="text-align:center">64</td>
</tr>
<tr>
<td style="text-align:center">conv10_2</td>
<td style="text-align:center">3$\times$3</td>
<td style="text-align:center">213</td>
<td style="text-align:center">264</td>
<td style="text-align:center">1,2,$\frac{1}{2}$</td>
<td style="text-align:center">100</td>
</tr>
<tr>
<td style="text-align:center">conv11_2</td>
<td style="text-align:center">1$\times$1</td>
<td style="text-align:center">264</td>
<td style="text-align:center">315</td>
<td style="text-align:center">1,2,$\frac{1}{2}$</td>
<td style="text-align:center">300</td>
</tr>
</tbody>
</table>
</div>
<p>以上所有的内容在源码中都是有对应的体现的，首先看关于default box的一些设置，代码在<code>ssd_vgg_300.py</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">default_params = SSDParams(</div><div class="line">    img_shape=(<span class="number">300</span>, <span class="number">300</span>),</div><div class="line">    num_classes=<span class="number">21</span>,</div><div class="line">    no_annotation_label=<span class="number">21</span>,</div><div class="line">    feat_layers=[<span class="string">'block4'</span>, <span class="string">'block7'</span>, <span class="string">'block8'</span>, <span class="string">'block9'</span>, <span class="string">'block10'</span>, <span class="string">'block11'</span>],</div><div class="line">    feat_shapes=[(<span class="number">38</span>, <span class="number">38</span>), (<span class="number">19</span>, <span class="number">19</span>), (<span class="number">10</span>, <span class="number">10</span>), (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>)],</div><div class="line">    anchor_size_bounds=[<span class="number">0.15</span>, <span class="number">0.90</span>],</div><div class="line">    <span class="comment"># anchor_size_bounds=[0.20, 0.90],</span></div><div class="line">    anchor_sizes=[(<span class="number">21.</span>, <span class="number">45.</span>),</div><div class="line">                  (<span class="number">45.</span>, <span class="number">99.</span>),</div><div class="line">                  (<span class="number">99.</span>, <span class="number">153.</span>),</div><div class="line">                  (<span class="number">153.</span>, <span class="number">207.</span>),</div><div class="line">                  (<span class="number">207.</span>, <span class="number">261.</span>),</div><div class="line">                  (<span class="number">261.</span>, <span class="number">315.</span>)],</div><div class="line">    <span class="comment"># anchor_sizes=[(30., 60.),</span></div><div class="line">    <span class="comment">#               (60., 111.),</span></div><div class="line">    <span class="comment">#               (111., 162.),</span></div><div class="line">    <span class="comment">#               (162., 213.),</span></div><div class="line">    <span class="comment">#               (213., 264.),</span></div><div class="line">    <span class="comment">#               (264., 315.)],</span></div><div class="line">    anchor_ratios=[[<span class="number">2</span>, <span class="number">.5</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span>/<span class="number">3</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span>/<span class="number">3</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span>/<span class="number">3</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>]],</div><div class="line">    anchor_steps=[<span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">100</span>, <span class="number">300</span>],</div><div class="line">    anchor_offset=<span class="number">0.5</span>,</div><div class="line">    normalizations=[<span class="number">20</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>],</div><div class="line">    prior_scaling=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;其中img_shape代表输入图片的大小；num_classes代表输入的类别（20+1个背景类）；feat_layers代表提取的层；feat_shapes代表各个提取的featrue map的大小；anchor_size_bounds代表前文所说的$s_k$（原论文中取0.2到0.9）；anchor_sizes保存的是各层提取的default boxes的大小，也就是上面说的实际尺度；anchor_ratios是前面所说的各层的default boxes的横纵比；anchor_steps指的实际是default box的中心点坐标映射回原图的比例，做法就是用中心点坐标乘以对应的step；anchor_offset对应前文的offset；其余变量暂时与default boxes的生成无关。</p>
<p>&emsp;&emsp;整个训练过程都在<code>train_ssd_network.py</code>中，从这个文件中可以看出，第一步anchors的获取是通过<code>ssd_anchors = ssd_net.anchors(ssd_shape)</code>这句代码来获取的，而<code>ssd_net</code>这个对象是经由一个工厂类生成的一个网络类，此处以ssd_vgg_300为例，可以当作<code>ssd_net</code>就是一个<code>ssd_vgg_300.py</code>中定义的<code>SSDNet</code>的实例。这个被调用的函数可以在<code>ssd_vgg_300.py</code>中找到。而这个函数也只有一句话，那就是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> ssd_anchors_all_layers(img_shape,</div><div class="line">                              self.params.feat_shapes,</div><div class="line">                              self.params.anchor_sizes,</div><div class="line">                              self.params.anchor_ratios,</div><div class="line">                              self.params.anchor_steps,</div><div class="line">                              self.params.anchor_offset,</div><div class="line">                              dtype)</div></pre></td></tr></table></figure>
<p><code>ssd_anchors_all_layers</code>在该文件中的后半部分定义，只有几句话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_anchors_all_layers</span><span class="params">(img_shape,</span></span></div><div class="line"><span class="function"><span class="params">                           layers_shape,</span></span></div><div class="line"><span class="function"><span class="params">                           anchor_sizes,</span></span></div><div class="line"><span class="function"><span class="params">                           anchor_ratios,</span></span></div><div class="line"><span class="function"><span class="params">                           anchor_steps,</span></span></div><div class="line"><span class="function"><span class="params">                           offset=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                           dtype=np.float32)</span>:</span></div><div class="line">    <span class="string">"""Compute anchor boxes for all feature layers.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    layers_anchors = []</div><div class="line">    <span class="keyword">for</span> i, s <span class="keyword">in</span> enumerate(layers_shape):</div><div class="line">        anchor_bboxes = ssd_anchor_one_layer(img_shape, s,</div><div class="line">                                             anchor_sizes[i],</div><div class="line">                                             anchor_ratios[i],</div><div class="line">                                             anchor_steps[i],</div><div class="line">                                             offset=offset, dtype=dtype)</div><div class="line">        layers_anchors.append(anchor_bboxes)</div><div class="line">    <span class="keyword">return</span> layers_anchors</div></pre></td></tr></table></figure>
<p>可以看出，它是一层一层获取default box，再添加到列表中，此处使用了获取一层default box的函数，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_anchor_one_layer</span><span class="params">(img_shape,</span></span></div><div class="line"><span class="function"><span class="params">                         feat_shape,</span></span></div><div class="line"><span class="function"><span class="params">                         sizes,</span></span></div><div class="line"><span class="function"><span class="params">                         ratios,</span></span></div><div class="line"><span class="function"><span class="params">                         step,</span></span></div><div class="line"><span class="function"><span class="params">                         offset=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                         dtype=np.float32)</span>:</span></div><div class="line">    <span class="string">"""Computer SSD default anchor boxes for one feature layer.</span></div><div class="line"><span class="string">    Determine the relative position grid of the centers, and the relative</span></div><div class="line"><span class="string">    width and height.</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">      feat_shape: Feature shape, used for computing relative position grids;</span></div><div class="line"><span class="string">      size: Absolute reference sizes;</span></div><div class="line"><span class="string">      ratios: Ratios to use on these features;</span></div><div class="line"><span class="string">      img_shape: Image shape, used for computing height, width relatively to the</span></div><div class="line"><span class="string">        former;</span></div><div class="line"><span class="string">      offset: Grid offset.</span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">      y, x, h, w: Relative x and y grids, and height and width.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Compute the position grid: simple way.</span></div><div class="line">    <span class="comment"># y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]</span></div><div class="line">    <span class="comment"># y = (y.astype(dtype) + offset) / feat_shape[0]</span></div><div class="line">    <span class="comment"># x = (x.astype(dtype) + offset) / feat_shape[1]</span></div><div class="line">    <span class="comment"># Weird SSD-Caffe computation using steps values...</span></div><div class="line">    y, x = np.mgrid[<span class="number">0</span>:feat_shape[<span class="number">0</span>], <span class="number">0</span>:feat_shape[<span class="number">1</span>]]</div><div class="line">    y = (y.astype(dtype) + offset) * step / img_shape[<span class="number">0</span>]</div><div class="line">    x = (x.astype(dtype) + offset) * step / img_shape[<span class="number">1</span>]</div><div class="line">    <span class="comment"># Expand dims to support easy broadcasting.</span></div><div class="line">    y = np.expand_dims(y, axis=<span class="number">-1</span>)</div><div class="line">    x = np.expand_dims(x, axis=<span class="number">-1</span>)</div><div class="line">    <span class="comment"># Compute relative height and width.</span></div><div class="line">    <span class="comment"># Tries to follow the original implementation of SSD for the order.</span></div><div class="line">    num_anchors = len(sizes) + len(ratios)</div><div class="line">    h = np.zeros((num_anchors, ), dtype=dtype)</div><div class="line">    w = np.zeros((num_anchors, ), dtype=dtype)</div><div class="line">    <span class="comment"># Add first anchor boxes with ratio=1.</span></div><div class="line">    h[<span class="number">0</span>] = sizes[<span class="number">0</span>] / img_shape[<span class="number">0</span>]</div><div class="line">    w[<span class="number">0</span>] = sizes[<span class="number">0</span>] / img_shape[<span class="number">1</span>]</div><div class="line">    di = <span class="number">1</span></div><div class="line">    <span class="keyword">if</span> len(sizes) &gt; <span class="number">1</span>:</div><div class="line">        h[<span class="number">1</span>] = math.sqrt(sizes[<span class="number">0</span>] * sizes[<span class="number">1</span>]) / img_shape[<span class="number">0</span>]</div><div class="line">        w[<span class="number">1</span>] = math.sqrt(sizes[<span class="number">0</span>] * sizes[<span class="number">1</span>]) / img_shape[<span class="number">1</span>]</div><div class="line">        di += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> i, r <span class="keyword">in</span> enumerate(ratios):</div><div class="line">        h[i+di] = sizes[<span class="number">0</span>] / img_shape[<span class="number">0</span>] / math.sqrt(r)</div><div class="line">        w[i+di] = sizes[<span class="number">0</span>] / img_shape[<span class="number">1</span>] * math.sqrt(r)</div><div class="line">    <span class="keyword">return</span> y, x, h, w</div></pre></td></tr></table></figure>
<p>代码结合上面的讲解很好理解，通过上述的步骤，就得到了所有层的default box的y，x，h，w。举例来说，对于第一个特征图而言，y，x，h，w，的shape分别为(38,38,1)，(38,38,1)，(4, )，(4, )。</p>
<h2 id="三、Bboxes-Encode"><a href="#三、Bboxes-Encode" class="headerlink" title="三、Bboxes Encode"></a>三、Bboxes Encode</h2><p>&emsp;&emsp;要想理解这部分，需要知道什么是边框回归，此处有一个很好的讲解博客：<a href="https://blog.csdn.net/zijin0802034/article/details/77685438" target="_blank" rel="external"> 边框回归详解 </a>。知道了什么是边框回归，也就能理解我们这一步要干什么，主要有两个任务：1.给每个default box找到其对应的ground truth box，顺带得到其类别和得分。2.计算其相对于对应的ground truth box的变换，也就是边框回归要得到的目标变换。显然，这两步正是相当于给default boxes打上label的过程，对应之前说过的分类任务和回归任务。</p>
<p>&emsp;&emsp;在train的过程中只用一句话得到了每个default box的分类，边框偏移以及得分（用IOU定义，与GT box的IOU越大，得分越高），如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">gclasses, glocalisations, gscores = \</div><div class="line">    ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)</div></pre></td></tr></table></figure>
<p>这个函数同样在<code>ssd_vgg_300.py</code>，源码是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bboxes_encode</span><span class="params">(self, labels, bboxes, anchors,</span></span></div><div class="line"><span class="function"><span class="params">                  scope=None)</span>:</span></div><div class="line">    <span class="string">"""Encode labels and bounding boxes.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">return</span> ssd_common.tf_ssd_bboxes_encode(</div><div class="line">        labels, bboxes, anchors,</div><div class="line">        self.params.num_classes,</div><div class="line">        self.params.no_annotation_label,</div><div class="line">        ignore_threshold=<span class="number">0.5</span>,</div><div class="line">        prior_scaling=self.params.prior_scaling,</div><div class="line">        scope=scope)</div></pre></td></tr></table></figure>
<p>可以看出，结果是通过一个叫<code>tf_ssd_bboxes_encode</code>的函数获得的，其定义于<code>ssd_common.py</code>，如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_ssd_bboxes_encode</span><span class="params">(labels,</span></span></div><div class="line"><span class="function"><span class="params">                         bboxes,</span></span></div><div class="line"><span class="function"><span class="params">                         anchors,</span></span></div><div class="line"><span class="function"><span class="params">                         num_classes,</span></span></div><div class="line"><span class="function"><span class="params">                         no_annotation_label,</span></span></div><div class="line"><span class="function"><span class="params">                         ignore_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                         prior_scaling=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span></span></div><div class="line"><span class="function"><span class="params">                         dtype=tf.float32,</span></span></div><div class="line"><span class="function"><span class="params">                         scope=<span class="string">'ssd_bboxes_encode'</span>)</span>:</span></div><div class="line">    <span class="string">"""Encode groundtruth labels and bounding boxes using SSD net anchors.</span></div><div class="line"><span class="string">    Encoding boxes for all feature layers.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">      labels: 1D Tensor(int64) containing groundtruth labels;</span></div><div class="line"><span class="string">      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;</span></div><div class="line"><span class="string">      anchors: List of Numpy array with layer anchors;</span></div><div class="line"><span class="string">      matching_threshold: Threshold for positive match with groundtruth bboxes;</span></div><div class="line"><span class="string">      prior_scaling: Scaling of encoded coordinates.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">      (target_labels, target_localizations, target_scores):</span></div><div class="line"><span class="string">        Each element is a list of target Tensors.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(scope):</div><div class="line">        target_labels = []</div><div class="line">        target_localizations = []</div><div class="line">        target_scores = []</div><div class="line">        <span class="keyword">for</span> i, anchors_layer <span class="keyword">in</span> enumerate(anchors):</div><div class="line">            <span class="keyword">with</span> tf.name_scope(<span class="string">'bboxes_encode_block_%i'</span> % i):</div><div class="line">                t_labels, t_loc, t_scores = \</div><div class="line">                    tf_ssd_bboxes_encode_layer(labels, bboxes, anchors_layer,</div><div class="line">                                               num_classes, no_annotation_label,</div><div class="line">                                               ignore_threshold,</div><div class="line">                                               prior_scaling, dtype)</div><div class="line">                target_labels.append(t_labels)</div><div class="line">                target_localizations.append(t_loc)</div><div class="line">                target_scores.append(t_scores)</div><div class="line">        <span class="keyword">return</span> target_labels, target_localizations, target_scores</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;可以看出，类似于anchors的获得，default box的标定也是先一个特征图一个特征图进行，之后再将一个特征图的结果分别放入对应列表中。下面来看每个特征图是如何处理的，处理一个特征图的函数是<code>tf_ssd_bboxes_encode_layer</code>，源码见下面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_ssd_bboxes_encode_layer</span><span class="params">(labels,</span></span></div><div class="line"><span class="function"><span class="params">                               bboxes,</span></span></div><div class="line"><span class="function"><span class="params">                               anchors_layer,</span></span></div><div class="line"><span class="function"><span class="params">                               num_classes,</span></span></div><div class="line"><span class="function"><span class="params">                               no_annotation_label,</span></span></div><div class="line"><span class="function"><span class="params">                               ignore_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                               prior_scaling=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span></span></div><div class="line"><span class="function"><span class="params">                               dtype=tf.float32)</span>:</span></div><div class="line">    <span class="string">"""Encode groundtruth labels and bounding boxes using SSD anchors from</span></div><div class="line"><span class="string">    one layer.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">      labels: 1D Tensor(int64) containing groundtruth labels;</span></div><div class="line"><span class="string">      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;</span></div><div class="line"><span class="string">      anchors_layer: Numpy array with layer anchors;</span></div><div class="line"><span class="string">      matching_threshold: Threshold for positive match with groundtruth bboxes;</span></div><div class="line"><span class="string">      prior_scaling: Scaling of encoded coordinates.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">      (target_labels, target_localizations, target_scores): Target Tensors.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Anchors coordinates and volume.</span></div><div class="line">    yref, xref, href, wref = anchors_layer</div><div class="line">    ymin = yref - href / <span class="number">2.</span></div><div class="line">    xmin = xref - wref / <span class="number">2.</span></div><div class="line">    ymax = yref + href / <span class="number">2.</span></div><div class="line">    xmax = xref + wref / <span class="number">2.</span></div><div class="line">    vol_anchors = (xmax - xmin) * (ymax - ymin)</div><div class="line"></div><div class="line">    <span class="comment"># Initialize tensors...</span></div><div class="line">    shape = (yref.shape[<span class="number">0</span>], yref.shape[<span class="number">1</span>], href.size)</div><div class="line">    feat_labels = tf.zeros(shape, dtype=tf.int64)</div><div class="line">    feat_scores = tf.zeros(shape, dtype=dtype)</div><div class="line"></div><div class="line">    feat_ymin = tf.zeros(shape, dtype=dtype)</div><div class="line">    feat_xmin = tf.zeros(shape, dtype=dtype)</div><div class="line">    feat_ymax = tf.ones(shape, dtype=dtype)</div><div class="line">    feat_xmax = tf.ones(shape, dtype=dtype)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jaccard_with_anchors</span><span class="params">(bbox)</span>:</span></div><div class="line">        <span class="string">"""Compute jaccard score between a box and the anchors.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        int_ymin = tf.maximum(ymin, bbox[<span class="number">0</span>])</div><div class="line">        int_xmin = tf.maximum(xmin, bbox[<span class="number">1</span>])</div><div class="line">        int_ymax = tf.minimum(ymax, bbox[<span class="number">2</span>])</div><div class="line">        int_xmax = tf.minimum(xmax, bbox[<span class="number">3</span>])</div><div class="line">        h = tf.maximum(int_ymax - int_ymin, <span class="number">0.</span>)</div><div class="line">        w = tf.maximum(int_xmax - int_xmin, <span class="number">0.</span>)</div><div class="line">        <span class="comment"># Volumes.</span></div><div class="line">        inter_vol = h * w</div><div class="line">        union_vol = vol_anchors - inter_vol \</div><div class="line">            + (bbox[<span class="number">2</span>] - bbox[<span class="number">0</span>]) * (bbox[<span class="number">3</span>] - bbox[<span class="number">1</span>])</div><div class="line">        jaccard = tf.div(inter_vol, union_vol)</div><div class="line">        <span class="keyword">return</span> jaccard</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersection_with_anchors</span><span class="params">(bbox)</span>:</span></div><div class="line">        <span class="string">"""Compute intersection between score a box and the anchors.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        int_ymin = tf.maximum(ymin, bbox[<span class="number">0</span>])</div><div class="line">        int_xmin = tf.maximum(xmin, bbox[<span class="number">1</span>])</div><div class="line">        int_ymax = tf.minimum(ymax, bbox[<span class="number">2</span>])</div><div class="line">        int_xmax = tf.minimum(xmax, bbox[<span class="number">3</span>])</div><div class="line">        h = tf.maximum(int_ymax - int_ymin, <span class="number">0.</span>)</div><div class="line">        w = tf.maximum(int_xmax - int_xmin, <span class="number">0.</span>)</div><div class="line">        inter_vol = h * w</div><div class="line">        scores = tf.div(inter_vol, vol_anchors)</div><div class="line">        <span class="keyword">return</span> scores</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(i, feat_labels, feat_scores,</span></span></div><div class="line"><span class="function"><span class="params">                  feat_ymin, feat_xmin, feat_ymax, feat_xmax)</span>:</span></div><div class="line">        <span class="string">"""Condition: check label index.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        r = tf.less(i, tf.shape(labels))</div><div class="line">        <span class="keyword">return</span> r[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, feat_labels, feat_scores,</span></span></div><div class="line"><span class="function"><span class="params">             feat_ymin, feat_xmin, feat_ymax, feat_xmax)</span>:</span></div><div class="line">        <span class="string">"""Body: update feature labels, scores and bboxes.</span></div><div class="line"><span class="string">        Follow the original SSD paper for that purpose:</span></div><div class="line"><span class="string">          - assign values when jaccard &gt; 0.5;</span></div><div class="line"><span class="string">          - only update if beat the score of other bboxes.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="comment"># Jaccard score.</span></div><div class="line">        label = labels[i]</div><div class="line">        bbox = bboxes[i]</div><div class="line">        jaccard = jaccard_with_anchors(bbox)</div><div class="line">        <span class="comment"># Mask: check threshold + scores + no annotations + num_classes.</span></div><div class="line">        mask = tf.greater(jaccard, feat_scores)</div><div class="line">        <span class="comment"># mask = tf.logical_and(mask, tf.greater(jaccard, matching_threshold))</span></div><div class="line">        mask = tf.logical_and(mask, feat_scores &gt; <span class="number">-0.5</span>)</div><div class="line">        mask = tf.logical_and(mask, label &lt; num_classes)</div><div class="line">        imask = tf.cast(mask, tf.int64)</div><div class="line">        fmask = tf.cast(mask, dtype)</div><div class="line">        <span class="comment"># Update values using mask.</span></div><div class="line">        feat_labels = imask * label + (<span class="number">1</span> - imask) * feat_labels</div><div class="line">        feat_scores = tf.where(mask, jaccard, feat_scores)</div><div class="line"></div><div class="line">        feat_ymin = fmask * bbox[<span class="number">0</span>] + (<span class="number">1</span> - fmask) * feat_ymin</div><div class="line">        feat_xmin = fmask * bbox[<span class="number">1</span>] + (<span class="number">1</span> - fmask) * feat_xmin</div><div class="line">        feat_ymax = fmask * bbox[<span class="number">2</span>] + (<span class="number">1</span> - fmask) * feat_ymax</div><div class="line">        feat_xmax = fmask * bbox[<span class="number">3</span>] + (<span class="number">1</span> - fmask) * feat_xmax</div><div class="line"></div><div class="line">        <span class="comment"># Check no annotation label: ignore these anchors...</span></div><div class="line">        <span class="comment"># interscts = intersection_with_anchors(bbox)</span></div><div class="line">        <span class="comment"># mask = tf.logical_and(interscts &gt; ignore_threshold,</span></div><div class="line">        <span class="comment">#                       label == no_annotation_label)</span></div><div class="line">        <span class="comment"># # Replace scores by -1.</span></div><div class="line">        <span class="comment"># feat_scores = tf.where(mask, -tf.cast(mask, dtype), feat_scores)</span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> [i+<span class="number">1</span>, feat_labels, feat_scores,</div><div class="line">                feat_ymin, feat_xmin, feat_ymax, feat_xmax]</div><div class="line">    <span class="comment"># Main loop definition.</span></div><div class="line">    i = <span class="number">0</span></div><div class="line">    [i, feat_labels, feat_scores,</div><div class="line">     feat_ymin, feat_xmin,</div><div class="line">     feat_ymax, feat_xmax] = tf.while_loop(condition, body,</div><div class="line">                                           [i, feat_labels, feat_scores,</div><div class="line">                                            feat_ymin, feat_xmin,</div><div class="line">                                            feat_ymax, feat_xmax])</div><div class="line">    <span class="comment"># Transform to center / size.</span></div><div class="line">    feat_cy = (feat_ymax + feat_ymin) / <span class="number">2.</span></div><div class="line">    feat_cx = (feat_xmax + feat_xmin) / <span class="number">2.</span></div><div class="line">    feat_h = feat_ymax - feat_ymin</div><div class="line">    feat_w = feat_xmax - feat_xmin</div><div class="line">    <span class="comment"># Encode features.</span></div><div class="line">    feat_cy = (feat_cy - yref) / href / prior_scaling[<span class="number">0</span>]</div><div class="line">    feat_cx = (feat_cx - xref) / wref / prior_scaling[<span class="number">1</span>]</div><div class="line">    feat_h = tf.log(feat_h / href) / prior_scaling[<span class="number">2</span>]</div><div class="line">    feat_w = tf.log(feat_w / wref) / prior_scaling[<span class="number">3</span>]</div><div class="line">    <span class="comment"># Use SSD ordering: x / y / w / h instead of ours.</span></div><div class="line">    feat_localizations = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=<span class="number">-1</span>)</div><div class="line">    <span class="keyword">return</span> feat_labels, feat_localizations, feat_scores</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个函数比较长，先看第一个函数之前的部分，首先是通过之前得到的一层特征图的y，x，h，w，计算每个default box的四个角的坐标及面积（此处利用了numpy的广播机制），随后初始化了一些空的tensor：类别标签、得分以及每个default box对应的GT box的四个角的坐标。shape也都符合之前的定义，此处以第一个特征图为例，其大小为38$\times$38，每个中心点对应4个default box，每个default box对应一个label，一个GT box和一个得分，所以所有初始化tensor的shape都是(38,38,4)。</p>
<p>&emsp;&emsp;下面是几个辅助函数，<code>jaccard_with_anchors(bbox)</code>用于计算bbox与所有default box的IOU；<code>intersection_with_anchors(bbox)</code>用于计算bbox与所有default box的交比上default box的面积的值，在此处没有用到这个函数；<code>condition</code>，<code>body</code>要和下面的<code>tf.while_loop</code>连起来看，<code>tf.while_loop</code>的执行逻辑是，若condition为真，执行body，<code>condition</code>只有一句话，其实就是判断<code>i</code>的值是否小于GT box的数目，也就是说整个循环逻辑是以每个GT box为单位进行的，<code>body</code>就是对每个GT box进行的操作。</p>
<p>&emsp;&emsp;在看<code>body</code>具体做了什么之前，需要先了解SSD中GT box的匹配机制。共有两个原则，第一个原则是每个GT box与其IOU最大的defaut box匹配，这样就能保证每个GT box都有一个与其匹配的default box。但是这样的话正负样本严重不平衡，因此还需要第二个原则，那就是对于未匹配的default box，若与某个GT box的IOU大于一个阈值（SSD中取0.5），那么也将其进行匹配，此处有一个问题就是若某个default box与几个GT box的IOU都大于阈值，选哪个与其匹配？显然，选与其IOU最大的那个GT box与之匹配。这样就大大增加了正样本的个数。还会有一个矛盾在于，假如一个GT box 1与其IOU最大的default box小于阈值，而该default box与某一个GT box 2的IOU大于阈值，如何选择。此处应该按照第一个原则，选择GT box 1，因为必须要保证每个GT box要有一个default box与之匹配。（实际情况中该矛盾发生可能较低，所以该tensorflow实现中仅实施了第二个原则。）</p>
<p>&emsp;&emsp;下面可以看一下<code>body</code>函数，可以看出，它的逻辑是使用某个GT box与所有default box的已经匹配的GT box的结果去比较，再决定是否更换每个default box对应的GT box。函数中出现了很多mask*A + (1-mask)*B的模式，mask的值只有0和1两种，那么这句话的意义就很显然了，如果mask为1，新值为A，否则为B，对应到具体情况中就是mask为1则更换对应GT box，为0则保持不变，那么决定是否更换的mask的值则来自于前面的条件判断，条件判断如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Mask: check threshold + scores + no annotations + num_classes.</span></div><div class="line">mask = tf.greater(jaccard, feat_scores)</div><div class="line"><span class="comment"># mask = tf.logical_and(mask, tf.greater(jaccard, matching_threshold))</span></div><div class="line">mask = tf.logical_and(mask, feat_scores &gt; <span class="number">-0.5</span>)</div><div class="line">mask = tf.logical_and(mask, label &lt; num_classes)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;此处更改了之前的逻辑，将大于阈值的部分去掉，改为只要大于之前的IOU，就进行GT box的匹配。</p>
<p>&emsp;&emsp;找到了所有default box对应的GT box的四个角的坐标，就可以开始进行边框偏移的计算，在SSD中此处有一个技巧，假设已知default box的位置$\boldsymbol{d=(d^{cx},d^{cy},d^w,d^h)}$，以及它对应的GT box的位置$\boldsymbol{b=(b^{cx},b^{cy},b^w,b^h)}$，通常的边框偏移是按照如下方式计算的</p>
<script type="math/tex; mode=display">
\begin{align}
& t^{cx} = \displaystyle\frac{b^{cx}-d^{cx}}{d^w} \\
& t^{cy} = \displaystyle\frac{b^{cy}-d^{cy}}{d^h} \\
& t^w = \displaystyle log\left(\frac{b^w}{d^w}\right) \\
& t^h = \displaystyle log\left(\frac{b^h}{d^h}\right) \\
\end{align}</script><p>&emsp;&emsp;这个过程称为编码（encode），对应的解码（decode）过程则为</p>
<script type="math/tex; mode=display">
\begin{align}
& b^{cx} = d^wt^{cx} + d^{cx} \\
& b^{cy} = d^ht^{cy} + d^{cy} \\
& b^w = d^wexp(t^w) \\
& b^h = d^hexp(t^h) \\
\end{align}</script><p>&emsp;&emsp;但是在SSD中设置了variance来调整对t的放缩，无论在解码还是编码时都会使用variance来控制，此时编码过程计算如下</p>
<script type="math/tex; mode=display">
\begin{align}
& t^{cx} = \displaystyle\frac{b^{cx}-d^{cx}}{d^w\times variance[0]} \\
& t^{cy} = \displaystyle\frac{b^{cy}-d^{cy}}{d^h\times variance[1]} \\
& t^w = \displaystyle \frac{log\left(\frac{b^w}{d^w}\right)}{variance[2]} \\
& t^h = \displaystyle \frac{log\left(\frac{b^h}{d^h}\right)}{variance[3]} \\
\end{align}</script><p>&emsp;&emsp;解码过程如下</p>
<script type="math/tex; mode=display">
\begin{align}
& b^{cx} = d^w(t^{cx}variance[0]) + d^{cx} \\
& b^{cy} = d^h(t^{cy}variance[1]) + d^{cy} \\
& b^w = d^wexp(t^wvariance[2]) \\
& b^h = d^hexp(t^hvariance[3]) \\
\end{align}</script><p>&emsp;&emsp;variance可以选择训练得到还是手动设定，在SSD中选择手动设定，这也就是<code>SSDParams</code>中<code>parior_scaling</code>中四个数的含义，其实就是对应的variance。</p>
<h2 id="四、网络结构"><a href="#四、网络结构" class="headerlink" title="四、网络结构"></a>四、网络结构</h2><p>&emsp;&emsp;除了对数据的预处理，以及并行化处理意外，接下来就是将数据喂进网络，得到每个default box的分类结果和边框偏移。接着看<code>train_ssd_network.py</code>，可以看到这样一句代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">predictions, localisations, logits, end_points = \</div><div class="line">    ssd_net.net(b_image, is_training=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;它调用了net函数，返回了四个变量，为了清楚这个函数做了什么，提前说明四个变量的含义：<code>predictions</code>就是default box在各个类上的得分，也就是后面的<code>logits</code>通过softmax得到的结果，这样<code>logits</code>是什么就无需解释了，<code>localisations</code>是对default box的边框偏移预测结果，<code>end_points</code>是一个字典，里面储存着各个block的输出特征图。</p>
<p>&emsp;&emsp;下面来看net函数，发现它的核心只有一句话</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">r = ssd_net(inputs,</div><div class="line">            num_classes=self.params.num_classes,</div><div class="line">            feat_layers=self.params.feat_layers,</div><div class="line">            anchor_sizes=self.params.anchor_sizes,</div><div class="line">            anchor_ratios=self.params.anchor_ratios,</div><div class="line">            normalizations=self.params.normalizations,</div><div class="line">            is_training=is_training,</div><div class="line">            dropout_keep_prob=dropout_keep_prob,</div><div class="line">            prediction_fn=prediction_fn,</div><div class="line">            reuse=reuse,</div><div class="line">            scope=scope)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;而ssd_net是定义在跟net相同文件（<code>ssd_vgg_300.py</code>）中的一个函数，我们可以在下面找到它的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_net</span><span class="params">(inputs,</span></span></div><div class="line"><span class="function"><span class="params">            num_classes=SSDNet.default_params.num_classes,</span></span></div><div class="line"><span class="function"><span class="params">            feat_layers=SSDNet.default_params.feat_layers,</span></span></div><div class="line"><span class="function"><span class="params">            anchor_sizes=SSDNet.default_params.anchor_sizes,</span></span></div><div class="line"><span class="function"><span class="params">            anchor_ratios=SSDNet.default_params.anchor_ratios,</span></span></div><div class="line"><span class="function"><span class="params">            normalizations=SSDNet.default_params.normalizations,</span></span></div><div class="line"><span class="function"><span class="params">            is_training=True,</span></span></div><div class="line"><span class="function"><span class="params">            dropout_keep_prob=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">            prediction_fn=slim.softmax,</span></span></div><div class="line"><span class="function"><span class="params">            reuse=None,</span></span></div><div class="line"><span class="function"><span class="params">            scope=<span class="string">'ssd_300_vgg'</span>)</span>:</span></div><div class="line">    <span class="string">"""SSD net definition.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># if data_format == 'NCHW':</span></div><div class="line">    <span class="comment">#     inputs = tf.transpose(inputs, perm=(0, 3, 1, 2))</span></div><div class="line"></div><div class="line">    <span class="comment"># End_points collect relevant activations for external use.</span></div><div class="line">    end_points = &#123;&#125;</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(scope, <span class="string">'ssd_300_vgg'</span>, [inputs], reuse=reuse):</div><div class="line">        <span class="comment"># Original VGG-16 blocks.</span></div><div class="line">        net = slim.repeat(inputs, <span class="number">2</span>, slim.conv2d, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1'</span>)</div><div class="line">        end_points[<span class="string">'block1'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool1'</span>)</div><div class="line">        <span class="comment"># Block 2.</span></div><div class="line">        net = slim.repeat(net, <span class="number">2</span>, slim.conv2d, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv2'</span>)</div><div class="line">        end_points[<span class="string">'block2'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</div><div class="line">        <span class="comment"># Block 3.</span></div><div class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</div><div class="line">        end_points[<span class="string">'block3'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool3'</span>)</div><div class="line">        <span class="comment"># Block 4.</span></div><div class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv4'</span>)</div><div class="line">        end_points[<span class="string">'block4'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool4'</span>)</div><div class="line">        <span class="comment"># Block 5.</span></div><div class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv5'</span>)</div><div class="line">        end_points[<span class="string">'block5'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'pool5'</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Additional SSD blocks.</span></div><div class="line">        <span class="comment"># Block 6: let's dilate the hell out of it!</span></div><div class="line">        net = slim.conv2d(net, <span class="number">1024</span>, [<span class="number">3</span>, <span class="number">3</span>], rate=<span class="number">6</span>, scope=<span class="string">'conv6'</span>)</div><div class="line">        end_points[<span class="string">'block6'</span>] = net</div><div class="line">        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)</div><div class="line">        <span class="comment"># Block 7: 1x1 conv. Because the fuck.</span></div><div class="line">        net = slim.conv2d(net, <span class="number">1024</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv7'</span>)</div><div class="line">        end_points[<span class="string">'block7'</span>] = net</div><div class="line">        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)</div><div class="line"></div><div class="line">        <span class="comment"># Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).</span></div><div class="line">        end_point = <span class="string">'block8'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = custom_layers.pad2d(net, pad=(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line">            net = slim.conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line">        end_point = <span class="string">'block9'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = custom_layers.pad2d(net, pad=(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line">        end_point = <span class="string">'block10'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line">        end_point = <span class="string">'block11'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line"></div><div class="line">        <span class="comment"># Prediction and localisations layers.</span></div><div class="line">        predictions = []</div><div class="line">        logits = []</div><div class="line">        localisations = []</div><div class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(feat_layers):</div><div class="line">            <span class="keyword">with</span> tf.variable_scope(layer + <span class="string">'_box'</span>):</div><div class="line">                p, l = ssd_multibox_layer(end_points[layer],</div><div class="line">                                          num_classes,</div><div class="line">                                          anchor_sizes[i],</div><div class="line">                                          anchor_ratios[i],</div><div class="line">                                          normalizations[i])</div><div class="line">            predictions.append(prediction_fn(p))</div><div class="line">            logits.append(p)</div><div class="line">            localisations.append(l)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> predictions, localisations, logits, end_points</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;代码结构十分清晰，首先看前面定义网络的部分，这个定网络的定义与VGG16类似，只不过替换了其中某些层，原因在第一部分可以看到，可以看到在这一部分中每个block的输出被放进了<code>end_points</code>字典中。而后面则是根据特征图生成分类结果和偏移结果的部分，可以看到也是逐层进行并放到一个列表中的形式，每一层预测结果的获得都调用了<code>ssd_multibox_layer</code>函数，下面就看一下该函数的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_multibox_layer</span><span class="params">(inputs,</span></span></div><div class="line"><span class="function"><span class="params">                       num_classes,</span></span></div><div class="line"><span class="function"><span class="params">                       sizes,</span></span></div><div class="line"><span class="function"><span class="params">                       ratios=[<span class="number">1</span>],</span></span></div><div class="line"><span class="function"><span class="params">                       normalization=<span class="number">-1</span>,</span></span></div><div class="line"><span class="function"><span class="params">                       bn_normalization=False)</span>:</span></div><div class="line">    <span class="string">"""Construct a multibox layer, return a class and localization predictions.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    net = inputs</div><div class="line">    <span class="keyword">if</span> normalization &gt; <span class="number">0</span>:</div><div class="line">        net = custom_layers.l2_normalization(net, scaling=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># Number of anchors.</span></div><div class="line">    num_anchors = len(sizes) + len(ratios)</div><div class="line"></div><div class="line">    <span class="comment"># Location.</span></div><div class="line">    num_loc_pred = num_anchors * <span class="number">4</span></div><div class="line">    loc_pred = slim.conv2d(net, num_loc_pred, [<span class="number">3</span>, <span class="number">3</span>], activation_fn=<span class="keyword">None</span>,</div><div class="line">                           scope=<span class="string">'conv_loc'</span>)</div><div class="line">    loc_pred = custom_layers.channel_to_last(loc_pred)</div><div class="line">    loc_pred = tf.reshape(loc_pred,</div><div class="line">                          tensor_shape(loc_pred, <span class="number">4</span>)[:<span class="number">-1</span>]+[num_anchors, <span class="number">4</span>])</div><div class="line">    <span class="comment"># Class prediction.</span></div><div class="line">    num_cls_pred = num_anchors * num_classes</div><div class="line">    cls_pred = slim.conv2d(net, num_cls_pred, [<span class="number">3</span>, <span class="number">3</span>], activation_fn=<span class="keyword">None</span>,</div><div class="line">                           scope=<span class="string">'conv_cls'</span>)</div><div class="line">    cls_pred = custom_layers.channel_to_last(cls_pred)</div><div class="line">    cls_pred = tf.reshape(cls_pred,</div><div class="line">                          tensor_shape(cls_pred, <span class="number">4</span>)[:<span class="number">-1</span>]+[num_anchors, num_classes])</div><div class="line">    <span class="keyword">return</span> cls_pred, loc_pred</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>ssd_multibox_layer</code>同样定义于<code>ssd_vgg_300.py</code>中，可以看到一开始对normalization值进行了判断，此处就是<code>SSDParams</code>中<code>normalizations</code>的作用，在SSD中由于第一个要提取特征层较浅，其norm较大，所以要对其进行沿channel方向的l2_normalize，而其他层无需进行此操作，代码中也仅是判断了<code>normalization</code>的值是否大于0，所以在<code>normalizations</code>中第一个值大于0，其他都小于0，20和-1无实际含义。</p>
<p>&emsp;&emsp;下面则是对location的计算，首先使用了一个3*3卷积，通道数是每个中心default box的个数乘4，代表y，x，w，h四个偏移，从而得到了每个中心的边框偏移的结果，此处又进行了一个reshape操作，其中<code>tensor_shape</code>得到<code>loc_pred</code>的形状，再通过切片将最后一维去掉，再加上两维，分别是每个中心default box的个数，和4，这样就得到了[batch_size,size[0],size[1],num_anchors,4]的tensor。以第二个特征层为例，<code>loc_pred</code>的形状为[batch_size,19,19,6,4]，同理可以得到第二个特征层的<code>cls_pred</code>的形状为[batch_size,19,19,6,21]。</p>
<h2 id="五、loss的计算"><a href="#五、loss的计算" class="headerlink" title="五、loss的计算"></a>五、loss的计算</h2><p>&emsp;&emsp;SSD的loss是一个multitask的loss，包含分类损失和定位损失，公式如下所示</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/total_loss.png" alt=""></p>
<p>&emsp;&emsp;所有此处所有的loss值均是对一张图而言的，式子中的$N$代表所有default box中正样本（有对应GT box）的数量，$\alpha$用于调整分类损失和定位损失的比例，下面看一下二者的具体计算。</p>
<p>&emsp;&emsp;首先看分类损失</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/conf_loss.png" alt=""></p>
<p>&emsp;&emsp;式子中的$x_{ij}^p\in \{0,1\}$，类似于一个指示函数，当第i个default box与第j个GT box匹配并属于第p类时，$x_{ij}^p=1$，其他情况下$x_{ij}^p=0$。显然$c_i^p$就是之前得到的<code>logits</code>，所以整个式子其实就是一个交叉熵损失。</p>
<p>​    接下来看一下定位损失。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/ssdtensorflow/loc_loss.png" alt=""></p>
<p>&emsp;&emsp;定位损失中的$x_{ij}^p$与分类损失中的含义相同，$\hat g_j^m$根据下面的定义，含义是default box到其对应GT box的偏移，$l_i^m$则是预测的偏移，对二者的误差使用了smooth L1 loss。</p>
<p>&emsp;&emsp;与之前的过程类似，可以在<code>train_ssd_network.py</code>中看到求loss的代码，如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ssd_net.losses(logits, localisations,</div><div class="line">               b_gclasses, b_glocalisations, b_gscores,</div><div class="line">               match_threshold=FLAGS.match_threshold,</div><div class="line">               negative_ratio=FLAGS.negative_ratio,</div><div class="line">               alpha=FLAGS.loss_alpha,</div><div class="line">               label_smoothing=FLAGS.label_smoothing)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;它调用了<code>SSDNet</code>的<code>losses</code>函数，只有一句话</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">losses</span><span class="params">(self, logits, localisations,</span></span></div><div class="line"><span class="function"><span class="params">           gclasses, glocalisations, gscores,</span></span></div><div class="line"><span class="function"><span class="params">           match_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">           negative_ratio=<span class="number">3.</span>,</span></span></div><div class="line"><span class="function"><span class="params">           alpha=<span class="number">1.</span>,</span></span></div><div class="line"><span class="function"><span class="params">           label_smoothing=<span class="number">0.</span>,</span></span></div><div class="line"><span class="function"><span class="params">           scope=<span class="string">'ssd_losses'</span>)</span>:</span></div><div class="line">    <span class="string">"""Define the SSD network losses.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">return</span> ssd_losses(logits, localisations,</div><div class="line">                      gclasses, glocalisations, gscores,</div><div class="line">                      match_threshold=match_threshold,</div><div class="line">                      negative_ratio=negative_ratio,</div><div class="line">                      alpha=alpha,</div><div class="line">                      label_smoothing=label_smoothing,</div><div class="line">                      scope=scope)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;它调用的<code>ssd_losses</code>是定义在相同文件中的函数，为了减少说明，使用了网上的有注释的版本，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># =========================================================================== #</span></div><div class="line"><span class="comment"># SSD loss function.</span></div><div class="line"><span class="comment"># =========================================================================== #</span></div><div class="line"><span class="comment">#logits.shape=[(5,38,38,4,21),(5,19,19,6,21),(5,10,10,6,21),(5,5,5,6,21),(5,3,3,4,21),(5,1,1,4,21)]</span></div><div class="line"><span class="comment">#localisations.shape=[(5,38,38,4,4),(5,19,19,6,4),(5,10,10,6,4),(5,5,5,6,4),(5,3,3,4,4),(5,1,1,4,4)],glocalisations同</span></div><div class="line"><span class="comment">#gclasses.shape=[(5,38,38,4),.................], gscores同</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_losses</span><span class="params">(logits, localisations,             </span></span></div><div class="line"><span class="function"><span class="params">               #预测类别, 预测位置</span></span></div><div class="line"><span class="function"><span class="params">               gclasses, glocalisations, gscores,</span></span></div><div class="line"><span class="function"><span class="params">               #ground truth 类别, ground truth 位置, ground truth 分数</span></span></div><div class="line"><span class="function"><span class="params">               match_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">               negative_ratio=<span class="number">3.</span>,</span></span></div><div class="line"><span class="function"><span class="params">               alpha=<span class="number">1.</span>,</span></span></div><div class="line"><span class="function"><span class="params">               label_smoothing=<span class="number">0.</span>,</span></span></div><div class="line"><span class="function"><span class="params">               device=<span class="string">'/cpu:0'</span>,</span></span></div><div class="line"><span class="function"><span class="params">               scope=None)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(scope, <span class="string">'ssd_losses'</span>):</div><div class="line">        lshape = tfe.get_shape(logits[<span class="number">0</span>], <span class="number">5</span>)</div><div class="line">        num_classes = lshape[<span class="number">-1</span>]</div><div class="line">        batch_size = lshape[<span class="number">0</span>]	<span class="comment">#5</span></div><div class="line"> </div><div class="line">        <span class="comment"># Flatten out all vectors!</span></div><div class="line">        flogits = []</div><div class="line">        fgclasses = []</div><div class="line">        fgscores = []</div><div class="line">        flocalisations = []</div><div class="line">        fglocalisations = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(logits)):</div><div class="line">            flogits.append(tf.reshape(logits[i], [<span class="number">-1</span>, num_classes]))</div><div class="line">            fgclasses.append(tf.reshape(gclasses[i], [<span class="number">-1</span>]))</div><div class="line">            fgscores.append(tf.reshape(gscores[i], [<span class="number">-1</span>]))</div><div class="line">            flocalisations.append(tf.reshape(localisations[i], [<span class="number">-1</span>, <span class="number">4</span>]))</div><div class="line">            fglocalisations.append(tf.reshape(glocalisations[i], [<span class="number">-1</span>, <span class="number">4</span>]))</div><div class="line"> </div><div class="line">        <span class="comment">#flogits.shape=[shape=(5×38×38×4,21), shape=(5×19×19×6,21), ......], 共6个feature map的组成</span></div><div class="line">        <span class="comment">#fgclasses.shape=[shape=(5×38×38×4), shape=(5×19×19×6), ......]其它相似</span></div><div class="line"> </div><div class="line">        <span class="comment"># And concat the crap!</span></div><div class="line">        <span class="comment">#logits.shape=(5×38×38×4+5×19×19×6+...+5×1×1×4, 21)</span></div><div class="line">        <span class="comment">#将[flogits[1],flogits[2],...,flogits[i],...]按第一维组合在一起，下同</span></div><div class="line">        logits = tf.concat(flogits, axis=<span class="number">0</span>) </div><div class="line">        </div><div class="line">        gclasses = tf.concat(fgclasses, axis=<span class="number">0</span>)</div><div class="line">        gscores = tf.concat(fgscores, axis=<span class="number">0</span>)</div><div class="line">        localisations = tf.concat(flocalisations, axis=<span class="number">0</span>)</div><div class="line">        glocalisations = tf.concat(fglocalisations, axis=<span class="number">0</span>)</div><div class="line">        dtype = logits.dtype</div><div class="line"> </div><div class="line">        <span class="comment"># Compute positive matching mask...</span></div><div class="line">        pmask = gscores &gt; match_threshold   <span class="comment">#得分&gt;0.5的为正样本(掩码)</span></div><div class="line">        fpmask = tf.cast(pmask, dtype)</div><div class="line">        n_positives = tf.reduce_sum(fpmask) <span class="comment">#正样本数</span></div><div class="line"> </div><div class="line">        <span class="comment"># Hard negative mining...</span></div><div class="line">        no_classes = tf.cast(pmask, tf.int32)</div><div class="line">        predictions = slim.softmax(logits)</div><div class="line">        nmask = tf.logical_and(tf.logical_not(pmask), <span class="comment">#得分&gt;-0.5且&lt;=0.5的样本</span></div><div class="line">                               gscores &gt; <span class="number">-0.5</span>)</div><div class="line">        fnmask = tf.cast(nmask, dtype) </div><div class="line">        </div><div class="line">        <span class="comment">#得分&gt;-0.5且&lt;=0.5的样本在第0类（负样本）处的预测值（softmax）. </span></div><div class="line">        <span class="comment">#nvalues=[[p1],[p2],...,[pN]],N为一个batch中的anchors的总数,</span></div><div class="line">        <span class="comment">#满足score&gt;0.5的样本,pi=0</span></div><div class="line">        nvalues = tf.where(nmask,                     </div><div class="line">                           predictions[:, <span class="number">0</span>],</div><div class="line">                           <span class="number">1.</span> - fnmask)</div><div class="line">                           </div><div class="line">        <span class="comment">#nvalues_flat=[p1,p2,...,pN]</span></div><div class="line">        nvalues_flat = tf.reshape(nvalues, [<span class="number">-1</span>])</div><div class="line">        </div><div class="line">        <span class="comment"># Number of negative entries to select.</span></div><div class="line">        <span class="comment">#负样本数取满足-0.5&lt;score&lt;=0.5和3倍正样本数中的最小值（保证正负样本比例不小于1:3）</span></div><div class="line">        max_neg_entries = tf.cast(tf.reduce_sum(fnmask), tf.int32)</div><div class="line">        n_neg = tf.cast(negative_ratio * n_positives, tf.int32) + batch_size</div><div class="line">        n_neg = tf.minimum(n_neg, max_neg_entries)</div><div class="line"> </div><div class="line">        <span class="comment">#返回-nvalues_flat中最大的k(n_neg)值,和其索引(从0开始),即nvalues_flat中最小的k个值</span></div><div class="line">        val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg)</div><div class="line">        <span class="comment">#nvalues_flat中最小的k个值中的最大值,对应样本记为Max Negative Hard样本</span></div><div class="line">        max_hard_pred = -val[<span class="number">-1</span>]</div><div class="line">        <span class="comment"># Final negative mask.</span></div><div class="line">        <span class="comment">#最终负样本为置信度小于Max Negative Hard的所有样本</span></div><div class="line">        nmask = tf.logical_and(nmask, nvalues &lt; max_hard_pred)</div><div class="line">        fnmask = tf.cast(nmask, dtype)</div><div class="line"> </div><div class="line">        <span class="comment"># Add cross-entropy loss.</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entropy_pos'</span>):</div><div class="line">            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,</div><div class="line">                                                                  labels=gclasses)</div><div class="line">            </div><div class="line">            <span class="comment">#loss乘正样本掩码得符合条件的正样本损失；损失除以batch_size(这里为5)</span></div><div class="line">            loss = tf.div(tf.reduce_sum(loss * fpmask), batch_size, name=<span class="string">'value'</span>) </div><div class="line">            tf.losses.add_loss(loss)		<span class="comment">#将当前loss添加到总loss集合</span></div><div class="line"> </div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entropy_neg'</span>):</div><div class="line">            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,</div><div class="line">                                                                  labels=no_classes)</div><div class="line">            loss = tf.div(tf.reduce_sum(loss * fnmask), batch_size, name=<span class="string">'value'</span>)</div><div class="line">            tf.losses.add_loss(loss)		<span class="comment">#将当前loss添加到总loss集合</span></div><div class="line"> </div><div class="line">        <span class="comment"># Add localization loss: smooth L1, L2, ...</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'localization'</span>):</div><div class="line">            <span class="comment"># Weights Tensor: positive mask + random negative.</span></div><div class="line">            weights = tf.expand_dims(alpha * fpmask, axis=<span class="number">-1</span>) <span class="comment">#位置项损失权重α</span></div><div class="line">            loss = custom_layers.abs_smooth(localisations - glocalisations)</div><div class="line">            loss = tf.div(tf.reduce_sum(loss * weights), batch_size, name=<span class="string">'value'</span>)</div><div class="line">            </div><div class="line">            <span class="comment">#将当前loss添加到总loss集合，最后通过tf.losses.get_total_loss()计算所有的loss</span></div><div class="line">            tf.losses.add_loss(loss)</div></pre></td></tr></table></figure>
<h2 id="六、Data-Augmentation"><a href="#六、Data-Augmentation" class="headerlink" title="六、Data Augmentation"></a>六、Data Augmentation</h2><p>&emsp;&emsp;此处数据增强在tf中有很多辅助函数，而pytorch对ssd的实现中（<a href="https://github.com/amdegroot/ssd.pytorch" target="_blank" rel="external">amdegroot/ssd.pytorch</a>）数据增强的部分都是使用的自己写的函数，先读HSV颜色空间相关内容，占坑。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文阅读的版本是tensorflow对SSD的实现，相对而言阅读难度要远低于caffe版本的实现，源代码可见&lt;a href=&quot;https://github.com/balancap/SSD-Tensorflow/&quot; target=&quot;_blank&quot; r
    
    </summary>
    
      <category term="CV" scheme="https://mingming97.github.io/categories/CV/"/>
    
    
      <category term="CV" scheme="https://mingming97.github.io/tags/CV/"/>
    
      <category term="Object Detection" scheme="https://mingming97.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>word2vec理解</title>
    <link href="https://mingming97.github.io/2018/06/08/Word2Vec-basic/"/>
    <id>https://mingming97.github.io/2018/06/08/Word2Vec-basic/</id>
    <published>2018-06-08T02:14:01.000Z</published>
    <updated>2018-06-12T11:41:07.971Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文需要对n-gram模型在评估语句上的应用有一定了解（n-gram还有一些其他应用，如模糊匹配、特征工程等），可以看作对<a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="external">word2vec 中的数学原理详解</a>这篇博文的一些思考。</p>
<p>&emsp;&emsp;首先一定要建立起word2vec的基本认识，在NLP的一些任务中，我们需要将自然语言进行处理并发现其中的规律。但是机器没有办法直接理解自然语言，这就需要我们将其抽象化，换成一种机器能理解的表示方式。这就是word2vec的目的，将训练集中的词语表示成n维向量空间中的一个向量，而这个向量要对解决的问题有着尽可能好的特征表示。由此可以看出，word2vec与要解决的问题（也就是要训练的语言模型）是紧密相关的，所以一般情况下他们是一起进行训练的。例如keras中的Embedding层，官方解释是将字典下标转换为固定大小的向量。事实上这就是word2vec的一个过程，暂且不提keras是如何做的，要明确的一点是，Embedding层中的参数也是要随着训练的模型一起更新的，这有助于更好地理解之后的Skip-gram和CBOW模型。</p>
<h2 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h2><p>&emsp;&emsp;先看一下通用的语言模型，如下图所示。下面对这个图做出一些解释，这是一个三层语言模型，首先看输入，$w_{t-n+1},\dots,w_{t-2}, w_{t-1}$等是$w_t$之前的n-1个词，index的含义是他们在词典中的下标；再看输出，第i个输出是$P(w_t=i|context)$，其中context表示$w_t$的上下文，在这里表示输入的之前的n-1个词。所以这个模型就是计算了已知前n-1个词，而第n个词为i的条件概率。按照论文中的解释，这个网络有三层，如下：</p>
<ul>
<li><p>输入层（Input Layer）：负责将词向量输入，也就是$C(w_{t-n+1}),\dots,C(w_{t-2}),C(w_{t-1})$，每个用m维向量表示，共n-1个首尾相接，所以形成了一个m(n-1)维的向量。</p>
</li>
<li><p>隐含层（Hidden Layer）：设之前得到的m(n-1)维的向量为X，则隐含层的输出$Z = tanh(WX + p)$。其中W为该层权重，也就是要训练的参数，p是bias。输出使用tanh作为activation function，所以不用做smoothing。</p>
</li>
<li><p>输出层（Outpu Layer）：输出层的结点数与词典的大小相同，代表下一个词是词典中某一个词的概率。按照图中所说就是$O = softmax(UZ + q)$。其中U为该层权重，q是bias，softmax是activation function。</p>
</li>
</ul>
<p>  <img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/network.png" alt=""></p>
<p>  &emsp;&emsp;之后要介绍的网络都建立在这个三层模型的基础上，只是对一些流程做出了改动。</p>
<h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><p>&emsp;&emsp;这个模型要解决的问题其实就是在知道当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的情况下，预测$w_t$。也就是要计算$P(w_t|context(w_t))$。大体如下图所示：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/CBOW-Architecture.jpg" alt=""></p>
<p>&emsp;&emsp;对于其模型而言，去掉了之前的Hidden Layer，增加了一个Projection Layer，之前是将所有向量拼接，而现在是将所有向量求和，减少了参数数目，大体的模型如下。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/CBOWmodel.png" alt=""></p>
<ul>
<li><p>输入层：$Context(w)$中2c个词的词向量。</p>
</li>
<li><p>投影层：输入的2c个词向量的累加，即$x_w = \displaystyle\sum_{i=1}^{2c}v(Context(w)_i)$。</p>
</li>
<li><p>输出层：是一棵以词频为权值的Huffman树，这其中每一个叶子节点都代表一个单词，每一个非叶子节点都可以视作一次二分类。</p>
</li>
</ul>
<p>&emsp;&emsp;如果我们将其看作多分类问题，由于词典非常庞大，softmax的计算时间会线性增长，开销太大。所以此处采用了一种叫hierarchical softmax的技术，很形象，将输出层变成了一棵二叉树，并对每个非叶子节点做logisitic回归，目标节点就是我们训练集中的真实的词，也就是从根节点到目标层对应的路径就是每次分类正确的结果。</p>
<p>  &emsp;&emsp;如何更新参数呢。对于单个非叶子节点而言，它被分为正类的概率是$\sigma(x_w^T\theta)= \displaystyle\frac{1}{1+e^{-x_w^T\theta}}$，分为负类的概率是$1-\sigma(x_w^T\theta)$。在word2vec的源码中，将哈夫曼编码为1（向左走）视为负类，0（向右走）视为正类。不妨设到目标叶子节点共有s层，其中从根节点到目标叶子节点之前的非叶子节点的参数为$\theta_1,\theta_2,\dots,\theta_{s-1}$，目标叶子节点的哈夫曼编码为$d_2,d_3,\dots,d_s$，则可以得到</p>
<script type="math/tex; mode=display">
p(d_j|x_w, \theta_{j-1})=
\begin{cases}
\sigma(x_w^T\theta_{j-1}), &d_j = 0;\\
1-\sigma(x_w^T\theta_{j-1}), &d_j = 1,
\end{cases}</script><p>写成整体表达式就是</p>
<script type="math/tex; mode=display">
p(d_j|x_w, \theta_{j-1}) = \left[\sigma(x_w^T\theta_{j-1})\right]^{1-d_j}\cdot\left[1-\sigma(x_w^T\theta_{j-1})\right]^{d_j}</script><p>以上是一个结点的情况，如要求$p(w|Context(w))$将每个节点概率相乘即可，也就是</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\displaystyle\prod_{j=2}^{s}p(d_j|x_w, \theta_{j-1})</script><p>为了方便计算取对数，表示如下</p>
<script type="math/tex; mode=display">
\begin{align}
L &= \displaystyle\sum_{w\in C}log\prod_{j=2}^{s_w}\left[p(d_j^w|x_w, \theta_{j-1}^w)\right] \\
&=\displaystyle\sum_{w \in C}\sum_{j=2}^{s_w}\left\{(1-d_j^w)log\left[\sigma(x_w^T\theta_{j-1}^w)\right]+d_j^wlog\left[1-\sigma(x_w^T\theta_{j-1}^w)\right]\right\}
\end{align}</script><p>以上就是CBOW的目标函数，注意d和$\theta$都有上标的原因是表示他们是$w$相关的。由于我们的目标是将这个函数最大化，所以要用Gradient Ascent。对于我们的整个模型，需要训练的参数有$\theta_{j-1}^w,x_w$，分别对他们求梯度，由于是加法，又由于对一个固定的w和j，对$\theta_j^w$求偏导有贡献的项仅仅有花括号中的w和j相同的对应项，而对$x_w$而言有贡献的是$j\in[2, s_w]$部分的花括号中的内容的偏导和，为方便计算，将花括号中的内容记作T。</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial T}{\partial\theta_{j-1}^w} &= \displaystyle\frac{\partial}{\partial\theta_{j-1}^w}\left\{(1-d_j^w)log\left[\sigma(x_w^T\theta_{j-1}^w)\right]+d_j^wlog\left[1-\sigma(x_w^T\theta_{j-1}^w)\right]\right\}\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))\displaystyle\frac{\partial x_w^T\theta_{j-1}^w}{\partial\theta_{j-1}^w} + d_j^w\cdot(-\sigma(x_w^T\theta_{j-1}^w))\displaystyle\frac{\partial x_w^T\theta_{j-1}^w}{\partial\theta_{j-1}^w}\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))x_w - d_j^w\sigma(x_w^T\theta_{j-1}^w)x_w\ \ \ \ (矩阵求导分母布局)\\
&=\left[(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]x_w
\end{align}</script><p>对于$\displaystyle\frac{\partial T}{\partial x_w}$，与上面的过程类似，可以得到</p>
<script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial x_w} =\left[(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]\theta_{j-1}^w</script><p>显然，对于参数$\theta_{j-1}^w$的更新，很容易，只需要</p>
<script type="math/tex; mode=display">
\theta_{j-1}^w\leftarrow\theta_{j-1}^w+\eta\left[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]x_w</script><p>而对于$x_w$，它是通过v(w)累加来的，所以我们真正应该更新的是v(w)，word2vec的做法如下</p>
<script type="math/tex; mode=display">
v(w)\leftarrow v(w) + \eta\displaystyle\sum_{j=2}^{s_w}\left[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]\theta_{j-1}^w,\ \ \ \ w\in Context(w)</script><p>这样做当然有一定道理，因为我们仅仅求出了他们的一个集体梯度，所以要将这个集体梯度反馈到每一个个体上。</p>
<p>综上对于一个训练样本$(Context(w), w)$，CBOW伪代码如下</p>
<script type="math/tex; mode=display">
e\leftarrow0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ \ x_w\leftarrow\displaystyle\sum_{w\in Context(w)} v(w)\\
for \ j \leftarrow2\ to\ s_w\ do\\
\ \ q \leftarrow \sigma(x_w^T\theta_{j-1}^w)\\
\ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^w-q)\\
\ \ e\leftarrow e + g\theta_{j-1}^w\\
\ \ \ \ \ \ \ \ \ \theta_{j-1}^w\leftarrow \theta_{j-1}^w+gx_w\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ for \ u \in Context(w) do\\
\ \ \ \ \ \ v(u) \leftarrow v(u) + e</script><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><p>&emsp;&emsp;与CBOW相反，这个模型要解决的问题是在已知词$w_t$的情况下，预测词$w_{t-2}, w_{t-1},w_{t+1},w_{t+2}$。也就是要计算$P(Context(w_t)|w_t)$，如下图所示：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/Skip-Gram-Architecture-26.jpg" alt=""></p>
<p>&emsp;&emsp;对于其模型，与CBOW模型类似，为了与其对比，保留了projection layer，如下：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/word2vec/SKIPGRAMmodel.png" alt=""></p>
<ul>
<li><p>输入层：词向量$v(w)$</p>
</li>
<li><p>投影层：为了与CBOW保持类似保留的，无实际作用。</p>
</li>
<li><p>输出层：一棵Huffman树。与CBOW类似，也是为了利用Hierarchical softmax，所输出的一棵以单词为叶子节点，非叶子节点作为二分类的单元的哈夫曼树。</p>
</li>
</ul>
<p>如何表示$P(Context(w)|w)$呢，可以将其拆解如下：</p>
<script type="math/tex; mode=display">
p(Context(w)|w) = \displaystyle\prod_{u \in Context(w)}p(u|w)</script><p>此处的$p(u|w)$可以借助CBOW的表示方式，表示为：</p>
<script type="math/tex; mode=display">
p(u|w) = \displaystyle\prod_{j=2}^{s}p(d_j|v_w, \theta_{j-1})</script><p>此处的$\theta,d_j$在总体表示中，同样是与语料中的$u$是相关的，将p写作二分类的形式后再取对数（与CBOW中操作相似），得到总体的目标函数</p>
<script type="math/tex; mode=display">
\begin{align}
L&= \displaystyle\sum_{w\in C}log\prod_{u\in Context(w)}\prod_{j=2}^{s_u}\left[p(d_j^u|v_w,\theta_{j-1}^u)\right]\\
&= \displaystyle\sum_{w \in C}\sum_{u\in Context(w)}\sum_{j=2}^{s_u} \left\{(1-d_j^u)log\left[\sigma(v_w^T\theta_{j-1}^u)\right]+d_j^ulog\left[1-\sigma(v_w^T\theta_{j-1}^u)\right]\right\}
\end{align}</script><p>出于与CBOW中同样的考虑，可将后面花括号中的内容记为T，与CBOW中推导过程完全相同，得到下面的结果：</p>
<script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial\theta_{j-1}^u} =\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]v_w</script><script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial v_w}=\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]\theta_{j-1}^u</script><p> 参数更新也是类似的（此处也是要求目标函数最大值）</p>
<script type="math/tex; mode=display">
\theta_{j-1}^u \leftarrow \theta_{j-1}^u + \eta\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]v_w</script><script type="math/tex; mode=display">
v_w\leftarrow v_w + \eta\displaystyle\sum_{u\in Context(w)}\sum_{j=2}^{s_u}\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]\theta_{j-1}^u</script><p>综上对于一个训练样本$(w, Context(w))$，Skip-gram伪代码如下</p>
<script type="math/tex; mode=display">
e\leftarrow 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
for\ u \in Context(w)\ do \\
for \ j \leftarrow2\ to\ s_u\ do\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q \leftarrow \sigma\left[v(w)^T\theta_{j-1}^u\right]\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^u-q)\\
\ \ \ \ \ \ \ \ e\leftarrow e + g\theta_{j-1}^u\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta_{j-1}^u\leftarrow \theta_{j-1}^u+gv(w)\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
v(w) \leftarrow v(w)+e\ \ \ \ \ \ \ \ \ \ \ \</script><p>但是在word2vec中，真实的做法是每处理一个$Context(w)$中的词，就进行一次更新，也就是如下的步骤：</p>
<script type="math/tex; mode=display">
for\ u \in Context(w)\ do \\
e\leftarrow 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
for \ j \leftarrow2\ to\ s_u\ do\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q \leftarrow \sigma\left[v(w)^T\theta_{j-1}^u\right]\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^u-q)\\
\ \ \ \ \ \ \ \ e\leftarrow e + g\theta_{j-1}^u\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta_{j-1}^u\leftarrow \theta_{j-1}^u+gv(w)\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
v(w) \leftarrow v(w)+e\ \ \ \\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\</script><p>&emsp;&emsp;以上两种就是基于Hierarchical softmax的CBOW和Skip-gram模型，还有基于Negative Sampling的两种模型，在这里暂且不提。如果想得知更多细节，可以在源码中获得，我个人只看过java版本的，没看过纯C版本的，尽管语言不同，但我想在一些处理细节上是相同的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文需要对n-gram模型在评估语句上的应用有一定了解（n-gram还有一些其他应用，如模糊匹配、特征工程等），可以看作对&lt;a href=&quot;https://blog.csdn.net/itplus/article/details/37969519&quot; 
    
    </summary>
    
      <category term="NLP" scheme="https://mingming97.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://mingming97.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>中文分词</title>
    <link href="https://mingming97.github.io/2018/05/17/chinese-split-word/"/>
    <id>https://mingming97.github.io/2018/05/17/chinese-split-word/</id>
    <published>2018-05-17T09:40:01.000Z</published>
    <updated>2018-05-18T09:51:04.149Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p>&emsp;&emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有三个要素：A是状态转移概率分布矩阵，简单说就是在任一时刻从一个隐含状态到另一个隐含状态的转移概率构成的矩阵；B是观测概率分布矩阵，其实就是在任一时刻给定隐含状态s生成观测状态o的条件概率$P(o|s)$构成的矩阵；$\pi$是初始概率矩阵，也就是在初始状态下各隐含状态的概率。而一般的HMM模型有三个基本问题：1. 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，计算$P(O|\lambda)$，这是评估问题。2. 给定观测序列$O = \{o_1, o_2, \dots, o_t\}$，求解模型$\lambda = (A, B, \pi$，使得$P(O|\lambda)$尽可能大，这是学习问题，若给定隐含状态序列S可以考虑用maximum likelihood来解决，若隐含状态序列则可以用Baum-Welch算法解决，不过这并不是本文重点。3. 给定 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，求使得$P(S|O)$最大的隐含状态序列$S = \{s_1, s_2, \dots, s_t\}$，这被称为解码问题或预测问题。对于分词这个任务来说，主要涉及到的是第三个问题。<br>&emsp;&emsp;jieba分词的源码就提供了解决这个问题的一个很好的范例。将隐含状态集合定义为$\{S, B, M, E\}$，S的含义是单字，B的含义是词头，M的含义是词中，E的含义是词尾。在 <code>jieba/finaseg/prob_start.py</code>中定义了初始概率$\pi$，在 <code>jieba/finaseg/prob_trans.py</code>中定义了状态转移概率$A$，在<code>jieba/finaseg/prob_emit.py</code>中定义了状态观测概率分布$B$，在用基于统计的方法获得以上这些之后就用Viterbi算法求一条使得$P(S|O)$最大的路径就好。（关于Viterbi算法还是在另篇文章中再说。）<br>&emsp;&emsp;考虑一下，此时如果我们不知道B，该如何定义要求解的函数。可以试着模仿Viterbi的想法，用$\delta_i(s)$表示到第i个字时状态为s时的最优值，则$\delta_{i+1}(s’) = max\{\delta_i(s)a_{ss’}P(s|i),  s\in\{S, B, M, E\}\}$，其中$a_{ss’}$是转移概率，$P(s|i)$表示第i个字状态是s的概率（这样定义是有着一定数学原理的，具体推导也借鉴了Viterbi算法原本的定义，核心思想是极大似然）。转移概率可通过统计的方法得到，那么$P(s|i)$呢？影响到这个概率的因素很多，不妨将这个问题转化为一个seq2seq的问题，输入一个序列，输出各个位置的4-tag标注。中文中通过前后文语境都能作为序列标注的依据，从而考虑使用Bi-directional的LSTM来进行这个任务。只要将输出接一层softmax就可以将结果当作概率使用。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>&emsp;&emsp;有一个经典的亚研院的语料库就是4-tag标注的，大概长这个样子。</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/msrtrain.png" alt=""></p>
<p>&emsp;&emsp;首先要明确我们训练数据的lstm的输入，应该是batch_size * sentence_len的一个tensor。以标点符号为分隔，汉语中单句话一般没有太长，所以此处统一每个句向量的长度为32。获得句向量的方式很简单，将文章中所有出现的字做成一个字典，将每个字用其在字典中对应的下位置表示，不足的长度用0补齐，就得到了句向量的表示。而对于tag，则可以使用one-hot的编码，注意要为补足句向量的0留一个编码位置，所以一共有5类tag。因此一句话中tag为s的字将被编码为[1, 0, 0, 0, 0]，tag为b的将被编码为[0, 1, 0, 0, 0]，以此类推。<br>&emsp;&emsp;明确了以上过程，就可以开始细节上的处理。注意到每行前面有很多“/s，”/s，‘/s，’/s一类的东西，我们的数据是不需要这类东西的，可以用正则处理掉。处理掉之后单看每行，可以方便的使用python的re.findall提取出来一个字和标签的元组组成的列表。再将二者分别处理，分别得到句子和标签。<br>&emsp;&emsp;通过标签得到独热编码的过程值得记录一下。面对的问题就是给出了一个类别的列表，如何得到独热编码。可以利用numpy的花式索引方便进行。代码如下：（虽然花式索引返回的是数据的拷贝，但是使用花式索引进行赋值却是在原数组上进行操作的）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">data = np.array(data)	<span class="comment"># 获得numpy的数组</span></div><div class="line">one_hot = np.zeros((len(data), <span class="number">5</span>), dtype=np.float64)	<span class="comment"># 获得len(data)个长度为5的0向量</span></div><div class="line">one_hot[range(len(data)), data] = <span class="number">1</span>		<span class="comment"># 二维花式索引 可以实现任意位置的操作</span></div><div class="line">default = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>], dtype=np.float64)	<span class="comment"># 定义默认向量</span></div><div class="line">default = np.tile(default, (<span class="number">32</span> - len(one_hot), <span class="number">1</span>))	<span class="comment"># 将默认向量在列方向上重复32-len(one_hot)次</span></div><div class="line">one_hot = np.concatenate((one_hot, default), axis = <span class="number">0</span>)	<span class="comment"># 将默认部分与有效部分拼接保证长度与句向量相同</span></div></pre></td></tr></table></figure></p>
<h2 id="模型定义及训练"><a href="#模型定义及训练" class="headerlink" title="模型定义及训练"></a>模型定义及训练</h2><p>&emsp;&emsp;接下类就是定义一个双向LSTM的过程，为了简单起见，使用了keras进行，keras中还有embedding层，正好适用于我们生成的句向量。keras封装度比较高，所以代码较短，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">inputs = Input(shape=(sent_len,), dtype=<span class="string">'int32'</span>)</div><div class="line">embedding = Embedding(len(charsets) + <span class="number">1</span>, word_size, input_length=sent_len, mask_zero=<span class="keyword">True</span>)(inputs)</div><div class="line">bilstm_layer = Bidirectional(LSTM(<span class="number">64</span>, return_sequences=<span class="keyword">True</span>), merge_mode=<span class="string">'sum'</span>)(embedding)</div><div class="line">output = TimeDistributed(Dense(<span class="number">5</span>, activation=<span class="string">'softmax'</span>))(bilstm_layer)</div><div class="line">model = Model(input=inputs, output=output) model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">json.dump(model.to_json(), open(os.path.join(filedir, model_name), <span class="string">"w"</span>))</div><div class="line">model.fit(x, y, batch_size=batch_size, epochs=epochs)</div><div class="line">model.save_weights(os.path.join(filedir, model_weights_name))</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;训练好了模型，输入一句话，就能给出各个位置对应各种标签的概率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sentence_embeddings, all_sentences, sentence_len = data.get_sent_embeddings(charsets, test_sentence)</div><div class="line">result = model.predict(sentence_embeddings, verbose=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<h2 id="计算概率最大路径"><a href="#计算概率最大路径" class="headerlink" title="计算概率最大路径"></a>计算概率最大路径</h2><p>&emsp;&emsp;根据之前对$\delta_i(s)$的定义，使用dp就可以计算概率最大的路径了。此处转移概率用的是相等概率，dp算法的具体实现上利用了python字典的性质，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(nodes)</span>:</span></div><div class="line">    path = &#123;<span class="string">'b'</span>: nodes[<span class="number">0</span>][<span class="string">'b'</span>], <span class="string">'s'</span>: nodes[<span class="number">0</span>][<span class="string">'s'</span>]&#125;</div><div class="line">    <span class="keyword">for</span> layer_num <span class="keyword">in</span> range(<span class="number">1</span>, len(nodes)):</div><div class="line">        old_path = path.copy()</div><div class="line">        path = &#123;&#125;</div><div class="line">        <span class="keyword">for</span> new_tag <span class="keyword">in</span> nodes[layer_num].keys():</div><div class="line">            tmp = &#123;&#125;</div><div class="line">            <span class="keyword">if</span> layer_num == len(nodes) - <span class="number">1</span>:</div><div class="line">                <span class="keyword">if</span> new_tag <span class="keyword">in</span> [<span class="string">"m"</span>, <span class="string">"b"</span>]:</div><div class="line">                    <span class="keyword">continue</span></div><div class="line">            <span class="keyword">for</span> old_path_tag <span class="keyword">in</span> old_path.keys():</div><div class="line">                <span class="keyword">if</span> old_path_tag[<span class="number">-1</span>]+new_tag <span class="keyword">in</span> transpose_matrix.keys():</div><div class="line">                    tmp[old_path_tag+new_tag] = old_path[old_path_tag] + \</div><div class="line">                                                nodes[layer_num][new_tag] + \</div><div class="line">                                                transpose_matrix[old_path_tag[<span class="number">-1</span>]+new_tag]</div><div class="line">            k = np.argmax(list(tmp.values()))</div><div class="line">            path[list(tmp.keys())[k]] = list(tmp.values())[k]</div><div class="line">    <span class="keyword">return</span> list(path.keys())[np.argmax(list(path.values()))]</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过以上几个步骤，就完成了对一句话4-tag的标注。自己试了几句话，效果还不错，见图：</p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/result1.png" alt=""></p>
<p><img src="http://ov718qcsg.bkt.clouddn.com/blog/chinesesplitwords/result2.png" alt=""></p>
<p>完整代码可以参照我的<a href="https://github.com/mingming97/Chinese-Word-Split" target="_blank" rel="external">Github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本思路&quot;&gt;&lt;a href=&quot;#基本思路&quot; class=&quot;headerlink&quot; title=&quot;基本思路&quot;&gt;&lt;/a&gt;基本思路&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有
    
    </summary>
    
      <category term="NLP" scheme="https://mingming97.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://mingming97.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>hexo配置LaTeX公式</title>
    <link href="https://mingming97.github.io/2018/05/15/hexo-latex/"/>
    <id>https://mingming97.github.io/2018/05/15/hexo-latex/</id>
    <published>2018-05-15T13:12:18.000Z</published>
    <updated>2018-05-18T10:11:23.240Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎 <code>hexo-renderer-marked</code>更换为<code>hexo-renderer-kramed</code>来渲染markdown。<br>&emsp;&emsp;首先要将之前的<code>hexo-renderer-marked</code>卸载，并安装<code>hexo-renderer-kramed</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm uninstall hexo-renderer-marked --save</div><div class="line">npm install hexo-renderer-kramed --save</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;在这之后建议在hexo的根目录下找到<code>package.json</code>文件，用文本编辑器打开它，删除字符串<code>hexo-renderer-marked</code>所在的一行并保存。之所以不直接卸载<code>hexo-renderer-marked</code>，是因为其他重要包极有可能在卸载该包的同时被删除。<br>&emsp;&emsp;不要忘了行内公式的转义字符，打开<code>./node_modules/kramed/lib/rules</code>，并修改<code>inline.js</code>文件的11和20行，分别修改为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</div><div class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;每次在写文章前，要在YAML font-matter中添加<code>mathjax: true</code>，这样便能确保启动mathjax引擎进行渲染了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎 &lt;code&gt;hexo-renderer-marked&lt;/code&gt;更换为&lt;code&gt;hexo-renderer-kramed&lt;/code&gt;来渲染markdown。&lt;
    
    </summary>
    
      <category term="hexo" scheme="https://mingming97.github.io/categories/hexo/"/>
    
    
      <category term="LaTeX" scheme="https://mingming97.github.io/tags/LaTeX/"/>
    
      <category term="hexo" scheme="https://mingming97.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>LIS的O(nlogn)实现原理</title>
    <link href="https://mingming97.github.io/2017/08/30/LIS-O-nlogn/"/>
    <id>https://mingming97.github.io/2017/08/30/LIS-O-nlogn/</id>
    <published>2017-08-30T08:07:39.000Z</published>
    <updated>2018-05-17T16:08:41.703Z</updated>
    
    <content type="html"><![CDATA[<p>　　朴素的LIS的<strong>O(n²)</strong>算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为<strong>d[i] = max{d[j] | j &lt; i &amp;&amp; a[j] &lt; a[i]} + 1</strong>。显然，对于一个i下的两个不同决策<strong>j，k (j,k &lt; i)，若a[j] &lt; a[k]，d[j] &gt;= d[k]，而a[i] &gt; a[k]时，k显然没有j决策更优</strong>。通过这种思想，我们发现决策有一定的单调性，从而其中可以用二分查找来降低时间复杂度。</p>
<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>　　我们可以维护一个决策序列B，用<strong>B[i]</strong>表示子序列长度为i时的最小末尾(a中的值)。可以看出如果<strong>i &gt; j</strong>，一定有<strong>B[i] &gt;= B[j]</strong>。原因很简单，看B的定义就可以知道。由上可知B是单调的序列，对于其中的的值可以进行二分查找。</p>
<p>　　所以可以按顺序枚举<strong>a[i]</strong>，如果<strong>a[i]</strong>的值比B中的最大值要大，则将<strong>a[i]</strong>放入<strong>B[i]</strong>的尾部，最大长度加1; 否则对B进行二分查找以找到<strong>a[i]</strong>该放入的位置，<strong>pos = min{j | B[j] &gt; a[i]}</strong>，用a[i]代替B[pos]，使a[i]成为子序列长度为pos时的最小值。大致过程如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (a[i] &gt; B[len<span class="number">-1</span>]) &#123;</div><div class="line">        B[len] = a[i];</div><div class="line">        len++;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">        pos = binarySearch(len, a[i]);</div><div class="line">        B[pos] = a[i];</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>完整代码请戳：<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/LIS-nlogn.cpp" target="_blank" rel="external">ljm’s github</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　朴素的LIS的&lt;strong&gt;O(n²)&lt;/strong&gt;算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为&lt;strong&gt;d[i] = max{d[j] | j &amp;lt; i &amp;amp;&amp;amp; a[j] &amp;lt; a[i]} +
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="dp" scheme="https://mingming97.github.io/tags/dp/"/>
    
  </entry>
  
  <entry>
    <title>并查集</title>
    <link href="https://mingming97.github.io/2017/08/28/Union-Find/"/>
    <id>https://mingming97.github.io/2017/08/28/Union-Find/</id>
    <published>2017-08-28T09:41:39.000Z</published>
    <updated>2018-05-17T16:08:39.771Z</updated>
    
    <content type="html"><![CDATA[<p>　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最快呢。并查集是一种轻量级的数据结构，其本质是一种集合，支持不相交集合的“并”和“查”的操作。由于在特定状况下，可以通过某些特殊处理将这两种操作的时间复杂度降至很低，所以在某些算法，例如tarjan求lca的实现中，也引入了这一数据结构。</p>
<h2 id="基本实现"><a href="#基本实现" class="headerlink" title="基本实现"></a>基本实现</h2><p>　　其实并查集的本质就是一个划分，商集中的每一个元素（集合）内的元素都可以建一棵树，放在一起就变成了森林。对于任意两个个元素，只需要看他们的树根是否相等，就可以判断他们是否属于同一集合；至于两个集合的合并，只需要将一棵树的树根指向另一棵树的树根。那么，可以用<strong>pre[i]</strong>记录i这个结点的父节点，初始化时只需令<strong>pre[i] = i</strong>，意味着每一个结点都属于一个单独的集合，也就是每一个节点都是树根。如果想查找树根怎么办呢？ 按照定义<strong>pre[i]</strong>是i的父亲，<strong>pre[pre[i]]</strong>就是i结点父亲的父亲，按照这个方式不断向上找直至<strong>pre[i]</strong>等于i，就找到了树根，可以使用递归来实现。如果想合并两个集合怎么办呢，只需要找到两个元素所在树的树根，将一个树根指向另一个，就完成了。</p>
<h2 id="时间复杂度优化"><a href="#时间复杂度优化" class="headerlink" title="时间复杂度优化"></a>时间复杂度优化</h2><h3 id="启发式合并"><a href="#启发式合并" class="headerlink" title="启发式合并"></a>启发式合并</h3><p>　　为了解决合并时树退化成链的情况，在合并时我们可以根据两棵树的深度合并，将最大深度小的向最大深度大的合并。如果两棵树的深度一样，则随便选一个作为根，并将根的最大深度+1。这样做的话在n次操作后，任何一棵树（一个集合）的深度最大不会超过<strong>[logn]+1</strong>，从而使查找的时间复杂度降为$O(logn)$。代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> rx = find(x), ry = find(y);</div><div class="line">    <span class="keyword">if</span> (rx != ry) &#123;</div><div class="line">        <span class="keyword">if</span> (rank[rx] == rank[ry]) &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">            rank[rx]++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (rank[rx] &lt; rank[ry]) &#123;</div><div class="line">            pre[rx] = ry;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="路径压缩"><a href="#路径压缩" class="headerlink" title="路径压缩"></a>路径压缩</h3><p>　　一般情况下我们只需要知道知道一个元素所在的树的根就可以，所以可以在查找元素的过程中，把路径上的所有子节点直接指向根节点，这样就可以将查找的复杂度降至O(1)。代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> x == pre[x] ? x : pre[x] = find(pre[x]);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="应用时的问题"><a href="#应用时的问题" class="headerlink" title="应用时的问题"></a>应用时的问题</h2><p>　　在具体问题中，两棵树是否可以按秩合并，以及是否可以路径压缩是需要按照题的要求来定的，本文只是提出了大体框架及基本实现（可移步<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/union-find.cpp" target="_blank" rel="external">我的github</a>查看）。具体节点信息的维护或是节点合并顺序等还需要自己判断。
　  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="并查集" scheme="https://mingming97.github.io/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/"/>
    
      <category term="数据结构" scheme="https://mingming97.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
