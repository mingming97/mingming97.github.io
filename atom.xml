<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ljm&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://mingming97.github.io/"/>
  <updated>2019-03-30T12:05:29.056Z</updated>
  <id>https://mingming97.github.io/</id>
  
  <author>
    <name>ljm</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mmdetection中的retinanet</title>
    <link href="https://mingming97.github.io/2019/03/29/mmdetection%20retinanet/"/>
    <id>https://mingming97.github.io/2019/03/29/mmdetection retinanet/</id>
    <published>2019-03-29T06:13:00.000Z</published>
    <updated>2019-03-30T12:05:29.056Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、大体流程"><a href="#一、大体流程" class="headerlink" title="一、大体流程"></a>一、大体流程</h2><h3 id="1-训练逻辑"><a href="#1-训练逻辑" class="headerlink" title="1.  训练逻辑"></a>1.  训练逻辑</h3><p>&emsp;&emsp;mmdetection的训练逻辑是借助了mmcv中的Runner类，做了一层封装。按照正常的逻辑思考，如果我们要训练一个网络，至少要知道输入数据、模型、优化器、loss等信息，这些信息在<code>tools/train.py</code>以及<code>mmdet/apis/train.py</code>中都有对应的体现，例如在runner构建时传入的model、batch_processor以及optimizer，就分别对应了模型、loss、以及优化器，而runner.run调用时传入的data_loaders则对应了输入数据的部分。</p>
<p>&emsp;&emsp;值得注意的是batch_processor，它所做的其实就是通过喂入数据进行前馈计算得到loss，然后返回一个对应的记录了各种loss信息以及数据信息的字典，而这个返回的loss信息的处理则调用了parse_losses函数。这个函数所做的其实就是将一个字典中所有的loss字段分别记录，最后相加得到最终的loss。知道了这个过程，就可以知道，在计算loss的时候就已经要乘上每一项对应的系数，返回的时候也要返回一个对应不同类loss名称的字典。</p>
<h3 id="2-技术细节"><a href="#2-技术细节" class="headerlink" title="2. 技术细节"></a>2. 技术细节</h3><p>&emsp;&emsp;此部分可以参照<a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="external">mmdetection</a>的<a href="https://github.com/open-mmlab/mmdetection/blob/master/TECHNICAL_DETAILS.md" target="_blank" rel="external">technical details</a>中的内容。在mmdetection中，构成model的有四类组件</p>
<ul>
<li><p>backbone：一般用于feature map的提取，例如resnet，vgg。</p>
</li>
<li><p>neck：在feature map和head之间的网络，例如FPN。</p>
</li>
<li><p>head：用于具体任务的网络，例如bbox regression、mask prediction。</p>
</li>
<li><p>roi extractor：从feature map进一步提取特征的部分，例如RoI Align。</p>
</li>
</ul>
<p>&emsp;&emsp;而把这些部分组合起来的就是detector，mmdetection中有两个典型的detector，一个是<code>SingleStageDetector</code>，一个是<code>TwoStageDetector</code>。一般一个detector中要实现四个抽象方法</p>
<ul>
<li><p><code>extract_feat()</code>：给出一个batch的图片，tensor的shape是(n, c, h, w)，提取出feature map。</p>
</li>
<li><p><code>forward_train()</code>：前馈计算得到loss。</p>
</li>
<li><p><code>simple_test()</code>：单个scale图片的测试模式。</p>
</li>
<li><p><code>aug_test()</code>：带有数据增强的测试模式。</p>
</li>
</ul>
<p>&emsp;&emsp;下面将围绕retinanet涉及到的组件进行讲解。</p>
<h2 id="二、网络结构"><a href="#二、网络结构" class="headerlink" title="二、网络结构"></a>二、网络结构</h2><h3 id="1-backbone"><a href="#1-backbone" class="headerlink" title="1. backbone"></a>1. backbone</h3><p>&emsp;&emsp;retinanet用到的backbone有res50，res101两种，当然还有ResNeXt等较新的网络。以res50为例，其有关backbone配置的部分如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">backbone=dict(</div><div class="line">    type=<span class="string">'ResNet'</span>,</div><div class="line">    depth=<span class="number">50</span>,</div><div class="line">    num_stages=<span class="number">4</span>,</div><div class="line">    out_indices=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),</div><div class="line">    frozen_stages=<span class="number">1</span>,</div><div class="line">    style=<span class="string">'pytorch'</span>)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;其中depth，num_stages都是resnet中常见的配置，out_indices也是指输出的特征图，与stage对应，此处代表C2，C3，C4，C5。而C2，C3，C4，C5分别对应resnet中第conv2_x、conv3_x、conv4_x、conv5_x块的输出，见下图</p>
<p><img src="https://i.loli.net/2019/03/30/5c9f5b3acd220.png" alt=""></p>
<p>&emsp;&emsp;有一些关于resnet的细节。conv3_x、conv4_x、conv5_x的第一个block中会有一个stride为2的卷积用于减小特征图大小，在caffe的实现中，是在第一个1$\times$1的卷积处，而pytorch的实现中是在中间3$\times$3的卷积处，在mmdetection的代码中都有对应体现。另一个就是<code>frozen_stages=1</code>，由于要对resnet做finetune，所以要冻结一部分浅层的参数，此处默认冻结conv1，而<code>fronzen_stages</code>就是控制<code>frozen_stages</code>之前所有stage的卷积块都会被冻结，在这里也就是冻结第一个stage，conv2_x。除此之外还有一点就是冻结了网络中所有的<code>BN</code>层，因为batch数目太小了，加<code>BN</code>没有意义。</p>
<p>&emsp;&emsp;从代码可以看出，需要保存的特征图放在了一个list中。在retinanet里，backbone的输出就是四个特征图，[C2, C3, C4, C5]。</p>
<h3 id="2-neck"><a href="#2-neck" class="headerlink" title="2. neck"></a>2. neck</h3><p>&emsp;&emsp;neck部分使用的是FPN。configs中相关配置如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">neck=dict(</div><div class="line">    type=<span class="string">'FPN'</span>,</div><div class="line">    in_channels=[<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],</div><div class="line">    out_channels=<span class="number">256</span>,</div><div class="line">    start_level=<span class="number">1</span>,</div><div class="line">    add_extra_convs=<span class="keyword">True</span>,</div><div class="line">    num_outs=<span class="number">5</span>)</div></pre></td></tr></table></figure>
<ul>
<li>in_channels：输入特征图的channel数。</li>
<li>out_channels：输出特征图的channel数。</li>
<li>start_level：起始特征图的层数，例如start_level=1，意思就是不会使用C2，lateral connection只需要连接C3、C4、C5。</li>
<li>add_extra_convs：添加额外卷积层，在retinanet中是用于生成P6和P7的conv。</li>
<li>num_outs：输出特征图个数，在retinanet中是P3、P4、P5、P6、P7。</li>
</ul>
<p>整体bottom-up以及top-down过程如下图所示</p>
<p><img src="https://i.loli.net/2019/03/30/5c9f5b3a65c35.png" alt=""></p>
<p>在经过neck后，会有五个尺度的特征图，同样保存在了一个list中，输出是[P3, P4, P5, P6, P6]。</p>
<h3 id="3-head"><a href="#3-head" class="headerlink" title="3. head"></a>3. head</h3><h4 id="（1）retinanet的head部分概述"><a href="#（1）retinanet的head部分概述" class="headerlink" title="（1）retinanet的head部分概述"></a>（1）retinanet的head部分概述</h4><p>&emsp;&emsp;在看这一部分之前，需要先了解一个重要的函数，这个函数屡次被用到，那就是<code>mmdet/core/utils/misc.py</code>中的<code>multi_apply()</code>函数，代码如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_apply</span><span class="params">(func, *args, **kwargs)</span>:</span></div><div class="line">    pfunc = partial(func, **kwargs) <span class="keyword">if</span> kwargs <span class="keyword">else</span> func</div><div class="line">    map_results = map(pfunc, *args)</div><div class="line">    <span class="keyword">return</span> tuple(map(list, zip(*map_results)))</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个函数的作用其实就是将多个序列中的每一组元素都通过func函数，再将所得结果转置后返回。这样解释可能比较抽象，举一个例子：假如有两个列表list1，list2，我们要计算这两个列表的element-wise sum和element-wise product，我们可以通过一个函数同时返回两个数的和和差，如<code>lambda x, y: (x + y, x * y)</code>。再使用map函数，也就是：<code>map(lambda x, y:(x + y, x * y), list1, list2)</code>，但是这样的结果是按照<code>[(sum, product), (sum, product), ...]</code>这样的形式组织的，所以要将它们转置，这样才能让结果中的和在一个列表中，差在一个列表中。</p>
<p>&emsp;&emsp;而用到这个函数则涉及到一个设计思想，那就是将问题按照不同的角度去分解。无论是在<code>mmdet/models/anchor_heads/anchor_head.py</code>还是在<code>mmdetection/mmdet/core/anchor/anchor_target</code>中，都能看到很多<code>_single()</code>结尾的函数，这样的函数解决的就是分解后的一个小问题。而将一个列表中每一个元素经过<code>multi_apply()</code>函数，再将结果组合起来，就得到了一个大问题的结果。具体到retinanet中，主要分解的角度有两个，一个是图片，另一个是特征图的尺度。这个角度的意思其实就是说在流程进行中，涉及到的数据的第一个维度的含义，第一种是图片数目，也就是一个batch中图片的数目作为第一个维度；第二种是特征图尺度的数目，在retinanet中有五个特征图，也就是第一个维度等于5。</p>
<p>&emsp;&emsp;需要逐图片解决的就是每个图片有关anchor的计算，比如anchor的assign，训练样本的sample，和label的获得等。而这一步解决后返回的结果需要是按特征图大小作为第一维度的，因为在<code>mmdet/models/retina_head.py</code>中的RetinaHead类完成了类别得分<code>cls_scores</code>，以及回归预测结果<code>bbox_preds</code>的计算，使用了<code>forward_single()</code>函数，这个函数是在RetinaHead的父类AnchorHead中被调用的，它的<code>forward()</code>只有一句话，那就是<code>return multi_apply(self.forward_single, feats)</code>，也就是说得到的<code>cls_scores</code>的shape是<code>[feat_size_num, batch_size, cls_num*A, N, M]</code>，<code>bbox_preds</code>的shape是<code>[feat_size_num, batch_size, 4*A, N, M]</code>（A是同一个中心不同大小不同长宽比的anchor数）。这两部分都不是严格的Tensor，因为不同feat_size下的N和M不同，因此需要将feat_size_num个Tensor放在一个list中。所以在计算的时候，要逐个feat_size进行计算，也就是<code>loss_single()</code>所完成的计算。</p>
<p>&emsp;&emsp;<code>RetinaHead</code>同样涉及到配置字典，下面是具体参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">bbox_head=dict(</div><div class="line">    type=<span class="string">'RetinaHead'</span>,</div><div class="line">    num_classes=<span class="number">81</span>,</div><div class="line">    in_channels=<span class="number">256</span>,</div><div class="line">    stacked_convs=<span class="number">4</span>,</div><div class="line">    feat_channels=<span class="number">256</span>,</div><div class="line">    octave_base_scale=<span class="number">4</span>,</div><div class="line">    scales_per_octave=<span class="number">3</span>,</div><div class="line">    anchor_ratios=[<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],</div><div class="line">    anchor_strides=[<span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>],</div><div class="line">    target_means=[<span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>],</div><div class="line">    target_stds=[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]))</div></pre></td></tr></table></figure>
<ul>
<li>num_classes：类别数，此处是默认算上背景的类别数，如果使用sigmoid分类，那样one hot向量全为0就可以代表背景类，就会将num_classes-1，具体代码可以参照<code>mmdet/models/anchor_heads/anchor_head.py</code>中<code>AnchorHead</code>类的<code>__init__()</code>方法，第65到68行。</li>
<li>in_channels：输入的特征图的channel数</li>
<li>stacked_convs：在分类分支和回归分支中堆叠起来的conv层数。</li>
<li>feat_channels：在堆叠起来的conv层中，特征的channel数</li>
<li>octave_base_scale：用于计算anchor_scales的参数，具体在下面解释</li>
<li>scales_per_octave：同上</li>
<li>anchor_ratios：anchor的aspect ratio</li>
<li>anchor_strides：在另一篇博文中有解释，可参照<a href="https://mingming97.github.io/2019/03/26/anchor%20in%20object%20detection">这篇博文</a></li>
<li>target_means，target_stds：在rpn中有用到，用于测试阶段proposals的获得。retinanet不涉及。</li>
</ul>
<h4 id="（2）cls-scores以及bbox-preds的计算"><a href="#（2）cls-scores以及bbox-preds的计算" class="headerlink" title="（2）cls_scores以及bbox_preds的计算"></a>（2）cls_scores以及bbox_preds的计算</h4><p>&emsp;&emsp;这一部分可以参照网络图来看<code>RetinaHead</code>中的代码</p>
<p><img src="https://i.loli.net/2019/03/30/5c9f5b3a6df3b.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_init_layers</span><span class="params">(self)</span>:</span></div><div class="line">    self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line">    self.cls_convs = nn.ModuleList()</div><div class="line">    self.reg_convs = nn.ModuleList()</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.stacked_convs):</div><div class="line">        chn = self.in_channels <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> self.feat_channels</div><div class="line">        self.cls_convs.append(</div><div class="line">            nn.Conv2d(chn, self.feat_channels, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</div><div class="line">        self.reg_convs.append(</div><div class="line">            nn.Conv2d(chn, self.feat_channels, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</div><div class="line">    self.retina_cls = nn.Conv2d(</div><div class="line">        self.feat_channels,</div><div class="line">        self.num_anchors * self.cls_out_channels,</div><div class="line">        <span class="number">3</span>,</div><div class="line">        padding=<span class="number">1</span>)</div><div class="line">    self.retina_reg = nn.Conv2d(</div><div class="line">        self.feat_channels, self.num_anchors * <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_single</span><span class="params">(self, x)</span>:</span></div><div class="line">    cls_feat = x</div><div class="line">    reg_feat = x</div><div class="line">    <span class="keyword">for</span> cls_conv <span class="keyword">in</span> self.cls_convs:</div><div class="line">        cls_feat = self.relu(cls_conv(cls_feat))</div><div class="line">    <span class="keyword">for</span> reg_conv <span class="keyword">in</span> self.reg_convs:</div><div class="line">        reg_feat = self.relu(reg_conv(reg_feat))</div><div class="line">    cls_score = self.retina_cls(cls_feat)</div><div class="line">    bbox_pred = self.retina_reg(reg_feat)</div><div class="line">    <span class="keyword">return</span> cls_score, bbox_pred</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;定义了几个堆叠层，与上面网络结构的描述基本一致。实现了<code>forward_single()</code>，这里注意到分类分支和回归分支是共享特征图的。<code>forward_single()</code>是用于处理一个尺度的特征图的，所以这个函数将会在<code>multi_apply()</code>中被使用，生成不同尺度特征图的分类和回归结果。因此输出的cls_scores，bbox_preds也是一个列表，<code>cls_scores</code>的存储格式是<code>[shape([batch_size, cls_num*A, H1, W1]), shape([batch_size, cls_num*A, H2, W2]), ...]</code>，<code>bbox_preds</code>的存储格式是<code>[shape([batch_size, 4*A, H1, W1]), shape([batch_size, 4*A, H2, W2]), ...]</code>。</p>
<h4 id="（3）anchor的获得"><a href="#（3）anchor的获得" class="headerlink" title="（3）anchor的获得"></a>（3）anchor的获得</h4><p>&emsp;&emsp;anchor的获得同样可以参照<a href="https://mingming97.github.io/2019/03/26/anchor%20in%20object%20detection">这篇博文</a>，这里再结合代码细致说明一下anchor的生成过程。base_anchor的生成在那篇博文中解释的很详尽，需要解释的是滑动生成所有anchor的部分，这一部分的代码如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">grid_anchors</span><span class="params">(self, featmap_size, stride=<span class="number">16</span>, device=<span class="string">'cuda'</span>)</span>:</span></div><div class="line">    base_anchors = self.base_anchors.to(device)</div><div class="line"></div><div class="line">    feat_h, feat_w = featmap_size</div><div class="line">    shift_x = torch.arange(<span class="number">0</span>, feat_w, device=device) * stride</div><div class="line">    shift_y = torch.arange(<span class="number">0</span>, feat_h, device=device) * stride</div><div class="line">    shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)</div><div class="line">    shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=<span class="number">-1</span>)</div><div class="line">    shifts = shifts.type_as(base_anchors)</div><div class="line">    <span class="comment"># first feat_w elements correspond to the first row of shifts</span></div><div class="line">    <span class="comment"># add A anchors (1, A, 4) to K shifts (K, 1, 4) to get</span></div><div class="line">    <span class="comment"># shifted anchors (K, A, 4), reshape to (K*A, 4)</span></div><div class="line"></div><div class="line">    all_anchors = base_anchors[<span class="keyword">None</span>, :, :] + shifts[:, <span class="keyword">None</span>, :]</div><div class="line">    all_anchors = all_anchors.view(<span class="number">-1</span>, <span class="number">4</span>)</div><div class="line">    <span class="comment"># first A rows correspond to A anchors of (0, 0) in feature map,</span></div><div class="line">    <span class="comment"># then (0, 1), (0, 2), ...</span></div><div class="line">    <span class="keyword">return</span> all_anchors</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>shift_x</code>以及<code>shift_y</code>就是坐标偏移的大小，很好理解，那么如何通过这个生成x方向的偏移和y方向的偏移呢，这就用到了<code>_meshgrid()</code>这个函数。这个函数如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_meshgrid</span><span class="params">(self, x, y, row_major=True)</span>:</span></div><div class="line">    xx = x.repeat(len(y))</div><div class="line">    yy = y.view(<span class="number">-1</span>, <span class="number">1</span>).repeat(<span class="number">1</span>, len(x)).view(<span class="number">-1</span>)</div><div class="line">    <span class="keyword">if</span> row_major:</div><div class="line">        <span class="keyword">return</span> xx, yy</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> yy, xx</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个函数看似复杂，其实结果很有规律，就是生成x方向所有的偏移和y方向上所有的偏移，之前的<code>shift_x</code>和<code>shift_y</code>仅仅是一组偏移。可以想象一个网格中，顶层有一行数字是x的偏移，那么要生成所有行x的偏移就要将x的偏移重复y的长度次；有一列数字是y的偏移，要生成所有列y的偏移就要讲y的偏移重复x的长度次，再将结果flat后输出，就得到了所有的偏移量，也就是<code>shift_xx</code>以及<code>shift_yy</code>。</p>
<p>&emsp;&emsp;之后将<code>shift_xx, shift_yy</code>叠起来，其实就生成了<code>shift_x</code>和<code>shift_y</code>的笛卡儿积，其形状是<code>[2, H*W]</code>，因为base_anchor左上角和右下角的坐标是同步平移的，所以最终将两组<code>shift_xx, shift_yy</code>堆叠起来得到了形状是<code>[4, H*W]</code>的<code>shifts</code>。之后利用了广播机制，每个点有A个anchor，A个anchor中每个anchor的偏移量都是相同的，所以将对应需要广播的维度设为1，最终得到H*W*A个anchor的坐标。</p>
<p>&emsp;&emsp;这里面还有一个<code>valid_flags</code>的获得，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_flags</span><span class="params">(self, featmap_size, valid_size, device=<span class="string">'cuda'</span>)</span>:</span></div><div class="line">    feat_h, feat_w = featmap_size</div><div class="line">    valid_h, valid_w = valid_size</div><div class="line">    <span class="keyword">assert</span> valid_h &lt;= feat_h <span class="keyword">and</span> valid_w &lt;= feat_w</div><div class="line">    valid_x = torch.zeros(feat_w, dtype=torch.uint8, device=device)</div><div class="line">    valid_y = torch.zeros(feat_h, dtype=torch.uint8, device=device)</div><div class="line">    valid_x[:valid_w] = <span class="number">1</span></div><div class="line">    valid_y[:valid_h] = <span class="number">1</span></div><div class="line">    valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)</div><div class="line">    valid = valid_xx &amp; valid_yy</div><div class="line">    valid = valid[:, <span class="keyword">None</span>].expand(</div><div class="line">        valid.size(<span class="number">0</span>), self.num_base_anchors).contiguous().view(<span class="number">-1</span>)</div><div class="line">    <span class="keyword">return</span> valid</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;首先计算出合法的h和w的边界，然后将x方向上合法的中心部分设为1，y方向上合法的中心部分设为1，再利用<code>_meshgrid</code>得到两个方向上所有的合法情况，与操作得到两个方向都合法的点，由于每个点有A个框，一个框合法余下的暂时都算作合法，expand每个点的结果，最终flat得到一个<code>shape([H*W*A])</code>的Tensor，记录着每个框的合法情况。</p>
<p>&emsp;&emsp;不同尺度的anchor及其valid_flag的获取是在<code>mmdet/models/anchor_heads/anchor_head.py</code>中<code>AnchorHead</code>类的方法<code>get_anchors()</code>中得到。最终会得到一个list[list[Tensors]]，最外层是图片个数，再内一层是尺度个数，里面的Tensors的shape是<code>[H*W*4, 4]</code>，其中H和W代表对应尺度特征图的高和宽。</p>
<h4 id="（4）Anchor-target的获取"><a href="#（4）Anchor-target的获取" class="headerlink" title="（4）Anchor target的获取"></a>（4）Anchor target的获取</h4><h5 id="①-anchor-target-mmdet-core-anchor-anchor-target-py"><a href="#①-anchor-target-mmdet-core-anchor-anchor-target-py" class="headerlink" title="① anchor_target (mmdet/core/anchor/anchor_target.py)"></a>① anchor_target (mmdet/core/anchor/anchor_target.py)</h5><p>&emsp;&emsp;得到了anchor后要通过两步来得到训练的目标。</p>
<ol>
<li>Assign：把各个anchors分配给gt box的过程</li>
<li>Sample：从所有的bbox中sample出训练样本的过程。</li>
</ol>
<p>&emsp;&emsp;1个batch中每张图片的训练目标的获取都是调用<code>mmdet/core/anchor/anchor_target.py</code>中<code>anchor_target()</code>函数得到的。下面来大致看一下这个函数干了什么，再细致看Assign和Sample的过程。</p>
<p>&emsp;&emsp;首先获取了每张图片中，每种尺度anchor的数目</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">num_level_anchors = [anchors.size(<span class="number">0</span>) <span class="keyword">for</span> anchors <span class="keyword">in</span> anchor_list[<span class="number">0</span>]]</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;然后将每张图片中所有尺度的anchor放在一起，例如：有两个尺度，第一个尺度有20个anchor，其对应Tensor的形状是<code>[20, 4]</code>，第二个尺度有10个anchor，对应Tensor的形状是<code>[10, 4]</code>，那么会将该图片中所有尺度的anchor放在一起，变成一个<code>[30, 4]</code>的anchor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_imgs):</div><div class="line">    <span class="keyword">assert</span> len(anchor_list[i]) == len(valid_flag_list[i])</div><div class="line">    anchor_list[i] = torch.cat(anchor_list[i])</div><div class="line">    valid_flag_list[i] = torch.cat(valid_flag_list[i])</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;得到了每张图的所有anchor，就逐个图片调用<code>anchor_target_single()</code>函数，计算得到每张图片的每个anchor的label及其对应的weights（后面会解释），到对应gt box的delta值及其weights，以及正样本和负样本的下标。注意此时结果是一个list[Tensor]，第一维度是图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">(all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,</div><div class="line"> pos_inds_list, neg_inds_list) = multi_apply(</div><div class="line">     anchor_target_single,</div><div class="line">     anchor_list,</div><div class="line">     valid_flag_list,</div><div class="line">     gt_bboxes_list,</div><div class="line">     gt_bboxes_ignore_list,</div><div class="line">     gt_labels_list,</div><div class="line">     img_metas,</div><div class="line">     target_means=target_means,</div><div class="line">     target_stds=target_stds,</div><div class="line">     cfg=cfg,</div><div class="line">     label_channels=label_channels,</div><div class="line">     sampling=sampling,</div><div class="line">     unmap_outputs=unmap_outputs)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;接下来计算所有图片中所有正样本以及负样本的个数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">num_total_pos = sum([max(inds.numel(), <span class="number">1</span>) <span class="keyword">for</span> inds <span class="keyword">in</span> pos_inds_list])</div><div class="line">num_total_neg = sum([max(inds.numel(), <span class="number">1</span>) <span class="keyword">for</span> inds <span class="keyword">in</span> neg_inds_list])</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;最后将所有以图片为第一维度的结果，通过函数<code>images_to_levels()</code>，转换成以特征图尺度个数为第一维度的结果，供loss计算使用。具体做法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">labels_list = images_to_levels(all_labels, num_level_anchors)</div><div class="line">label_weights_list = images_to_levels(all_label_weights, num_level_anchors)</div><div class="line">bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)</div><div class="line">bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;下面是<code>image_to_levels()</code>的实现，首先将列表中每个img的结果堆叠起来，最后再将结果按每个尺度中anchor个数切片即可。此处以bbox_targets为例，假如一个batch_size有2张图片，一共有三个尺度的特征图分别有30、20、10个anchor。 那么一开始的输入就是<code>[shape([60, 4]), shape([60, 4])]</code>，将其堆叠起来就能得到<code>shape([2, 60, 4])</code>的Tensor，之后再按照特征图个数切片，最后得到<code>[shape([2, 30, 4]), shape([2, 20, 4]), shape([2, 10, 4])]</code>的list。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">images_to_levels</span><span class="params">(target, num_level_anchors)</span>:</span></div><div class="line">    <span class="string">"""Convert targets by image to targets by feature level.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    [target_img0, target_img1] -&gt; [target_level0, target_level1, ...]</span></div><div class="line"><span class="string">    """</span></div><div class="line">    target = torch.stack(target, <span class="number">0</span>)</div><div class="line">    level_targets = []</div><div class="line">    start = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> num_level_anchors:</div><div class="line">        end = start + n</div><div class="line">        level_targets.append(target[:, start:end].squeeze(<span class="number">0</span>))</div><div class="line">        start = end</div><div class="line">    <span class="keyword">return</span> level_targets</div></pre></td></tr></table></figure></p>
<h5 id="②-anchor-target-single-mmdet-core-anchor-anchor-target-py"><a href="#②-anchor-target-single-mmdet-core-anchor-anchor-target-py" class="headerlink" title="② anchor_target_single (mmdet/core/anchor/anchor_target.py)"></a>② anchor_target_single (mmdet/core/anchor/anchor_target.py)</h5><p>&emsp;&emsp;在这里只需要关注一张图中所有的anchor即可，首先用<code>anchors = flat_anchors[inside_flags, :]</code>来将所有有效的anchor提取出来，以减少计算量。而为了与<code>cls_scores</code>和<code>bbox_preds</code>的形状相符合，最后所有结果还要unmap回函数输入flat_anchors中。</p>
<p>&emsp;&emsp;再然后使用<code>Assigner</code>和<code>Sampler</code>来获取训练样本，这里内容较多，决定单开贴另说。只提一个小细节，由于使用了focal loss，一定程度解决了前景背景类别不均衡问题，所以sample的时候sample了全部的背景样本。对于其他的detector则需要采取一定措施限制负样本的个数，保持正负样本比例。</p>
<p>&emsp;&emsp;经过这两个部分后，就得到了一个<code>SamplingResult</code> ，这个类定义在了<code>mmdet/core/bbox/samplers/sampling_result.py</code>，里面主要保存了如下内容</p>
<ul>
<li>pos_inds：正样本的下标</li>
<li>neg_inds：负样本的下标</li>
<li>pos_bboxes：正样本的bbox坐标</li>
<li>neg_bboxes：负样本的bbox坐标</li>
<li>pos_is_gt：正样本的bbox是否就是gt_bbox</li>
<li>num_gts：gt_bbox的个数</li>
<li>pos_assigned_gt_inds：正样本所对应的gt bbox在gt_bboxes中的下标</li>
<li>pos_gt_bboxes：每个正样本对应的gt_bbox的坐标</li>
<li>pos_gt_labels：每个正样本对应的gt的label</li>
</ul>
<p>&emsp;&emsp;之后通过这些信息来计算target和weight，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">num_valid_anchors = anchors.shape[<span class="number">0</span>]</div><div class="line">bbox_targets = torch.zeros_like(anchors)</div><div class="line">bbox_weights = torch.zeros_like(anchors)</div><div class="line">labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)</div><div class="line">label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)</div><div class="line"></div><div class="line">pos_inds = sampling_result.pos_inds</div><div class="line">neg_inds = sampling_result.neg_inds</div><div class="line"><span class="keyword">if</span> len(pos_inds) &gt; <span class="number">0</span>:</div><div class="line">    pos_bbox_targets = bbox2delta(sampling_result.pos_bboxes,</div><div class="line">                                  sampling_result.pos_gt_bboxes,</div><div class="line">                                  target_means, target_stds)</div><div class="line">    bbox_targets[pos_inds, :] = pos_bbox_targets</div><div class="line">    bbox_weights[pos_inds, :] = <span class="number">1.0</span></div><div class="line">    <span class="keyword">if</span> gt_labels <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        labels[pos_inds] = <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]</div><div class="line">    <span class="keyword">if</span> cfg.pos_weight &lt;= <span class="number">0</span>:</div><div class="line">        label_weights[pos_inds] = <span class="number">1.0</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        label_weights[pos_inds] = cfg.pos_weight</div><div class="line"><span class="keyword">if</span> len(neg_inds) &gt; <span class="number">0</span>:</div><div class="line">    label_weights[neg_inds] = <span class="number">1.0</span></div></pre></td></tr></table></figure>
<p>&emsp;&emsp;注意到这里面的bbox_targets和label结果中都对应了weights且shape与它们自己相同，这些weights默认都是0。对于bbox_targets_weights，需要将正样本weight设为1，而对于label，需要将正负样本的weight都设为1。注意到这里面不一定正负样本加起来就是全部valid anchor，因为assign过程中有一部分anchor会根据IoU大小被忽略，所以正负样本都要特地赋值为1。</p>
<p>&emsp;&emsp;最后就是将结果对应回<code>flat_anchors</code>，调用了同一文件中的<code>unmap()</code>函数。这个unmap的过程很简单，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> unmap_outputs:</div><div class="line">    num_total_anchors = flat_anchors.size(<span class="number">0</span>)</div><div class="line">    labels = unmap(labels, num_total_anchors, inside_flags)</div><div class="line">    label_weights = unmap(label_weights, num_total_anchors, inside_flags)</div><div class="line">    <span class="keyword">if</span> label_channels &gt; <span class="number">1</span>:</div><div class="line">        labels, label_weights = expand_binary_labels(</div><div class="line">            labels, label_weights, label_channels)</div><div class="line">    bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)</div><div class="line">    bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;这是unmap函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unmap</span><span class="params">(data, count, inds, fill=<span class="number">0</span>)</span>:</span></div><div class="line">    <span class="string">""" Unmap a subset of item (data) back to the original set of items (of</span></div><div class="line"><span class="string">    size count) """</span></div><div class="line">    <span class="keyword">if</span> data.dim() == <span class="number">1</span>:</div><div class="line">        ret = data.new_full((count, ), fill)</div><div class="line">        ret[inds] = data</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        new_size = (count, ) + data.size()[<span class="number">1</span>:]</div><div class="line">        ret = data.new_full(new_size, fill)</div><div class="line">        ret[inds, :] = data</div><div class="line">    <span class="keyword">return</span> ret</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里要注意一个细节，<code>label_channels &gt; 1</code>的情况下就要调用<code>expand_binary_label()</code>函数。<code>label_channels</code>的值其实是在<code>mmdet/models/anchor_heads/anchor_head.py</code>中计算的，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">label_channels = self.cls_out_channels <span class="keyword">if</span> self.use_sigmoid_cls <span class="keyword">else</span> <span class="number">1</span></div></pre></td></tr></table></figure>
<p>&emsp;&emsp;含义就是如果使用sigmoid得到分类结果（每个channel都是一个二分类，属于该类目标值为1，不属于该类目标值为0），那么label_channels就是分类结果的channel数；否则为1。retinanet要使用focal loss，自然每个channel都是一个二分类，所以这里label_channels肯定大于1。这样<code>expand_binary_label()</code>的作用就清楚了，之前每个anchor的label都是对应label的序号，现在要将这个序号转化为一个one-hot的vector（背景类为全0的vector），以下是<code>expand_binary_label()</code>的实现，其实就是一个将序号转化为one-hot vector的过程，并不复杂。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_binary_labels</span><span class="params">(labels, label_weights, label_channels)</span>:</span></div><div class="line">    bin_labels = labels.new_full((labels.size(<span class="number">0</span>), label_channels), <span class="number">0</span>)</div><div class="line">    inds = torch.nonzero(labels &gt;= <span class="number">1</span>).squeeze()</div><div class="line">    <span class="keyword">if</span> inds.numel() &gt; <span class="number">0</span>:</div><div class="line">        bin_labels[inds, labels[inds] - <span class="number">1</span>] = <span class="number">1</span></div><div class="line">    bin_label_weights = label_weights.view(<span class="number">-1</span>, <span class="number">1</span>).expand(</div><div class="line">        label_weights.size(<span class="number">0</span>), label_channels)</div><div class="line">    <span class="keyword">return</span> bin_labels, bin_label_weights</div></pre></td></tr></table></figure>
<h4 id="5-loss的计算"><a href="#5-loss的计算" class="headerlink" title="(5) loss的计算"></a>(5) loss的计算</h4><p>&emsp;&emsp;loss的计算就是逐个尺度进行计算的过程，调用了<code>multi_apply()</code>函数将每个尺度下的预测和标签信息等传入<code>loss_single()</code>进行计算。如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">losses_cls, losses_reg = multi_apply(</div><div class="line">    self.loss_single,</div><div class="line">    cls_scores,</div><div class="line">    bbox_preds,</div><div class="line">    labels_list,</div><div class="line">    label_weights_list,</div><div class="line">    bbox_targets_list,</div><div class="line">    bbox_weights_list,</div><div class="line">    num_total_samples=num_total_samples,</div><div class="line">    cfg=cfg)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;这些传入的变量都是在之前的步骤中得到的，不难知道他们代表的含义。除了<code>num_total_samples</code>需要解释一下，表面看这个变量的含义是sample出的总样本的个数，但是它的计算方式却是<code>num_total_samples = (num_total_pos if self.use_focal_loss else num_total_pos + num_total_neg)</code>。为什么在使用focal loss的时候仅仅取正样本呢，这是因为这个变量在loss的计算中作为<code>avg_factor</code>使用，类似于loss的平均值，其它的检测方式中都有对负样本的sample过程，这样正负样本的和就不会很多。但是在focal loss中使用的是<code>PseudoSampler</code>，近乎于取了全部的负样本，这样正负样本的和就会非常大，loss的和除完这个数会非常小，使得训练无法进行。这个细节在focal loss的paper中有提到，如下</p>
<blockquote>
<p>The total focal loss of an image is computed as the sum of the focal loss over all ~100k anchors, <em>normalized by the number of anchors assigned to a ground-truth box</em>. </p>
</blockquote>
<p>&emsp;&emsp;下面就具体看一下一个尺度的loss的计算，这些是在<code>mmdet/models/anchor_heads/anchor_head.py</code>中的<code>loss_single()</code>函数中计算的。下面逐个部分看一下loss的计算。</p>
<h5 id="①-分类损失"><a href="#①-分类损失" class="headerlink" title="① 分类损失"></a>① 分类损失</h5><p>&emsp;&emsp;首先是cls_loss的计算，先将<code>labels</code>以及<code>label_weights</code> reshape成<code>[N, cls_num]</code>的形式（在focal loss中使用二分类损失，如果不用二分类损失每个anchor的label就是一个数字，就直接reshape成一个<code>[N]</code>的Tensor即可）；对于<code>cls_score</code>来说，它的shape是<code>[batch_size, num_cls, H, W]</code>，所以需要先交换维度后再reshape成<br><code>[N, cls_num]</code>的Tensor。接下来就是cls_criterion的选择，这部分不用解释，根据具体设置选择即可，在retinanet中选择的是<code>weighted_sigmoid_focal_loss</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> self.use_sigmoid_cls:</div><div class="line">    labels = labels.reshape(<span class="number">-1</span>, self.cls_out_channels)</div><div class="line">    label_weights = label_weights.reshape(<span class="number">-1</span>, self.cls_out_channels)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    labels = labels.reshape(<span class="number">-1</span>)</div><div class="line">    label_weights = label_weights.reshape(<span class="number">-1</span>)</div><div class="line">cls_score = cls_score.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(</div><div class="line">    <span class="number">-1</span>, self.cls_out_channels)</div><div class="line"><span class="keyword">if</span> self.use_sigmoid_cls:</div><div class="line">    <span class="keyword">if</span> self.use_focal_loss:</div><div class="line">        cls_criterion = weighted_sigmoid_focal_loss</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        cls_criterion = weighted_binary_cross_entropy</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">if</span> self.use_focal_loss:</div><div class="line">        <span class="keyword">raise</span> NotImplementedError</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        cls_criterion = weighted_cross_entropy</div><div class="line"><span class="keyword">if</span> self.use_focal_loss:</div><div class="line">    loss_cls = cls_criterion(</div><div class="line">        cls_score,</div><div class="line">        labels,</div><div class="line">        label_weights,</div><div class="line">        gamma=cfg.gamma,</div><div class="line">        alpha=cfg.alpha,</div><div class="line">        avg_factor=num_total_samples)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    loss_cls = cls_criterion(</div><div class="line">        cls_score, labels, label_weights, avg_factor=num_total_samples)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;下面看一下focal loss的具体实现，其中涉及到<code>mmdet/core/loss/losses.py</code>中的<code>sigmoid_focal_loss()</code>和<code>weighted_sigmoid_focal_loss()</code>。主要计算其实在<code>sigmoid_focal_loss()</code>中。在看代码之前首先看一下focal loss的定义</p>
<script type="math/tex; mode=display">
\textbf{FL}(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)</script><p>&emsp;&emsp;其中$\alpha$和$\gamma$都属于超参，而$p_t$代表属于第t类的概率，而对于retinanet输出的每个channel在经过sigmoid后都代表属于该类别的概率，所以对于二分类的$p_t$和$\alpha_t$可以写作如下形式，设网络某个channel的输出为$p$，该类的label记作$t\in \{0,1\}$。</p>
<script type="math/tex; mode=display">
p_t=pt+(1-p)(1-t) \\
\alpha_t=p\alpha+(1-\alpha)(1-t)</script><p>代入focal loss的计算式，可以得到</p>
<script type="math/tex; mode=display">
\textbf{FL}(p_t)=-(p\alpha+(1-\alpha)(1-t))((1-p)t+p(1-t))^\gamma log(pt+(1-p)(1-t))</script><p>&emsp;&emsp;下面就可以利用这个计算式来计算focal loss，除去系数，其实剩下部分就是一个BCELoss，所以只有系数需要自己计算，而且不要忘记用系数与weight做element-wise product，将无效的anchor的loss置为0。代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_focal_loss</span><span class="params">(pred,</span></span></div><div class="line"><span class="function"><span class="params">                       target,</span></span></div><div class="line"><span class="function"><span class="params">                       weight,</span></span></div><div class="line"><span class="function"><span class="params">                       gamma=<span class="number">2.0</span>,</span></span></div><div class="line"><span class="function"><span class="params">                       alpha=<span class="number">0.25</span>,</span></span></div><div class="line"><span class="function"><span class="params">                       reduction=<span class="string">'mean'</span>)</span>:</span></div><div class="line">    pred_sigmoid = pred.sigmoid()</div><div class="line">    target = target.type_as(pred)</div><div class="line">    pt = (<span class="number">1</span> - pred_sigmoid) * target + pred_sigmoid * (<span class="number">1</span> - target)</div><div class="line">    weight = (alpha * target + (<span class="number">1</span> - alpha) * (<span class="number">1</span> - target)) * weight</div><div class="line">    weight = weight * pt.pow(gamma)</div><div class="line">    loss = F.binary_cross_entropy_with_logits(</div><div class="line">        pred, target, reduction=<span class="string">'none'</span>) * weight</div><div class="line">    reduction_enum = F._Reduction.get_enum(reduction)</div><div class="line">    <span class="comment"># none: 0, mean:1, sum: 2</span></div><div class="line">    <span class="keyword">if</span> reduction_enum == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> loss</div><div class="line">    <span class="keyword">elif</span> reduction_enum == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> loss.mean()</div><div class="line">    <span class="keyword">elif</span> reduction_enum == <span class="number">2</span>:</div><div class="line">        <span class="keyword">return</span> loss.sum()</div></pre></td></tr></table></figure>
<h5 id="②-回归损失"><a href="#②-回归损失" class="headerlink" title="② 回归损失"></a>② 回归损失</h5><p>&emsp;&emsp;下面是边框回归loss的计算，类似地，先对<code>bbox_targets</code>、<code>bbox_weights</code>和<code>bbox_pred</code>进行了reshape。随后调用<code>weighted_smoothl1()</code>进行计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">bbox_targets = bbox_targets.reshape(<span class="number">-1</span>, <span class="number">4</span>)</div><div class="line">bbox_weights = bbox_weights.reshape(<span class="number">-1</span>, <span class="number">4</span>)</div><div class="line">bbox_pred = bbox_pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">4</span>)</div><div class="line">loss_reg = weighted_smoothl1(</div><div class="line">    bbox_pred,</div><div class="line">    bbox_targets,</div><div class="line">    bbox_weights,</div><div class="line">    beta=cfg.smoothl1_beta,</div><div class="line">    avg_factor=num_total_samples)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;同样地，<code>weighted_smoothl1()</code>定义在<code>mmdet/core/loss/losses.py</code>中，这里面beta代表回归损失的系数，其余就是损失的计算。注意，在计算完loss后，同样要与<code>weight</code>做element-wise product，将非正样本的无用的loss清0。这里说一点题外话，这种给loss上加weight的做法非常像掩码，在mmdetection中weight都设置成与对应的Tensor相同的shape，例如bbox_target的shape是<code>[N, 4]</code>，weight的shape也是<code>[N, 4]</code>，并没有直接设置为<code>[N]</code>，这样十分清晰，要忽略不相关训练样本的loss直接做element-wise product就好。<code>weighted_smoothl1()</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted_smoothl1</span><span class="params">(pred, target, weight, beta=<span class="number">1.0</span>, avg_factor=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> avg_factor <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        avg_factor = torch.sum(weight &gt; <span class="number">0</span>).float().item() / <span class="number">4</span> + <span class="number">1e-6</span></div><div class="line">    loss = smooth_l1_loss(pred, target, beta, reduction=<span class="string">'none'</span>)</div><div class="line">    <span class="keyword">return</span> torch.sum(loss * weight)[<span class="keyword">None</span>] / avg_factor</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、大体流程&quot;&gt;&lt;a href=&quot;#一、大体流程&quot; class=&quot;headerlink&quot; title=&quot;一、大体流程&quot;&gt;&lt;/a&gt;一、大体流程&lt;/h2&gt;&lt;h3 id=&quot;1-训练逻辑&quot;&gt;&lt;a href=&quot;#1-训练逻辑&quot; class=&quot;headerlink&quot; titl
    
    </summary>
    
      <category term="CV" scheme="https://mingming97.github.io/categories/CV/"/>
    
    
      <category term="object detection" scheme="https://mingming97.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>物体检测中的anchor</title>
    <link href="https://mingming97.github.io/2019/03/26/anchor%20in%20object%20detection/"/>
    <id>https://mingming97.github.io/2019/03/26/anchor in object detection/</id>
    <published>2019-03-26T09:55:00.000Z</published>
    <updated>2019-03-26T11:48:27.147Z</updated>
    
    <content type="html"><![CDATA[<p>anchor自从在faster rcnn中提出后，被广泛应用于物体检测中，流程可概括如下（one stage）</p>
<ol>
<li>通过backbone和neck（fpn等）网络得到feature map并根据feature map用head网络生成各个尺度的cls score和reg score</li>
<li>生成anchor</li>
<li>将anchor assign给对应的gt box</li>
<li>根据assign result按一定策略sample出正负样本</li>
<li>计算loss</li>
</ol>
<p>（以上英文单词均遵循mmdetection的命名）</p>
<p>&emsp;&emsp;而anchor生成在一般的实现中都会使用到三个参数，anchor ratios，anchor strides，scales，其中anchor ratios很好解释，指的就是宽高比（h:s）；而anchor strides则有一点绕，我的理解是它指的是感受野的大小，也就是对应feature map对应到原图多少个像素，所以它的值等于原图中anchor的基础大小，这在代码中一般写作base_size，有了base_size，就可以得到一个基础框，此时一般使用((0, 0), (anchor_stride-1, anchor_stride-1))这个框，也就是图中最左上的一个框。。通过这个坐标，可以很轻松的计算出中心(x_ctr, y_ctr)，宽和高就是base_size；scales也很简单，就是放缩的尺度，要将宽和高按照各个scale的值放大。</p>
<p>&emsp;&emsp;例如：当anchor stride = 4，anchor_ratios = [0.5, 1.0, 2.0]，scales = [4, 8, 16]时，首先可以得到一个((0,0),(3,3))的基础框，宽高都为4，将这个4$\times$4的区域按照anchor_ratios进行变换，保证面积不变，宽高比符合要求，可以做如下转换</p>
<script type="math/tex; mode=display">
\begin{align}
& hs=h*\sqrt{ratio} \\
& ws = \displaystyle\frac{w}{\sqrt{ratio}}
\end{align}</script><p>也可以按如下方式转换</p>
<script type="math/tex; mode=display">
\begin{align}
& hs = \sqrt{wh\cdot ratio} \\
& ws = \displaystyle\frac{\sqrt{wh}}{\sqrt{ratio}}
\end{align}</script><p>&emsp;&emsp;无论用哪种方式都能保证面积不变，而宽高比符合anchor_ratios。将转换后的hs和ws乘上对应的scales，就能得到放缩后的anchor，有三种ratio，三种scale，一共可以得到9个anchor，再根据中心坐标，算出每个anchor的((xmin, ymin),(xmax, ymax))。</p>
<p>&emsp;&emsp;得到左上角的所有anchor坐标之后，只需要对他们进行坐标的平移，就能得到整张图片上所有的anchor的坐标，有平移就会涉及到x和y方向的stride，一般将stride设为与anchor_stride相等，也可以根据情况自己设置。</p>
<p>&emsp;&emsp;mmdetection中AnchorGenerator类实现的就是这一过程，除此之外，如果坐标在图外显然是invalid的，mmdetection中还对feat_map上有效的anchor进行了计算，该类中仅仅处理了右侧和下侧的边界问题，而对于上侧和左侧的边界，也就是0一侧的判断是在<code>anchor_target.py</code>中的<code>anchor_inside_flags</code>函数中处理的，该函数中同样考虑了右侧和下侧的边界，得到的是所有合法anchor。</p>
<p>以下是AnchorGenerator的代码，结合上面的说明理解起来并不困难</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnchorGenerator</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, base_size, scales, ratios, scale_major=True, ctr=None)</span>:</span></div><div class="line">        self.base_size = base_size</div><div class="line">        self.scales = torch.Tensor(scales)</div><div class="line">        self.ratios = torch.Tensor(ratios)</div><div class="line">        self.scale_major = scale_major</div><div class="line">        self.ctr = ctr</div><div class="line">        self.base_anchors = self.gen_base_anchors()</div><div class="line"></div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_base_anchors</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.base_anchors.size(<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen_base_anchors</span><span class="params">(self)</span>:</span></div><div class="line">        w = self.base_size</div><div class="line">        h = self.base_size</div><div class="line">        <span class="keyword">if</span> self.ctr <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            x_ctr = <span class="number">0.5</span> * (w - <span class="number">1</span>)</div><div class="line">            y_ctr = <span class="number">0.5</span> * (h - <span class="number">1</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            x_ctr, y_ctr = self.ctr</div><div class="line"></div><div class="line">        h_ratios = torch.sqrt(self.ratios)</div><div class="line">        w_ratios = <span class="number">1</span> / h_ratios</div><div class="line">        <span class="keyword">if</span> self.scale_major:</div><div class="line">            ws = (w * w_ratios[:, <span class="keyword">None</span>] * self.scales[<span class="keyword">None</span>, :]).view(<span class="number">-1</span>)</div><div class="line">            hs = (h * h_ratios[:, <span class="keyword">None</span>] * self.scales[<span class="keyword">None</span>, :]).view(<span class="number">-1</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            ws = (w * self.scales[:, <span class="keyword">None</span>] * w_ratios[<span class="keyword">None</span>, :]).view(<span class="number">-1</span>)</div><div class="line">            hs = (h * self.scales[:, <span class="keyword">None</span>] * h_ratios[<span class="keyword">None</span>, :]).view(<span class="number">-1</span>)</div><div class="line"></div><div class="line">        base_anchors = torch.stack(</div><div class="line">            [</div><div class="line">                x_ctr - <span class="number">0.5</span> * (ws - <span class="number">1</span>), y_ctr - <span class="number">0.5</span> * (hs - <span class="number">1</span>),</div><div class="line">                x_ctr + <span class="number">0.5</span> * (ws - <span class="number">1</span>), y_ctr + <span class="number">0.5</span> * (hs - <span class="number">1</span>)</div><div class="line">            ],</div><div class="line">            dim=<span class="number">-1</span>).round()</div><div class="line"></div><div class="line">        <span class="keyword">return</span> base_anchors</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_meshgrid</span><span class="params">(self, x, y, row_major=True)</span>:</span></div><div class="line">        xx = x.repeat(len(y))</div><div class="line">        yy = y.view(<span class="number">-1</span>, <span class="number">1</span>).repeat(<span class="number">1</span>, len(x)).view(<span class="number">-1</span>)</div><div class="line">        <span class="keyword">if</span> row_major:</div><div class="line">            <span class="keyword">return</span> xx, yy</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> yy, xx</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grid_anchors</span><span class="params">(self, featmap_size, stride=<span class="number">16</span>, device=<span class="string">'cuda'</span>)</span>:</span></div><div class="line">        base_anchors = self.base_anchors.to(device)</div><div class="line"></div><div class="line">        feat_h, feat_w = featmap_size</div><div class="line">        shift_x = torch.arange(<span class="number">0</span>, feat_w, device=device) * stride</div><div class="line">        shift_y = torch.arange(<span class="number">0</span>, feat_h, device=device) * stride</div><div class="line">        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)</div><div class="line">        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=<span class="number">-1</span>)</div><div class="line">        shifts = shifts.type_as(base_anchors)</div><div class="line">        <span class="comment"># first feat_w elements correspond to the first row of shifts</span></div><div class="line">        <span class="comment"># add A anchors (1, A, 4) to K shifts (K, 1, 4) to get</span></div><div class="line">        <span class="comment"># shifted anchors (K, A, 4), reshape to (K*A, 4)</span></div><div class="line"></div><div class="line">        all_anchors = base_anchors[<span class="keyword">None</span>, :, :] + shifts[:, <span class="keyword">None</span>, :]</div><div class="line">        all_anchors = all_anchors.view(<span class="number">-1</span>, <span class="number">4</span>)</div><div class="line">        <span class="comment"># first A rows correspond to A anchors of (0, 0) in feature map,</span></div><div class="line">        <span class="comment"># then (0, 1), (0, 2), ...</span></div><div class="line">        <span class="keyword">return</span> all_anchors</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">valid_flags</span><span class="params">(self, featmap_size, valid_size, device=<span class="string">'cuda'</span>)</span>:</span></div><div class="line">        feat_h, feat_w = featmap_size</div><div class="line">        valid_h, valid_w = valid_size</div><div class="line">        <span class="keyword">assert</span> valid_h &lt;= feat_h <span class="keyword">and</span> valid_w &lt;= feat_w</div><div class="line">        valid_x = torch.zeros(feat_w, dtype=torch.uint8, device=device)</div><div class="line">        valid_y = torch.zeros(feat_h, dtype=torch.uint8, device=device)</div><div class="line">        valid_x[:valid_w] = <span class="number">1</span></div><div class="line">        valid_y[:valid_h] = <span class="number">1</span></div><div class="line">        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)</div><div class="line">        valid = valid_xx &amp; valid_yy</div><div class="line">        valid = valid[:, <span class="keyword">None</span>].expand(</div><div class="line">            valid.size(<span class="number">0</span>), self.num_base_anchors).contiguous().view(<span class="number">-1</span>)</div><div class="line">        <span class="keyword">return</span> valid</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;anchor自从在faster rcnn中提出后，被广泛应用于物体检测中，流程可概括如下（one stage）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过backbone和neck（fpn等）网络得到feature map并根据feature map用head网络生成各个尺度的cls 
    
    </summary>
    
      <category term="CV" scheme="https://mingming97.github.io/categories/CV/"/>
    
    
      <category term="object detection" scheme="https://mingming97.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>SSD源码阅读及原理详解</title>
    <link href="https://mingming97.github.io/2018/10/21/SSD/"/>
    <id>https://mingming97.github.io/2018/10/21/SSD/</id>
    <published>2018-10-21T07:08:00.000Z</published>
    <updated>2019-01-13T08:33:27.593Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文阅读的版本是tensorflow对SSD的实现，相对而言阅读难度要远低于caffe版本的实现，源代码可见<a href="https://github.com/balancap/SSD-Tensorflow/" target="_blank" rel="external">balancap/SSD-Tensorflow</a>。</p>
<h2 id="一、思路"><a href="#一、思路" class="headerlink" title="一、思路"></a>一、思路</h2><p>&emsp;&emsp;SSD的网络结构在论文中清晰可见，如图所示。具体是使用了VGG的基础结构，保留了前五层，将fc6和fc7改为了带孔卷积层，而且去掉了池化层和dropout，在不增加参数的条件下指数级扩大了感受野，而且没有因为池化导致丢失太多的信息；后面再额外增加了3个卷积层和一个average pooling层，目的是使用不同层的feature map来检测不同尺度的物体。之后从前面的卷积层中提取了conv4_3，从后面新增的卷积层中提取了conv7，conv8_2，conv9_2，conv10_2，conv11_2来作为检测使用的特征图，在这些不同尺度特征图上提取不同数目的default boxes，对其进行分类和边框回归得到物体框和类别，最后进行nms来进行筛选。简而言之，SSD预测的目标就是以一张图中的所有提取出的anchor boxes为窗口，检测其中是否有物体，如果有，预测它的类别并对其位置进行精修，没有物体则将其分类为背景。</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af67a3a0cf.png" alt="structure.png"></p>
<p>&emsp;&emsp;通俗一点的思路如下面的两个图所示，SSD所做的其实就是将feature map用额外的两个卷积层去卷积得到分类评分和边框回归的偏移，其中k表示从该层feature的每个anchor处提取的不同default boxes的个数，这些词具体是什么可以在后面的代码细节中看到。其他的一些细节，例如数据增广，mining hard examples等，也都在代码中有体现。</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af67a838a7.png" alt="ssdstruct.png"></p>
<p>下面是提取结果的卷积层的放大图。</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af67a2f47b.png" alt="extrafeature.png"></p>
<p>&emsp;&emsp;每个feature map可以分两条路，分别得到分类结果和回归结果，再通过已有的ground truth box及其类别得到每个default box的分类和边框偏移，就可以计算loss，进行训练了。</p>
<h2 id="二、default-boxes提取"><a href="#二、default-boxes提取" class="headerlink" title="二、default boxes提取"></a>二、default boxes提取</h2><p>&emsp;&emsp;default boxes的选取与faster rcnn中的anchor有一些类似，就是按照不同的scale和ratio生成k个boxes，看下面的图就能大概了解其思想</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af67a6a4ea.png" alt="defaultboxes1.png"></p>
<p><img src="https://i.loli.net/2019/01/13/5c3af67a81572.png" alt="defaultboxes2.png"></p>
<p><img src="https://i.loli.net/2019/01/13/5c3af67a7644c.png" alt="defaultboxes3.png"></p>
<ul>
<li><p>scale：scale指的是所检测的框的大小相对于原图的比例。比例低的可以框出图中的小物体，比例高的可以框出图中的大物体。深层次的feature map适合检测大物体，所以此处使用了一个线性关系来设置各个feature map所检测的scale。公式如下</p>
<script type="math/tex; mode=display">
s_k = s_{min} + \displaystyle\frac{s_{max}-s_{min}}{m-1}(k-1),k\in[1,m]</script><p>其中m是特征图的个数，实际取的时候为5，因为conv4_3层是单独设置的大小。$s_k$是第k个特征图的scale，$s_{min}$和$s_{max}$表示scale的最小值和最大值，在原论文中分别取0.2和0.9，而第一个特征图的scale一般设为$s_{min}$的一半，为0.1，所以对于300$\times$300的图片，最小的比例为300*0.1=30，之后每个对应feature map所检测的default boxes的大小都是300*$s_k$。在caffe源码中的计算是先给出了$s_k$的增长步长，也就是$\displaystyle \lfloor\frac{\lfloor s_{max}\times100\rfloor-\lfloor s_{min}\times100\rfloor}{m-1}\rfloor=17$，由此可以得到5个值分别为20，37，54，71，88（后面还会得到另一个虚拟值是88+17=105）。这些比例乘图片大小再除以100，就能得到各个特征图的大小分别为60，111，162，213，264。再结合最小比例，可以得到default boxes的实际尺度分别为30，60，111，162，213，264。</p>
</li>
<li><p>aspect ratio：aspect ratio指的是default boxes的横纵比，一般有$\displaystyle a_r \in \{1,2,3,\frac{1}{2},\frac{1}{3}\}$，对于特定的横纵比，会使用</p>
<script type="math/tex; mode=display">
w_k^a=s_k\sqrt{a_r}\ \ \ \ \ \ h_k^a=\displaystyle\frac{s_k}{\sqrt{a_r}}</script><p>来计算真正的宽度和高度（此处$s_k$也是指真实的大小，也就是上文中的30，60，111，…）。默认情况下，每个特征图会有一个$a_r=1$的default box，除此之外还会设置一个$s_k^{\ ‘}=\sqrt{s_k s_{k+1}}$，$a_r=1$的default box，也就是说每个特征图中都会设置两个大小不同的正方形的default box，此处最后一个特征图就需要用到之前的虚拟值105（对应的实际尺度是315）。然而在实现时，使用的比例是可以自己选择的，理论上每个feature map都应该有6个default boxes，但是实际实现中某些层只使用了4个default box，没有使用长宽比为$3$和$\frac{1}{3}$的default box。</p>
</li>
<li><p>default box中心：default box的中心在计算的时候需要恢复为原图的相对坐标，所以每个中心设置为$\displaystyle\left(\frac{i+0.5}{|f_k|},\frac{j+0.5}{|f_k|}\right)\ \ i,j \in [0, |f_k|)$，其中$|f_k|$表示第k个feature map的大小。</p>
</li>
</ul>
<p>综上可以得到如下表格</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">feature map</th>
<th style="text-align:center">feature map size</th>
<th style="text-align:center">min_size($\textbf{s}_\textbf{k}$)</th>
<th style="text-align:center">max_size($\textbf{s}_\textbf{k+1}$)</th>
<th style="text-align:center">aspect_ratio</th>
<th style="text-align:center">step</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">conv4_3</td>
<td style="text-align:center">38$\times$38</td>
<td style="text-align:center">30</td>
<td style="text-align:center">60</td>
<td style="text-align:center">1,2,$\frac{1}{2}$</td>
<td style="text-align:center">8</td>
</tr>
<tr>
<td style="text-align:center">conv7</td>
<td style="text-align:center">19$\times$19</td>
<td style="text-align:center">60</td>
<td style="text-align:center">111</td>
<td style="text-align:center">1,2,3,$\frac{1}{2}$,$\frac{1}{3}$</td>
<td style="text-align:center">16</td>
</tr>
<tr>
<td style="text-align:center">conv8_2</td>
<td style="text-align:center">10$\times$10</td>
<td style="text-align:center">111</td>
<td style="text-align:center">162</td>
<td style="text-align:center">1,2,3,$\frac{1}{2}$,$\frac{1}{3}$</td>
<td style="text-align:center">32</td>
</tr>
<tr>
<td style="text-align:center">conv9_2</td>
<td style="text-align:center">5$\times$5</td>
<td style="text-align:center">162</td>
<td style="text-align:center">213</td>
<td style="text-align:center">1,2,3,$\frac{1}{2}$,$\frac{1}{3}$</td>
<td style="text-align:center">64</td>
</tr>
<tr>
<td style="text-align:center">conv10_2</td>
<td style="text-align:center">3$\times$3</td>
<td style="text-align:center">213</td>
<td style="text-align:center">264</td>
<td style="text-align:center">1,2,$\frac{1}{2}$</td>
<td style="text-align:center">100</td>
</tr>
<tr>
<td style="text-align:center">conv11_2</td>
<td style="text-align:center">1$\times$1</td>
<td style="text-align:center">264</td>
<td style="text-align:center">315</td>
<td style="text-align:center">1,2,$\frac{1}{2}$</td>
<td style="text-align:center">300</td>
</tr>
</tbody>
</table>
</div>
<p>以上所有的内容在源码中都是有对应的体现的，首先看关于default box的一些设置，代码在<code>ssd_vgg_300.py</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">default_params = SSDParams(</div><div class="line">    img_shape=(<span class="number">300</span>, <span class="number">300</span>),</div><div class="line">    num_classes=<span class="number">21</span>,</div><div class="line">    no_annotation_label=<span class="number">21</span>,</div><div class="line">    feat_layers=[<span class="string">'block4'</span>, <span class="string">'block7'</span>, <span class="string">'block8'</span>, <span class="string">'block9'</span>, <span class="string">'block10'</span>, <span class="string">'block11'</span>],</div><div class="line">    feat_shapes=[(<span class="number">38</span>, <span class="number">38</span>), (<span class="number">19</span>, <span class="number">19</span>), (<span class="number">10</span>, <span class="number">10</span>), (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>)],</div><div class="line">    anchor_size_bounds=[<span class="number">0.15</span>, <span class="number">0.90</span>],</div><div class="line">    <span class="comment"># anchor_size_bounds=[0.20, 0.90],</span></div><div class="line">    anchor_sizes=[(<span class="number">21.</span>, <span class="number">45.</span>),</div><div class="line">                  (<span class="number">45.</span>, <span class="number">99.</span>),</div><div class="line">                  (<span class="number">99.</span>, <span class="number">153.</span>),</div><div class="line">                  (<span class="number">153.</span>, <span class="number">207.</span>),</div><div class="line">                  (<span class="number">207.</span>, <span class="number">261.</span>),</div><div class="line">                  (<span class="number">261.</span>, <span class="number">315.</span>)],</div><div class="line">    <span class="comment"># anchor_sizes=[(30., 60.),</span></div><div class="line">    <span class="comment">#               (60., 111.),</span></div><div class="line">    <span class="comment">#               (111., 162.),</span></div><div class="line">    <span class="comment">#               (162., 213.),</span></div><div class="line">    <span class="comment">#               (213., 264.),</span></div><div class="line">    <span class="comment">#               (264., 315.)],</span></div><div class="line">    anchor_ratios=[[<span class="number">2</span>, <span class="number">.5</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span>/<span class="number">3</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span>/<span class="number">3</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span>/<span class="number">3</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>],</div><div class="line">                   [<span class="number">2</span>, <span class="number">.5</span>]],</div><div class="line">    anchor_steps=[<span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">100</span>, <span class="number">300</span>],</div><div class="line">    anchor_offset=<span class="number">0.5</span>,</div><div class="line">    normalizations=[<span class="number">20</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>],</div><div class="line">    prior_scaling=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;其中img_shape代表输入图片的大小；num_classes代表输入的类别（20+1个背景类）；feat_layers代表提取的层；feat_shapes代表各个提取的featrue map的大小；anchor_size_bounds代表前文所说的$s_k$（原论文中取0.2到0.9）；anchor_sizes保存的是各层提取的default boxes的大小，也就是上面说的实际尺度；anchor_ratios是前面所说的各层的default boxes的横纵比；anchor_steps指的实际是default box的中心点坐标映射回原图的比例，做法就是用中心点坐标乘以对应的step；anchor_offset对应前文的offset；其余变量暂时与default boxes的生成无关。</p>
<p>&emsp;&emsp;整个训练过程都在<code>train_ssd_network.py</code>中，从这个文件中可以看出，第一步anchors的获取是通过<code>ssd_anchors = ssd_net.anchors(ssd_shape)</code>这句代码来获取的，而<code>ssd_net</code>这个对象是经由一个工厂类生成的一个网络类，此处以ssd_vgg_300为例，可以当作<code>ssd_net</code>就是一个<code>ssd_vgg_300.py</code>中定义的<code>SSDNet</code>的实例。这个被调用的函数可以在<code>ssd_vgg_300.py</code>中找到。而这个函数也只有一句话，那就是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> ssd_anchors_all_layers(img_shape,</div><div class="line">                              self.params.feat_shapes,</div><div class="line">                              self.params.anchor_sizes,</div><div class="line">                              self.params.anchor_ratios,</div><div class="line">                              self.params.anchor_steps,</div><div class="line">                              self.params.anchor_offset,</div><div class="line">                              dtype)</div></pre></td></tr></table></figure>
<p><code>ssd_anchors_all_layers</code>在该文件中的后半部分定义，只有几句话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_anchors_all_layers</span><span class="params">(img_shape,</span></span></div><div class="line"><span class="function"><span class="params">                           layers_shape,</span></span></div><div class="line"><span class="function"><span class="params">                           anchor_sizes,</span></span></div><div class="line"><span class="function"><span class="params">                           anchor_ratios,</span></span></div><div class="line"><span class="function"><span class="params">                           anchor_steps,</span></span></div><div class="line"><span class="function"><span class="params">                           offset=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                           dtype=np.float32)</span>:</span></div><div class="line">    <span class="string">"""Compute anchor boxes for all feature layers.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    layers_anchors = []</div><div class="line">    <span class="keyword">for</span> i, s <span class="keyword">in</span> enumerate(layers_shape):</div><div class="line">        anchor_bboxes = ssd_anchor_one_layer(img_shape, s,</div><div class="line">                                             anchor_sizes[i],</div><div class="line">                                             anchor_ratios[i],</div><div class="line">                                             anchor_steps[i],</div><div class="line">                                             offset=offset, dtype=dtype)</div><div class="line">        layers_anchors.append(anchor_bboxes)</div><div class="line">    <span class="keyword">return</span> layers_anchors</div></pre></td></tr></table></figure>
<p>可以看出，它是一层一层获取default box，再添加到列表中，此处使用了获取一层default box的函数，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_anchor_one_layer</span><span class="params">(img_shape,</span></span></div><div class="line"><span class="function"><span class="params">                         feat_shape,</span></span></div><div class="line"><span class="function"><span class="params">                         sizes,</span></span></div><div class="line"><span class="function"><span class="params">                         ratios,</span></span></div><div class="line"><span class="function"><span class="params">                         step,</span></span></div><div class="line"><span class="function"><span class="params">                         offset=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                         dtype=np.float32)</span>:</span></div><div class="line">    <span class="string">"""Computer SSD default anchor boxes for one feature layer.</span></div><div class="line"><span class="string">    Determine the relative position grid of the centers, and the relative</span></div><div class="line"><span class="string">    width and height.</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">      feat_shape: Feature shape, used for computing relative position grids;</span></div><div class="line"><span class="string">      size: Absolute reference sizes;</span></div><div class="line"><span class="string">      ratios: Ratios to use on these features;</span></div><div class="line"><span class="string">      img_shape: Image shape, used for computing height, width relatively to the</span></div><div class="line"><span class="string">        former;</span></div><div class="line"><span class="string">      offset: Grid offset.</span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">      y, x, h, w: Relative x and y grids, and height and width.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Compute the position grid: simple way.</span></div><div class="line">    <span class="comment"># y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]</span></div><div class="line">    <span class="comment"># y = (y.astype(dtype) + offset) / feat_shape[0]</span></div><div class="line">    <span class="comment"># x = (x.astype(dtype) + offset) / feat_shape[1]</span></div><div class="line">    <span class="comment"># Weird SSD-Caffe computation using steps values...</span></div><div class="line">    y, x = np.mgrid[<span class="number">0</span>:feat_shape[<span class="number">0</span>], <span class="number">0</span>:feat_shape[<span class="number">1</span>]]</div><div class="line">    y = (y.astype(dtype) + offset) * step / img_shape[<span class="number">0</span>]</div><div class="line">    x = (x.astype(dtype) + offset) * step / img_shape[<span class="number">1</span>]</div><div class="line">    <span class="comment"># Expand dims to support easy broadcasting.</span></div><div class="line">    y = np.expand_dims(y, axis=<span class="number">-1</span>)</div><div class="line">    x = np.expand_dims(x, axis=<span class="number">-1</span>)</div><div class="line">    <span class="comment"># Compute relative height and width.</span></div><div class="line">    <span class="comment"># Tries to follow the original implementation of SSD for the order.</span></div><div class="line">    num_anchors = len(sizes) + len(ratios)</div><div class="line">    h = np.zeros((num_anchors, ), dtype=dtype)</div><div class="line">    w = np.zeros((num_anchors, ), dtype=dtype)</div><div class="line">    <span class="comment"># Add first anchor boxes with ratio=1.</span></div><div class="line">    h[<span class="number">0</span>] = sizes[<span class="number">0</span>] / img_shape[<span class="number">0</span>]</div><div class="line">    w[<span class="number">0</span>] = sizes[<span class="number">0</span>] / img_shape[<span class="number">1</span>]</div><div class="line">    di = <span class="number">1</span></div><div class="line">    <span class="keyword">if</span> len(sizes) &gt; <span class="number">1</span>:</div><div class="line">        h[<span class="number">1</span>] = math.sqrt(sizes[<span class="number">0</span>] * sizes[<span class="number">1</span>]) / img_shape[<span class="number">0</span>]</div><div class="line">        w[<span class="number">1</span>] = math.sqrt(sizes[<span class="number">0</span>] * sizes[<span class="number">1</span>]) / img_shape[<span class="number">1</span>]</div><div class="line">        di += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> i, r <span class="keyword">in</span> enumerate(ratios):</div><div class="line">        h[i+di] = sizes[<span class="number">0</span>] / img_shape[<span class="number">0</span>] / math.sqrt(r)</div><div class="line">        w[i+di] = sizes[<span class="number">0</span>] / img_shape[<span class="number">1</span>] * math.sqrt(r)</div><div class="line">    <span class="keyword">return</span> y, x, h, w</div></pre></td></tr></table></figure>
<p>代码结合上面的讲解很好理解，通过上述的步骤，就得到了所有层的default box的y，x，h，w。举例来说，对于第一个特征图而言，y，x，h，w，的shape分别为(38,38,1)，(38,38,1)，(4, )，(4, )。</p>
<h2 id="三、Bboxes-Encode"><a href="#三、Bboxes-Encode" class="headerlink" title="三、Bboxes Encode"></a>三、Bboxes Encode</h2><p>&emsp;&emsp;要想理解这部分，需要知道什么是边框回归，此处有一个很好的讲解博客：<a href="https://blog.csdn.net/zijin0802034/article/details/77685438" target="_blank" rel="external"> 边框回归详解 </a>。知道了什么是边框回归，也就能理解我们这一步要干什么，主要有两个任务：1.给每个default box找到其对应的ground truth box，顺带得到其类别和得分。2.计算其相对于对应的ground truth box的变换，也就是边框回归要得到的目标变换。显然，这两步正是相当于给default boxes打上label的过程，对应之前说过的分类任务和回归任务。</p>
<p>&emsp;&emsp;在train的过程中只用一句话得到了每个default box的分类，边框偏移以及得分（用IOU定义，与GT box的IOU越大，得分越高），如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">gclasses, glocalisations, gscores = \</div><div class="line">    ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)</div></pre></td></tr></table></figure>
<p>这个函数同样在<code>ssd_vgg_300.py</code>，源码是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bboxes_encode</span><span class="params">(self, labels, bboxes, anchors,</span></span></div><div class="line"><span class="function"><span class="params">                  scope=None)</span>:</span></div><div class="line">    <span class="string">"""Encode labels and bounding boxes.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">return</span> ssd_common.tf_ssd_bboxes_encode(</div><div class="line">        labels, bboxes, anchors,</div><div class="line">        self.params.num_classes,</div><div class="line">        self.params.no_annotation_label,</div><div class="line">        ignore_threshold=<span class="number">0.5</span>,</div><div class="line">        prior_scaling=self.params.prior_scaling,</div><div class="line">        scope=scope)</div></pre></td></tr></table></figure>
<p>可以看出，结果是通过一个叫<code>tf_ssd_bboxes_encode</code>的函数获得的，其定义于<code>ssd_common.py</code>，如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_ssd_bboxes_encode</span><span class="params">(labels,</span></span></div><div class="line"><span class="function"><span class="params">                         bboxes,</span></span></div><div class="line"><span class="function"><span class="params">                         anchors,</span></span></div><div class="line"><span class="function"><span class="params">                         num_classes,</span></span></div><div class="line"><span class="function"><span class="params">                         no_annotation_label,</span></span></div><div class="line"><span class="function"><span class="params">                         ignore_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                         prior_scaling=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span></span></div><div class="line"><span class="function"><span class="params">                         dtype=tf.float32,</span></span></div><div class="line"><span class="function"><span class="params">                         scope=<span class="string">'ssd_bboxes_encode'</span>)</span>:</span></div><div class="line">    <span class="string">"""Encode groundtruth labels and bounding boxes using SSD net anchors.</span></div><div class="line"><span class="string">    Encoding boxes for all feature layers.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">      labels: 1D Tensor(int64) containing groundtruth labels;</span></div><div class="line"><span class="string">      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;</span></div><div class="line"><span class="string">      anchors: List of Numpy array with layer anchors;</span></div><div class="line"><span class="string">      matching_threshold: Threshold for positive match with groundtruth bboxes;</span></div><div class="line"><span class="string">      prior_scaling: Scaling of encoded coordinates.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">      (target_labels, target_localizations, target_scores):</span></div><div class="line"><span class="string">        Each element is a list of target Tensors.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(scope):</div><div class="line">        target_labels = []</div><div class="line">        target_localizations = []</div><div class="line">        target_scores = []</div><div class="line">        <span class="keyword">for</span> i, anchors_layer <span class="keyword">in</span> enumerate(anchors):</div><div class="line">            <span class="keyword">with</span> tf.name_scope(<span class="string">'bboxes_encode_block_%i'</span> % i):</div><div class="line">                t_labels, t_loc, t_scores = \</div><div class="line">                    tf_ssd_bboxes_encode_layer(labels, bboxes, anchors_layer,</div><div class="line">                                               num_classes, no_annotation_label,</div><div class="line">                                               ignore_threshold,</div><div class="line">                                               prior_scaling, dtype)</div><div class="line">                target_labels.append(t_labels)</div><div class="line">                target_localizations.append(t_loc)</div><div class="line">                target_scores.append(t_scores)</div><div class="line">        <span class="keyword">return</span> target_labels, target_localizations, target_scores</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;可以看出，类似于anchors的获得，default box的标定也是先一个特征图一个特征图进行，之后再将一个特征图的结果分别放入对应列表中。下面来看每个特征图是如何处理的，处理一个特征图的函数是<code>tf_ssd_bboxes_encode_layer</code>，源码见下面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_ssd_bboxes_encode_layer</span><span class="params">(labels,</span></span></div><div class="line"><span class="function"><span class="params">                               bboxes,</span></span></div><div class="line"><span class="function"><span class="params">                               anchors_layer,</span></span></div><div class="line"><span class="function"><span class="params">                               num_classes,</span></span></div><div class="line"><span class="function"><span class="params">                               no_annotation_label,</span></span></div><div class="line"><span class="function"><span class="params">                               ignore_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                               prior_scaling=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span></span></div><div class="line"><span class="function"><span class="params">                               dtype=tf.float32)</span>:</span></div><div class="line">    <span class="string">"""Encode groundtruth labels and bounding boxes using SSD anchors from</span></div><div class="line"><span class="string">    one layer.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">      labels: 1D Tensor(int64) containing groundtruth labels;</span></div><div class="line"><span class="string">      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;</span></div><div class="line"><span class="string">      anchors_layer: Numpy array with layer anchors;</span></div><div class="line"><span class="string">      matching_threshold: Threshold for positive match with groundtruth bboxes;</span></div><div class="line"><span class="string">      prior_scaling: Scaling of encoded coordinates.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">      (target_labels, target_localizations, target_scores): Target Tensors.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Anchors coordinates and volume.</span></div><div class="line">    yref, xref, href, wref = anchors_layer</div><div class="line">    ymin = yref - href / <span class="number">2.</span></div><div class="line">    xmin = xref - wref / <span class="number">2.</span></div><div class="line">    ymax = yref + href / <span class="number">2.</span></div><div class="line">    xmax = xref + wref / <span class="number">2.</span></div><div class="line">    vol_anchors = (xmax - xmin) * (ymax - ymin)</div><div class="line"></div><div class="line">    <span class="comment"># Initialize tensors...</span></div><div class="line">    shape = (yref.shape[<span class="number">0</span>], yref.shape[<span class="number">1</span>], href.size)</div><div class="line">    feat_labels = tf.zeros(shape, dtype=tf.int64)</div><div class="line">    feat_scores = tf.zeros(shape, dtype=dtype)</div><div class="line"></div><div class="line">    feat_ymin = tf.zeros(shape, dtype=dtype)</div><div class="line">    feat_xmin = tf.zeros(shape, dtype=dtype)</div><div class="line">    feat_ymax = tf.ones(shape, dtype=dtype)</div><div class="line">    feat_xmax = tf.ones(shape, dtype=dtype)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jaccard_with_anchors</span><span class="params">(bbox)</span>:</span></div><div class="line">        <span class="string">"""Compute jaccard score between a box and the anchors.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        int_ymin = tf.maximum(ymin, bbox[<span class="number">0</span>])</div><div class="line">        int_xmin = tf.maximum(xmin, bbox[<span class="number">1</span>])</div><div class="line">        int_ymax = tf.minimum(ymax, bbox[<span class="number">2</span>])</div><div class="line">        int_xmax = tf.minimum(xmax, bbox[<span class="number">3</span>])</div><div class="line">        h = tf.maximum(int_ymax - int_ymin, <span class="number">0.</span>)</div><div class="line">        w = tf.maximum(int_xmax - int_xmin, <span class="number">0.</span>)</div><div class="line">        <span class="comment"># Volumes.</span></div><div class="line">        inter_vol = h * w</div><div class="line">        union_vol = vol_anchors - inter_vol \</div><div class="line">            + (bbox[<span class="number">2</span>] - bbox[<span class="number">0</span>]) * (bbox[<span class="number">3</span>] - bbox[<span class="number">1</span>])</div><div class="line">        jaccard = tf.div(inter_vol, union_vol)</div><div class="line">        <span class="keyword">return</span> jaccard</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersection_with_anchors</span><span class="params">(bbox)</span>:</span></div><div class="line">        <span class="string">"""Compute intersection between score a box and the anchors.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        int_ymin = tf.maximum(ymin, bbox[<span class="number">0</span>])</div><div class="line">        int_xmin = tf.maximum(xmin, bbox[<span class="number">1</span>])</div><div class="line">        int_ymax = tf.minimum(ymax, bbox[<span class="number">2</span>])</div><div class="line">        int_xmax = tf.minimum(xmax, bbox[<span class="number">3</span>])</div><div class="line">        h = tf.maximum(int_ymax - int_ymin, <span class="number">0.</span>)</div><div class="line">        w = tf.maximum(int_xmax - int_xmin, <span class="number">0.</span>)</div><div class="line">        inter_vol = h * w</div><div class="line">        scores = tf.div(inter_vol, vol_anchors)</div><div class="line">        <span class="keyword">return</span> scores</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(i, feat_labels, feat_scores,</span></span></div><div class="line"><span class="function"><span class="params">                  feat_ymin, feat_xmin, feat_ymax, feat_xmax)</span>:</span></div><div class="line">        <span class="string">"""Condition: check label index.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        r = tf.less(i, tf.shape(labels))</div><div class="line">        <span class="keyword">return</span> r[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, feat_labels, feat_scores,</span></span></div><div class="line"><span class="function"><span class="params">             feat_ymin, feat_xmin, feat_ymax, feat_xmax)</span>:</span></div><div class="line">        <span class="string">"""Body: update feature labels, scores and bboxes.</span></div><div class="line"><span class="string">        Follow the original SSD paper for that purpose:</span></div><div class="line"><span class="string">          - assign values when jaccard &gt; 0.5;</span></div><div class="line"><span class="string">          - only update if beat the score of other bboxes.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="comment"># Jaccard score.</span></div><div class="line">        label = labels[i]</div><div class="line">        bbox = bboxes[i]</div><div class="line">        jaccard = jaccard_with_anchors(bbox)</div><div class="line">        <span class="comment"># Mask: check threshold + scores + no annotations + num_classes.</span></div><div class="line">        mask = tf.greater(jaccard, feat_scores)</div><div class="line">        <span class="comment"># mask = tf.logical_and(mask, tf.greater(jaccard, matching_threshold))</span></div><div class="line">        mask = tf.logical_and(mask, feat_scores &gt; <span class="number">-0.5</span>)</div><div class="line">        mask = tf.logical_and(mask, label &lt; num_classes)</div><div class="line">        imask = tf.cast(mask, tf.int64)</div><div class="line">        fmask = tf.cast(mask, dtype)</div><div class="line">        <span class="comment"># Update values using mask.</span></div><div class="line">        feat_labels = imask * label + (<span class="number">1</span> - imask) * feat_labels</div><div class="line">        feat_scores = tf.where(mask, jaccard, feat_scores)</div><div class="line"></div><div class="line">        feat_ymin = fmask * bbox[<span class="number">0</span>] + (<span class="number">1</span> - fmask) * feat_ymin</div><div class="line">        feat_xmin = fmask * bbox[<span class="number">1</span>] + (<span class="number">1</span> - fmask) * feat_xmin</div><div class="line">        feat_ymax = fmask * bbox[<span class="number">2</span>] + (<span class="number">1</span> - fmask) * feat_ymax</div><div class="line">        feat_xmax = fmask * bbox[<span class="number">3</span>] + (<span class="number">1</span> - fmask) * feat_xmax</div><div class="line"></div><div class="line">        <span class="comment"># Check no annotation label: ignore these anchors...</span></div><div class="line">        <span class="comment"># interscts = intersection_with_anchors(bbox)</span></div><div class="line">        <span class="comment"># mask = tf.logical_and(interscts &gt; ignore_threshold,</span></div><div class="line">        <span class="comment">#                       label == no_annotation_label)</span></div><div class="line">        <span class="comment"># # Replace scores by -1.</span></div><div class="line">        <span class="comment"># feat_scores = tf.where(mask, -tf.cast(mask, dtype), feat_scores)</span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> [i+<span class="number">1</span>, feat_labels, feat_scores,</div><div class="line">                feat_ymin, feat_xmin, feat_ymax, feat_xmax]</div><div class="line">    <span class="comment"># Main loop definition.</span></div><div class="line">    i = <span class="number">0</span></div><div class="line">    [i, feat_labels, feat_scores,</div><div class="line">     feat_ymin, feat_xmin,</div><div class="line">     feat_ymax, feat_xmax] = tf.while_loop(condition, body,</div><div class="line">                                           [i, feat_labels, feat_scores,</div><div class="line">                                            feat_ymin, feat_xmin,</div><div class="line">                                            feat_ymax, feat_xmax])</div><div class="line">    <span class="comment"># Transform to center / size.</span></div><div class="line">    feat_cy = (feat_ymax + feat_ymin) / <span class="number">2.</span></div><div class="line">    feat_cx = (feat_xmax + feat_xmin) / <span class="number">2.</span></div><div class="line">    feat_h = feat_ymax - feat_ymin</div><div class="line">    feat_w = feat_xmax - feat_xmin</div><div class="line">    <span class="comment"># Encode features.</span></div><div class="line">    feat_cy = (feat_cy - yref) / href / prior_scaling[<span class="number">0</span>]</div><div class="line">    feat_cx = (feat_cx - xref) / wref / prior_scaling[<span class="number">1</span>]</div><div class="line">    feat_h = tf.log(feat_h / href) / prior_scaling[<span class="number">2</span>]</div><div class="line">    feat_w = tf.log(feat_w / wref) / prior_scaling[<span class="number">3</span>]</div><div class="line">    <span class="comment"># Use SSD ordering: x / y / w / h instead of ours.</span></div><div class="line">    feat_localizations = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=<span class="number">-1</span>)</div><div class="line">    <span class="keyword">return</span> feat_labels, feat_localizations, feat_scores</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个函数比较长，先看第一个函数之前的部分，首先是通过之前得到的一层特征图的y，x，h，w，计算每个default box的四个角的坐标及面积（此处利用了numpy的广播机制），随后初始化了一些空的tensor：类别标签、得分以及每个default box对应的GT box的四个角的坐标。shape也都符合之前的定义，此处以第一个特征图为例，其大小为38$\times$38，每个中心点对应4个default box，每个default box对应一个label，一个GT box和一个得分，所以所有初始化tensor的shape都是(38,38,4)。</p>
<p>&emsp;&emsp;下面是几个辅助函数，<code>jaccard_with_anchors(bbox)</code>用于计算bbox与所有default box的IOU；<code>intersection_with_anchors(bbox)</code>用于计算bbox与所有default box的交比上default box的面积的值，在此处没有用到这个函数；<code>condition</code>，<code>body</code>要和下面的<code>tf.while_loop</code>连起来看，<code>tf.while_loop</code>的执行逻辑是，若condition为真，执行body，<code>condition</code>只有一句话，其实就是判断<code>i</code>的值是否小于GT box的数目，也就是说整个循环逻辑是以每个GT box为单位进行的，<code>body</code>就是对每个GT box进行的操作。</p>
<p>&emsp;&emsp;在看<code>body</code>具体做了什么之前，需要先了解SSD中GT box的匹配机制。共有两个原则，第一个原则是每个GT box与其IOU最大的defaut box匹配，这样就能保证每个GT box都有一个与其匹配的default box。但是这样的话正负样本严重不平衡，因此还需要第二个原则，那就是对于未匹配的default box，若与某个GT box的IOU大于一个阈值（SSD中取0.5），那么也将其进行匹配，此处有一个问题就是若某个default box与几个GT box的IOU都大于阈值，选哪个与其匹配？显然，选与其IOU最大的那个GT box与之匹配。这样就大大增加了正样本的个数。还会有一个矛盾在于，假如一个GT box 1与其IOU最大的default box小于阈值，而该default box与某一个GT box 2的IOU大于阈值，如何选择。此处应该按照第一个原则，选择GT box 1，因为必须要保证每个GT box要有一个default box与之匹配。（实际情况中该矛盾发生可能较低，所以该tensorflow实现中仅实施了第二个原则。）</p>
<p>&emsp;&emsp;下面可以看一下<code>body</code>函数，可以看出，它的逻辑是使用某个GT box与所有default box的已经匹配的GT box的结果去比较，再决定是否更换每个default box对应的GT box。函数中出现了很多mask*A + (1-mask)*B的模式，mask的值只有0和1两种，那么这句话的意义就很显然了，如果mask为1，新值为A，否则为B，对应到具体情况中就是mask为1则更换对应GT box，为0则保持不变，那么决定是否更换的mask的值则来自于前面的条件判断，条件判断如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Mask: check threshold + scores + no annotations + num_classes.</span></div><div class="line">mask = tf.greater(jaccard, feat_scores)</div><div class="line"><span class="comment"># mask = tf.logical_and(mask, tf.greater(jaccard, matching_threshold))</span></div><div class="line">mask = tf.logical_and(mask, feat_scores &gt; <span class="number">-0.5</span>)</div><div class="line">mask = tf.logical_and(mask, label &lt; num_classes)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;此处更改了之前的逻辑，将大于阈值的部分去掉，改为只要大于之前的IOU，就进行GT box的匹配。</p>
<p>&emsp;&emsp;找到了所有default box对应的GT box的四个角的坐标，就可以开始进行边框偏移的计算，在SSD中此处有一个技巧，假设已知default box的位置$\boldsymbol{d=(d^{cx},d^{cy},d^w,d^h)}$，以及它对应的GT box的位置$\boldsymbol{b=(b^{cx},b^{cy},b^w,b^h)}$，通常的边框偏移是按照如下方式计算的</p>
<script type="math/tex; mode=display">
\begin{align}
& t^{cx} = \displaystyle\frac{b^{cx}-d^{cx}}{d^w} \\
& t^{cy} = \displaystyle\frac{b^{cy}-d^{cy}}{d^h} \\
& t^w = \displaystyle log\left(\frac{b^w}{d^w}\right) \\
& t^h = \displaystyle log\left(\frac{b^h}{d^h}\right) \\
\end{align}</script><p>&emsp;&emsp;这个过程称为编码（encode），对应的解码（decode）过程则为</p>
<script type="math/tex; mode=display">
\begin{align}
& b^{cx} = d^wt^{cx} + d^{cx} \\
& b^{cy} = d^ht^{cy} + d^{cy} \\
& b^w = d^wexp(t^w) \\
& b^h = d^hexp(t^h) \\
\end{align}</script><p>&emsp;&emsp;但是在SSD中设置了variance来调整对t的放缩，无论在解码还是编码时都会使用variance来控制，此时编码过程计算如下</p>
<script type="math/tex; mode=display">
\begin{align}
& t^{cx} = \displaystyle\frac{b^{cx}-d^{cx}}{d^w\times variance[0]} \\
& t^{cy} = \displaystyle\frac{b^{cy}-d^{cy}}{d^h\times variance[1]} \\
& t^w = \displaystyle \frac{log\left(\frac{b^w}{d^w}\right)}{variance[2]} \\
& t^h = \displaystyle \frac{log\left(\frac{b^h}{d^h}\right)}{variance[3]} \\
\end{align}</script><p>&emsp;&emsp;解码过程如下</p>
<script type="math/tex; mode=display">
\begin{align}
& b^{cx} = d^w(t^{cx}variance[0]) + d^{cx} \\
& b^{cy} = d^h(t^{cy}variance[1]) + d^{cy} \\
& b^w = d^wexp(t^wvariance[2]) \\
& b^h = d^hexp(t^hvariance[3]) \\
\end{align}</script><p>&emsp;&emsp;variance可以选择训练得到还是手动设定，在SSD中选择手动设定，这也就是<code>SSDParams</code>中<code>parior_scaling</code>中四个数的含义，其实就是对应的variance。</p>
<h2 id="四、网络结构"><a href="#四、网络结构" class="headerlink" title="四、网络结构"></a>四、网络结构</h2><p>&emsp;&emsp;除了对数据的预处理，以及并行化处理意外，接下来就是将数据喂进网络，得到每个default box的分类结果和边框偏移。接着看<code>train_ssd_network.py</code>，可以看到这样一句代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">predictions, localisations, logits, end_points = \</div><div class="line">    ssd_net.net(b_image, is_training=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;它调用了net函数，返回了四个变量，为了清楚这个函数做了什么，提前说明四个变量的含义：<code>predictions</code>就是default box在各个类上的得分，也就是后面的<code>logits</code>通过softmax得到的结果，这样<code>logits</code>是什么就无需解释了，<code>localisations</code>是对default box的边框偏移预测结果，<code>end_points</code>是一个字典，里面储存着各个block的输出特征图。</p>
<p>&emsp;&emsp;下面来看net函数，发现它的核心只有一句话</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">r = ssd_net(inputs,</div><div class="line">            num_classes=self.params.num_classes,</div><div class="line">            feat_layers=self.params.feat_layers,</div><div class="line">            anchor_sizes=self.params.anchor_sizes,</div><div class="line">            anchor_ratios=self.params.anchor_ratios,</div><div class="line">            normalizations=self.params.normalizations,</div><div class="line">            is_training=is_training,</div><div class="line">            dropout_keep_prob=dropout_keep_prob,</div><div class="line">            prediction_fn=prediction_fn,</div><div class="line">            reuse=reuse,</div><div class="line">            scope=scope)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;而ssd_net是定义在跟net相同文件（<code>ssd_vgg_300.py</code>）中的一个函数，我们可以在下面找到它的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_net</span><span class="params">(inputs,</span></span></div><div class="line"><span class="function"><span class="params">            num_classes=SSDNet.default_params.num_classes,</span></span></div><div class="line"><span class="function"><span class="params">            feat_layers=SSDNet.default_params.feat_layers,</span></span></div><div class="line"><span class="function"><span class="params">            anchor_sizes=SSDNet.default_params.anchor_sizes,</span></span></div><div class="line"><span class="function"><span class="params">            anchor_ratios=SSDNet.default_params.anchor_ratios,</span></span></div><div class="line"><span class="function"><span class="params">            normalizations=SSDNet.default_params.normalizations,</span></span></div><div class="line"><span class="function"><span class="params">            is_training=True,</span></span></div><div class="line"><span class="function"><span class="params">            dropout_keep_prob=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">            prediction_fn=slim.softmax,</span></span></div><div class="line"><span class="function"><span class="params">            reuse=None,</span></span></div><div class="line"><span class="function"><span class="params">            scope=<span class="string">'ssd_300_vgg'</span>)</span>:</span></div><div class="line">    <span class="string">"""SSD net definition.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># if data_format == 'NCHW':</span></div><div class="line">    <span class="comment">#     inputs = tf.transpose(inputs, perm=(0, 3, 1, 2))</span></div><div class="line"></div><div class="line">    <span class="comment"># End_points collect relevant activations for external use.</span></div><div class="line">    end_points = &#123;&#125;</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(scope, <span class="string">'ssd_300_vgg'</span>, [inputs], reuse=reuse):</div><div class="line">        <span class="comment"># Original VGG-16 blocks.</span></div><div class="line">        net = slim.repeat(inputs, <span class="number">2</span>, slim.conv2d, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1'</span>)</div><div class="line">        end_points[<span class="string">'block1'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool1'</span>)</div><div class="line">        <span class="comment"># Block 2.</span></div><div class="line">        net = slim.repeat(net, <span class="number">2</span>, slim.conv2d, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv2'</span>)</div><div class="line">        end_points[<span class="string">'block2'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</div><div class="line">        <span class="comment"># Block 3.</span></div><div class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</div><div class="line">        end_points[<span class="string">'block3'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool3'</span>)</div><div class="line">        <span class="comment"># Block 4.</span></div><div class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv4'</span>)</div><div class="line">        end_points[<span class="string">'block4'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool4'</span>)</div><div class="line">        <span class="comment"># Block 5.</span></div><div class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv5'</span>)</div><div class="line">        end_points[<span class="string">'block5'</span>] = net</div><div class="line">        net = slim.max_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'pool5'</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Additional SSD blocks.</span></div><div class="line">        <span class="comment"># Block 6: let's dilate the hell out of it!</span></div><div class="line">        net = slim.conv2d(net, <span class="number">1024</span>, [<span class="number">3</span>, <span class="number">3</span>], rate=<span class="number">6</span>, scope=<span class="string">'conv6'</span>)</div><div class="line">        end_points[<span class="string">'block6'</span>] = net</div><div class="line">        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)</div><div class="line">        <span class="comment"># Block 7: 1x1 conv. Because the fuck.</span></div><div class="line">        net = slim.conv2d(net, <span class="number">1024</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv7'</span>)</div><div class="line">        end_points[<span class="string">'block7'</span>] = net</div><div class="line">        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)</div><div class="line"></div><div class="line">        <span class="comment"># Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).</span></div><div class="line">        end_point = <span class="string">'block8'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = custom_layers.pad2d(net, pad=(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line">            net = slim.conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line">        end_point = <span class="string">'block9'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = custom_layers.pad2d(net, pad=(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line">        end_point = <span class="string">'block10'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line">        end_point = <span class="string">'block11'</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(end_point):</div><div class="line">            net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'conv1x1'</span>)</div><div class="line">            net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3x3'</span>, padding=<span class="string">'VALID'</span>)</div><div class="line">        end_points[end_point] = net</div><div class="line"></div><div class="line">        <span class="comment"># Prediction and localisations layers.</span></div><div class="line">        predictions = []</div><div class="line">        logits = []</div><div class="line">        localisations = []</div><div class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(feat_layers):</div><div class="line">            <span class="keyword">with</span> tf.variable_scope(layer + <span class="string">'_box'</span>):</div><div class="line">                p, l = ssd_multibox_layer(end_points[layer],</div><div class="line">                                          num_classes,</div><div class="line">                                          anchor_sizes[i],</div><div class="line">                                          anchor_ratios[i],</div><div class="line">                                          normalizations[i])</div><div class="line">            predictions.append(prediction_fn(p))</div><div class="line">            logits.append(p)</div><div class="line">            localisations.append(l)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> predictions, localisations, logits, end_points</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;代码结构十分清晰，首先看前面定义网络的部分，这个定网络的定义与VGG16类似，只不过替换了其中某些层，原因在第一部分可以看到，可以看到在这一部分中每个block的输出被放进了<code>end_points</code>字典中。而后面则是根据特征图生成分类结果和偏移结果的部分，可以看到也是逐层进行并放到一个列表中的形式，每一层预测结果的获得都调用了<code>ssd_multibox_layer</code>函数，下面就看一下该函数的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_multibox_layer</span><span class="params">(inputs,</span></span></div><div class="line"><span class="function"><span class="params">                       num_classes,</span></span></div><div class="line"><span class="function"><span class="params">                       sizes,</span></span></div><div class="line"><span class="function"><span class="params">                       ratios=[<span class="number">1</span>],</span></span></div><div class="line"><span class="function"><span class="params">                       normalization=<span class="number">-1</span>,</span></span></div><div class="line"><span class="function"><span class="params">                       bn_normalization=False)</span>:</span></div><div class="line">    <span class="string">"""Construct a multibox layer, return a class and localization predictions.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    net = inputs</div><div class="line">    <span class="keyword">if</span> normalization &gt; <span class="number">0</span>:</div><div class="line">        net = custom_layers.l2_normalization(net, scaling=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># Number of anchors.</span></div><div class="line">    num_anchors = len(sizes) + len(ratios)</div><div class="line"></div><div class="line">    <span class="comment"># Location.</span></div><div class="line">    num_loc_pred = num_anchors * <span class="number">4</span></div><div class="line">    loc_pred = slim.conv2d(net, num_loc_pred, [<span class="number">3</span>, <span class="number">3</span>], activation_fn=<span class="keyword">None</span>,</div><div class="line">                           scope=<span class="string">'conv_loc'</span>)</div><div class="line">    loc_pred = custom_layers.channel_to_last(loc_pred)</div><div class="line">    loc_pred = tf.reshape(loc_pred,</div><div class="line">                          tensor_shape(loc_pred, <span class="number">4</span>)[:<span class="number">-1</span>]+[num_anchors, <span class="number">4</span>])</div><div class="line">    <span class="comment"># Class prediction.</span></div><div class="line">    num_cls_pred = num_anchors * num_classes</div><div class="line">    cls_pred = slim.conv2d(net, num_cls_pred, [<span class="number">3</span>, <span class="number">3</span>], activation_fn=<span class="keyword">None</span>,</div><div class="line">                           scope=<span class="string">'conv_cls'</span>)</div><div class="line">    cls_pred = custom_layers.channel_to_last(cls_pred)</div><div class="line">    cls_pred = tf.reshape(cls_pred,</div><div class="line">                          tensor_shape(cls_pred, <span class="number">4</span>)[:<span class="number">-1</span>]+[num_anchors, num_classes])</div><div class="line">    <span class="keyword">return</span> cls_pred, loc_pred</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>ssd_multibox_layer</code>同样定义于<code>ssd_vgg_300.py</code>中，可以看到一开始对normalization值进行了判断，此处就是<code>SSDParams</code>中<code>normalizations</code>的作用，在SSD中由于第一个要提取特征层较浅，其norm较大，所以要对其进行沿channel方向的l2_normalize，而其他层无需进行此操作，代码中也仅是判断了<code>normalization</code>的值是否大于0，所以在<code>normalizations</code>中第一个值大于0，其他都小于0，20和-1无实际含义。</p>
<p>&emsp;&emsp;下面则是对location的计算，首先使用了一个3*3卷积，通道数是每个中心default box的个数乘4，代表y，x，w，h四个偏移，从而得到了每个中心的边框偏移的结果，此处又进行了一个reshape操作，其中<code>tensor_shape</code>得到<code>loc_pred</code>的形状，再通过切片将最后一维去掉，再加上两维，分别是每个中心default box的个数，和4，这样就得到了[batch_size,size[0],size[1],num_anchors,4]的tensor。以第二个特征层为例，<code>loc_pred</code>的形状为[batch_size,19,19,6,4]，同理可以得到第二个特征层的<code>cls_pred</code>的形状为[batch_size,19,19,6,21]。</p>
<h2 id="五、loss的计算"><a href="#五、loss的计算" class="headerlink" title="五、loss的计算"></a>五、loss的计算</h2><p>&emsp;&emsp;SSD的loss是一个multitask的loss，包含分类损失和定位损失，公式如下所示</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af679cd911.png" alt="total_loss.png"></p>
<p>&emsp;&emsp;所有此处所有的loss值均是对一张图而言的，式子中的$N$代表所有default box中正样本（有对应GT box）的数量，$\alpha$用于调整分类损失和定位损失的比例，下面看一下二者的具体计算。</p>
<p>&emsp;&emsp;首先看分类损失</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af679a4ee9.png" alt="conf_loss.png"></p>
<p>&emsp;&emsp;式子中的$x_{ij}^p\in \{0,1\}$，类似于一个指示函数，当第i个default box与第j个GT box匹配并属于第p类时，$x_{ij}^p=1$，其他情况下$x_{ij}^p=0$。显然$c_i^p$就是之前得到的<code>logits</code>，所以整个式子其实就是一个交叉熵损失。</p>
<p>​    接下来看一下定位损失。</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af67a24d49.png" alt="loc_loss.png"></p>
<p>&emsp;&emsp;定位损失中的$x_{ij}^p$与分类损失中的含义相同，$\hat g_j^m$根据下面的定义，含义是default box到其对应GT box的偏移，$l_i^m$则是预测的偏移，对二者的误差使用了smooth L1 loss。</p>
<p>&emsp;&emsp;与之前的过程类似，可以在<code>train_ssd_network.py</code>中看到求loss的代码，如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ssd_net.losses(logits, localisations,</div><div class="line">               b_gclasses, b_glocalisations, b_gscores,</div><div class="line">               match_threshold=FLAGS.match_threshold,</div><div class="line">               negative_ratio=FLAGS.negative_ratio,</div><div class="line">               alpha=FLAGS.loss_alpha,</div><div class="line">               label_smoothing=FLAGS.label_smoothing)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;它调用了<code>SSDNet</code>的<code>losses</code>函数，只有一句话</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">losses</span><span class="params">(self, logits, localisations,</span></span></div><div class="line"><span class="function"><span class="params">           gclasses, glocalisations, gscores,</span></span></div><div class="line"><span class="function"><span class="params">           match_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">           negative_ratio=<span class="number">3.</span>,</span></span></div><div class="line"><span class="function"><span class="params">           alpha=<span class="number">1.</span>,</span></span></div><div class="line"><span class="function"><span class="params">           label_smoothing=<span class="number">0.</span>,</span></span></div><div class="line"><span class="function"><span class="params">           scope=<span class="string">'ssd_losses'</span>)</span>:</span></div><div class="line">    <span class="string">"""Define the SSD network losses.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">return</span> ssd_losses(logits, localisations,</div><div class="line">                      gclasses, glocalisations, gscores,</div><div class="line">                      match_threshold=match_threshold,</div><div class="line">                      negative_ratio=negative_ratio,</div><div class="line">                      alpha=alpha,</div><div class="line">                      label_smoothing=label_smoothing,</div><div class="line">                      scope=scope)</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;它调用的<code>ssd_losses</code>是定义在相同文件中的函数，为了减少说明，使用了网上的有注释的版本，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># =========================================================================== #</span></div><div class="line"><span class="comment"># SSD loss function.</span></div><div class="line"><span class="comment"># =========================================================================== #</span></div><div class="line"><span class="comment">#logits.shape=[(5,38,38,4,21),(5,19,19,6,21),(5,10,10,6,21),(5,5,5,6,21),(5,3,3,4,21),(5,1,1,4,21)]</span></div><div class="line"><span class="comment">#localisations.shape=[(5,38,38,4,4),(5,19,19,6,4),(5,10,10,6,4),(5,5,5,6,4),(5,3,3,4,4),(5,1,1,4,4)],glocalisations同</span></div><div class="line"><span class="comment">#gclasses.shape=[(5,38,38,4),.................], gscores同</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_losses</span><span class="params">(logits, localisations,             </span></span></div><div class="line"><span class="function"><span class="params">               #预测类别, 预测位置</span></span></div><div class="line"><span class="function"><span class="params">               gclasses, glocalisations, gscores,</span></span></div><div class="line"><span class="function"><span class="params">               #ground truth 类别, ground truth 位置, ground truth 分数</span></span></div><div class="line"><span class="function"><span class="params">               match_threshold=<span class="number">0.5</span>,</span></span></div><div class="line"><span class="function"><span class="params">               negative_ratio=<span class="number">3.</span>,</span></span></div><div class="line"><span class="function"><span class="params">               alpha=<span class="number">1.</span>,</span></span></div><div class="line"><span class="function"><span class="params">               label_smoothing=<span class="number">0.</span>,</span></span></div><div class="line"><span class="function"><span class="params">               device=<span class="string">'/cpu:0'</span>,</span></span></div><div class="line"><span class="function"><span class="params">               scope=None)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(scope, <span class="string">'ssd_losses'</span>):</div><div class="line">        lshape = tfe.get_shape(logits[<span class="number">0</span>], <span class="number">5</span>)</div><div class="line">        num_classes = lshape[<span class="number">-1</span>]</div><div class="line">        batch_size = lshape[<span class="number">0</span>]	<span class="comment">#5</span></div><div class="line"> </div><div class="line">        <span class="comment"># Flatten out all vectors!</span></div><div class="line">        flogits = []</div><div class="line">        fgclasses = []</div><div class="line">        fgscores = []</div><div class="line">        flocalisations = []</div><div class="line">        fglocalisations = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(logits)):</div><div class="line">            flogits.append(tf.reshape(logits[i], [<span class="number">-1</span>, num_classes]))</div><div class="line">            fgclasses.append(tf.reshape(gclasses[i], [<span class="number">-1</span>]))</div><div class="line">            fgscores.append(tf.reshape(gscores[i], [<span class="number">-1</span>]))</div><div class="line">            flocalisations.append(tf.reshape(localisations[i], [<span class="number">-1</span>, <span class="number">4</span>]))</div><div class="line">            fglocalisations.append(tf.reshape(glocalisations[i], [<span class="number">-1</span>, <span class="number">4</span>]))</div><div class="line"> </div><div class="line">        <span class="comment">#flogits.shape=[shape=(5×38×38×4,21), shape=(5×19×19×6,21), ......], 共6个feature map的组成</span></div><div class="line">        <span class="comment">#fgclasses.shape=[shape=(5×38×38×4), shape=(5×19×19×6), ......]其它相似</span></div><div class="line"> </div><div class="line">        <span class="comment"># And concat the crap!</span></div><div class="line">        <span class="comment">#logits.shape=(5×38×38×4+5×19×19×6+...+5×1×1×4, 21)</span></div><div class="line">        <span class="comment">#将[flogits[1],flogits[2],...,flogits[i],...]按第一维组合在一起，下同</span></div><div class="line">        logits = tf.concat(flogits, axis=<span class="number">0</span>) </div><div class="line">        </div><div class="line">        gclasses = tf.concat(fgclasses, axis=<span class="number">0</span>)</div><div class="line">        gscores = tf.concat(fgscores, axis=<span class="number">0</span>)</div><div class="line">        localisations = tf.concat(flocalisations, axis=<span class="number">0</span>)</div><div class="line">        glocalisations = tf.concat(fglocalisations, axis=<span class="number">0</span>)</div><div class="line">        dtype = logits.dtype</div><div class="line"> </div><div class="line">        <span class="comment"># Compute positive matching mask...</span></div><div class="line">        pmask = gscores &gt; match_threshold   <span class="comment">#得分&gt;0.5的为正样本(掩码)</span></div><div class="line">        fpmask = tf.cast(pmask, dtype)</div><div class="line">        n_positives = tf.reduce_sum(fpmask) <span class="comment">#正样本数</span></div><div class="line"> </div><div class="line">        <span class="comment"># Hard negative mining...</span></div><div class="line">        no_classes = tf.cast(pmask, tf.int32)</div><div class="line">        predictions = slim.softmax(logits)</div><div class="line">        nmask = tf.logical_and(tf.logical_not(pmask), <span class="comment">#得分&gt;-0.5且&lt;=0.5的样本</span></div><div class="line">                               gscores &gt; <span class="number">-0.5</span>)</div><div class="line">        fnmask = tf.cast(nmask, dtype) </div><div class="line">        </div><div class="line">        <span class="comment">#得分&gt;-0.5且&lt;=0.5的样本在第0类（负样本）处的预测值（softmax）. </span></div><div class="line">        <span class="comment">#nvalues=[[p1],[p2],...,[pN]],N为一个batch中的anchors的总数,</span></div><div class="line">        <span class="comment">#满足score&gt;0.5的样本,pi=0</span></div><div class="line">        nvalues = tf.where(nmask,                     </div><div class="line">                           predictions[:, <span class="number">0</span>],</div><div class="line">                           <span class="number">1.</span> - fnmask)</div><div class="line">                           </div><div class="line">        <span class="comment">#nvalues_flat=[p1,p2,...,pN]</span></div><div class="line">        nvalues_flat = tf.reshape(nvalues, [<span class="number">-1</span>])</div><div class="line">        </div><div class="line">        <span class="comment"># Number of negative entries to select.</span></div><div class="line">        <span class="comment">#负样本数取满足-0.5&lt;score&lt;=0.5和3倍正样本数中的最小值（保证正负样本比例不小于1:3）</span></div><div class="line">        max_neg_entries = tf.cast(tf.reduce_sum(fnmask), tf.int32)</div><div class="line">        n_neg = tf.cast(negative_ratio * n_positives, tf.int32) + batch_size</div><div class="line">        n_neg = tf.minimum(n_neg, max_neg_entries)</div><div class="line"> </div><div class="line">        <span class="comment">#返回-nvalues_flat中最大的k(n_neg)值,和其索引(从0开始),即nvalues_flat中最小的k个值</span></div><div class="line">        val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg)</div><div class="line">        <span class="comment">#nvalues_flat中最小的k个值中的最大值,对应样本记为Max Negative Hard样本</span></div><div class="line">        max_hard_pred = -val[<span class="number">-1</span>]</div><div class="line">        <span class="comment"># Final negative mask.</span></div><div class="line">        <span class="comment">#最终负样本为置信度小于Max Negative Hard的所有样本</span></div><div class="line">        nmask = tf.logical_and(nmask, nvalues &lt; max_hard_pred)</div><div class="line">        fnmask = tf.cast(nmask, dtype)</div><div class="line"> </div><div class="line">        <span class="comment"># Add cross-entropy loss.</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entropy_pos'</span>):</div><div class="line">            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,</div><div class="line">                                                                  labels=gclasses)</div><div class="line">            </div><div class="line">            <span class="comment">#loss乘正样本掩码得符合条件的正样本损失；损失除以batch_size(这里为5)</span></div><div class="line">            loss = tf.div(tf.reduce_sum(loss * fpmask), batch_size, name=<span class="string">'value'</span>) </div><div class="line">            tf.losses.add_loss(loss)		<span class="comment">#将当前loss添加到总loss集合</span></div><div class="line"> </div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entropy_neg'</span>):</div><div class="line">            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,</div><div class="line">                                                                  labels=no_classes)</div><div class="line">            loss = tf.div(tf.reduce_sum(loss * fnmask), batch_size, name=<span class="string">'value'</span>)</div><div class="line">            tf.losses.add_loss(loss)		<span class="comment">#将当前loss添加到总loss集合</span></div><div class="line"> </div><div class="line">        <span class="comment"># Add localization loss: smooth L1, L2, ...</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'localization'</span>):</div><div class="line">            <span class="comment"># Weights Tensor: positive mask + random negative.</span></div><div class="line">            weights = tf.expand_dims(alpha * fpmask, axis=<span class="number">-1</span>) <span class="comment">#位置项损失权重α</span></div><div class="line">            loss = custom_layers.abs_smooth(localisations - glocalisations)</div><div class="line">            loss = tf.div(tf.reduce_sum(loss * weights), batch_size, name=<span class="string">'value'</span>)</div><div class="line">            </div><div class="line">            <span class="comment">#将当前loss添加到总loss集合，最后通过tf.losses.get_total_loss()计算所有的loss</span></div><div class="line">            tf.losses.add_loss(loss)</div></pre></td></tr></table></figure>
<h2 id="六、Data-Augmentation"><a href="#六、Data-Augmentation" class="headerlink" title="六、Data Augmentation"></a>六、Data Augmentation</h2><p>&emsp;&emsp;此处数据增强在tf中有很多辅助函数，而pytorch对ssd的实现中（<a href="https://github.com/amdegroot/ssd.pytorch" target="_blank" rel="external">amdegroot/ssd.pytorch</a>）数据增强的部分都是使用的自己写的函数，先读HSV颜色空间相关内容，占坑。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文阅读的版本是tensorflow对SSD的实现，相对而言阅读难度要远低于caffe版本的实现，源代码可见&lt;a href=&quot;https://github.com/balancap/SSD-Tensorflow/&quot; target=&quot;_blank&quot; r
    
    </summary>
    
      <category term="CV" scheme="https://mingming97.github.io/categories/CV/"/>
    
    
      <category term="CV" scheme="https://mingming97.github.io/tags/CV/"/>
    
      <category term="Object Detection" scheme="https://mingming97.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>word2vec理解</title>
    <link href="https://mingming97.github.io/2018/06/08/Word2Vec-basic/"/>
    <id>https://mingming97.github.io/2018/06/08/Word2Vec-basic/</id>
    <published>2018-06-08T02:14:01.000Z</published>
    <updated>2019-01-13T08:36:11.331Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文需要对n-gram模型在评估语句上的应用有一定了解（n-gram还有一些其他应用，如模糊匹配、特征工程等），可以看作对<a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="external">word2vec 中的数学原理详解</a>这篇博文的一些思考。</p>
<p>&emsp;&emsp;首先一定要建立起word2vec的基本认识，在NLP的一些任务中，我们需要将自然语言进行处理并发现其中的规律。但是机器没有办法直接理解自然语言，这就需要我们将其抽象化，换成一种机器能理解的表示方式。这就是word2vec的目的，将训练集中的词语表示成n维向量空间中的一个向量，而这个向量要对解决的问题有着尽可能好的特征表示。由此可以看出，word2vec与要解决的问题（也就是要训练的语言模型）是紧密相关的，所以一般情况下他们是一起进行训练的。例如keras中的Embedding层，官方解释是将字典下标转换为固定大小的向量。事实上这就是word2vec的一个过程，暂且不提keras是如何做的，要明确的一点是，Embedding层中的参数也是要随着训练的模型一起更新的，这有助于更好地理解之后的Skip-gram和CBOW模型。</p>
<h2 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h2><p>&emsp;&emsp;先看一下通用的语言模型，如下图所示。下面对这个图做出一些解释，这是一个三层语言模型，首先看输入，$w_{t-n+1},\dots,w_{t-2}, w_{t-1}$等是$w_t$之前的n-1个词，index的含义是他们在词典中的下标；再看输出，第i个输出是$P(w_t=i|context)$，其中context表示$w_t$的上下文，在这里表示输入的之前的n-1个词。所以这个模型就是计算了已知前n-1个词，而第n个词为i的条件概率。按照论文中的解释，这个网络有三层，如下：</p>
<ul>
<li><p>输入层（Input Layer）：负责将词向量输入，也就是$C(w_{t-n+1}),\dots,C(w_{t-2}),C(w_{t-1})$，每个用m维向量表示，共n-1个首尾相接，所以形成了一个m(n-1)维的向量。</p>
</li>
<li><p>隐含层（Hidden Layer）：设之前得到的m(n-1)维的向量为X，则隐含层的输出$Z = tanh(WX + p)$。其中W为该层权重，也就是要训练的参数，p是bias。输出使用tanh作为activation function，所以不用做smoothing。</p>
</li>
<li><p>输出层（Outpu Layer）：输出层的结点数与词典的大小相同，代表下一个词是词典中某一个词的概率。按照图中所说就是$O = softmax(UZ + q)$。其中U为该层权重，q是bias，softmax是activation function。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/01/13/5c3af82c18641.png" alt="network.png"></p>
<p>  &emsp;&emsp;之后要介绍的网络都建立在这个三层模型的基础上，只是对一些流程做出了改动。</p>
<h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><p>&emsp;&emsp;这个模型要解决的问题其实就是在知道当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的情况下，预测$w_t$。也就是要计算$P(w_t|context(w_t))$。大体如下图所示：</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af82bcf170.jpg" alt="CBOW-Architecture.jpg"></p>
<p>&emsp;&emsp;对于其模型而言，去掉了之前的Hidden Layer，增加了一个Projection Layer，之前是将所有向量拼接，而现在是将所有向量求和，减少了参数数目，大体的模型如下。</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af82be502d.png" alt="CBOWmodel.png"></p>
<ul>
<li><p>输入层：$Context(w)$中2c个词的词向量。</p>
</li>
<li><p>投影层：输入的2c个词向量的累加，即$x_w = \displaystyle\sum_{i=1}^{2c}v(Context(w)_i)$。</p>
</li>
<li><p>输出层：是一棵以词频为权值的Huffman树，这其中每一个叶子节点都代表一个单词，每一个非叶子节点都可以视作一次二分类。</p>
</li>
</ul>
<p>&emsp;&emsp;如果我们将其看作多分类问题，由于词典非常庞大，softmax的计算时间会线性增长，开销太大。所以此处采用了一种叫hierarchical softmax的技术，很形象，将输出层变成了一棵二叉树，并对每个非叶子节点做logisitic回归，目标节点就是我们训练集中的真实的词，也就是从根节点到目标层对应的路径就是每次分类正确的结果。</p>
<p>  &emsp;&emsp;如何更新参数呢。对于单个非叶子节点而言，它被分为正类的概率是$\sigma(x_w^T\theta)= \displaystyle\frac{1}{1+e^{-x_w^T\theta}}$，分为负类的概率是$1-\sigma(x_w^T\theta)$。在word2vec的源码中，将哈夫曼编码为1（向左走）视为负类，0（向右走）视为正类。不妨设到目标叶子节点共有s层，其中从根节点到目标叶子节点之前的非叶子节点的参数为$\theta_1,\theta_2,\dots,\theta_{s-1}$，目标叶子节点的哈夫曼编码为$d_2,d_3,\dots,d_s$，则可以得到</p>
<script type="math/tex; mode=display">
p(d_j|x_w, \theta_{j-1})=
\begin{cases}
\sigma(x_w^T\theta_{j-1}), &d_j = 0;\\
1-\sigma(x_w^T\theta_{j-1}), &d_j = 1,
\end{cases}</script><p>写成整体表达式就是</p>
<script type="math/tex; mode=display">
p(d_j|x_w, \theta_{j-1}) = \left[\sigma(x_w^T\theta_{j-1})\right]^{1-d_j}\cdot\left[1-\sigma(x_w^T\theta_{j-1})\right]^{d_j}</script><p>以上是一个结点的情况，如要求$p(w|Context(w))$将每个节点概率相乘即可，也就是</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\displaystyle\prod_{j=2}^{s}p(d_j|x_w, \theta_{j-1})</script><p>为了方便计算取对数，表示如下</p>
<script type="math/tex; mode=display">
\begin{align}
L &= \displaystyle\sum_{w\in C}log\prod_{j=2}^{s_w}\left[p(d_j^w|x_w, \theta_{j-1}^w)\right] \\
&=\displaystyle\sum_{w \in C}\sum_{j=2}^{s_w}\left\{(1-d_j^w)log\left[\sigma(x_w^T\theta_{j-1}^w)\right]+d_j^wlog\left[1-\sigma(x_w^T\theta_{j-1}^w)\right]\right\}
\end{align}</script><p>以上就是CBOW的目标函数，注意d和$\theta$都有上标的原因是表示他们是$w$相关的。由于我们的目标是将这个函数最大化，所以要用Gradient Ascent。对于我们的整个模型，需要训练的参数有$\theta_{j-1}^w,x_w$，分别对他们求梯度，由于是加法，又由于对一个固定的w和j，对$\theta_j^w$求偏导有贡献的项仅仅有花括号中的w和j相同的对应项，而对$x_w$而言有贡献的是$j\in[2, s_w]$部分的花括号中的内容的偏导和，为方便计算，将花括号中的内容记作T。</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial T}{\partial\theta_{j-1}^w} &= \displaystyle\frac{\partial}{\partial\theta_{j-1}^w}\left\{(1-d_j^w)log\left[\sigma(x_w^T\theta_{j-1}^w)\right]+d_j^wlog\left[1-\sigma(x_w^T\theta_{j-1}^w)\right]\right\}\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))\displaystyle\frac{\partial x_w^T\theta_{j-1}^w}{\partial\theta_{j-1}^w} + d_j^w\cdot(-\sigma(x_w^T\theta_{j-1}^w))\displaystyle\frac{\partial x_w^T\theta_{j-1}^w}{\partial\theta_{j-1}^w}\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))x_w - d_j^w\sigma(x_w^T\theta_{j-1}^w)x_w\ \ \ \ (矩阵求导分母布局)\\
&=\left[(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]x_w
\end{align}</script><p>对于$\displaystyle\frac{\partial T}{\partial x_w}$，与上面的过程类似，可以得到</p>
<script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial x_w} =\left[(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]\theta_{j-1}^w</script><p>显然，对于参数$\theta_{j-1}^w$的更新，很容易，只需要</p>
<script type="math/tex; mode=display">
\theta_{j-1}^w\leftarrow\theta_{j-1}^w+\eta\left[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]x_w</script><p>而对于$x_w$，它是通过v(w)累加来的，所以我们真正应该更新的是v(w)，word2vec的做法如下</p>
<script type="math/tex; mode=display">
v(w)\leftarrow v(w) + \eta\displaystyle\sum_{j=2}^{s_w}\left[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)\right]\theta_{j-1}^w,\ \ \ \ w\in Context(w)</script><p>这样做当然有一定道理，因为我们仅仅求出了他们的一个集体梯度，所以要将这个集体梯度反馈到每一个个体上。</p>
<p>综上对于一个训练样本$(Context(w), w)$，CBOW伪代码如下</p>
<script type="math/tex; mode=display">
e\leftarrow0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ \ x_w\leftarrow\displaystyle\sum_{w\in Context(w)} v(w)\\
for \ j \leftarrow2\ to\ s_w\ do\\
\ \ q \leftarrow \sigma(x_w^T\theta_{j-1}^w)\\
\ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^w-q)\\
\ \ e\leftarrow e + g\theta_{j-1}^w\\
\ \ \ \ \ \ \ \ \ \theta_{j-1}^w\leftarrow \theta_{j-1}^w+gx_w\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ for \ u \in Context(w) do\\
\ \ \ \ \ \ v(u) \leftarrow v(u) + e</script><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><p>&emsp;&emsp;与CBOW相反，这个模型要解决的问题是在已知词$w_t$的情况下，预测词$w_{t-2}, w_{t-1},w_{t+1},w_{t+2}$。也就是要计算$P(Context(w_t)|w_t)$，如下图所示：</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af82bd993d.jpg" alt="Skip-Gram-Architecture-26.jpg"></p>
<p>&emsp;&emsp;对于其模型，与CBOW模型类似，为了与其对比，保留了projection layer，如下：</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af82c03dcc.png" alt="SKIPGRAMmodel.png"></p>
<ul>
<li><p>输入层：词向量$v(w)$</p>
</li>
<li><p>投影层：为了与CBOW保持类似保留的，无实际作用。</p>
</li>
<li><p>输出层：一棵Huffman树。与CBOW类似，也是为了利用Hierarchical softmax，所输出的一棵以单词为叶子节点，非叶子节点作为二分类的单元的哈夫曼树。</p>
</li>
</ul>
<p>如何表示$P(Context(w)|w)$呢，可以将其拆解如下：</p>
<script type="math/tex; mode=display">
p(Context(w)|w) = \displaystyle\prod_{u \in Context(w)}p(u|w)</script><p>此处的$p(u|w)$可以借助CBOW的表示方式，表示为：</p>
<script type="math/tex; mode=display">
p(u|w) = \displaystyle\prod_{j=2}^{s}p(d_j|v_w, \theta_{j-1})</script><p>此处的$\theta,d_j$在总体表示中，同样是与语料中的$u$是相关的，将p写作二分类的形式后再取对数（与CBOW中操作相似），得到总体的目标函数</p>
<script type="math/tex; mode=display">
\begin{align}
L&= \displaystyle\sum_{w\in C}log\prod_{u\in Context(w)}\prod_{j=2}^{s_u}\left[p(d_j^u|v_w,\theta_{j-1}^u)\right]\\
&= \displaystyle\sum_{w \in C}\sum_{u\in Context(w)}\sum_{j=2}^{s_u} \left\{(1-d_j^u)log\left[\sigma(v_w^T\theta_{j-1}^u)\right]+d_j^ulog\left[1-\sigma(v_w^T\theta_{j-1}^u)\right]\right\}
\end{align}</script><p>出于与CBOW中同样的考虑，可将后面花括号中的内容记为T，与CBOW中推导过程完全相同，得到下面的结果：</p>
<script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial\theta_{j-1}^u} =\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]v_w</script><script type="math/tex; mode=display">
\displaystyle\frac{\partial T}{\partial v_w}=\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]\theta_{j-1}^u</script><p> 参数更新也是类似的（此处也是要求目标函数最大值）</p>
<script type="math/tex; mode=display">
\theta_{j-1}^u \leftarrow \theta_{j-1}^u + \eta\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]v_w</script><script type="math/tex; mode=display">
v_w\leftarrow v_w + \eta\displaystyle\sum_{u\in Context(w)}\sum_{j=2}^{s_u}\left[(1-d_j^u-\sigma(v_w^T\theta_{j-1}^u)\right]\theta_{j-1}^u</script><p>综上对于一个训练样本$(w, Context(w))$，Skip-gram伪代码如下</p>
<script type="math/tex; mode=display">
e\leftarrow 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
for\ u \in Context(w)\ do \\
for \ j \leftarrow2\ to\ s_u\ do\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q \leftarrow \sigma\left[v(w)^T\theta_{j-1}^u\right]\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^u-q)\\
\ \ \ \ \ \ \ \ e\leftarrow e + g\theta_{j-1}^u\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta_{j-1}^u\leftarrow \theta_{j-1}^u+gv(w)\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
v(w) \leftarrow v(w)+e\ \ \ \ \ \ \ \ \ \ \ \</script><p>但是在word2vec中，真实的做法是每处理一个$Context(w)$中的词，就进行一次更新，也就是如下的步骤：</p>
<script type="math/tex; mode=display">
for\ u \in Context(w)\ do \\
e\leftarrow 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
for \ j \leftarrow2\ to\ s_u\ do\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q \leftarrow \sigma\left[v(w)^T\theta_{j-1}^u\right]\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g \leftarrow\eta(1-d_j^u-q)\\
\ \ \ \ \ \ \ \ e\leftarrow e + g\theta_{j-1}^u\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta_{j-1}^u\leftarrow \theta_{j-1}^u+gv(w)\\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
v(w) \leftarrow v(w)+e\ \ \ \\
end\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\</script><p>&emsp;&emsp;以上两种就是基于Hierarchical softmax的CBOW和Skip-gram模型，还有基于Negative Sampling的两种模型，在这里暂且不提。如果想得知更多细节，可以在源码中获得，我个人只看过java版本的，没看过纯C版本的，尽管语言不同，但我想在一些处理细节上是相同的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文需要对n-gram模型在评估语句上的应用有一定了解（n-gram还有一些其他应用，如模糊匹配、特征工程等），可以看作对&lt;a href=&quot;https://blog.csdn.net/itplus/article/details/37969519&quot; 
    
    </summary>
    
      <category term="NLP" scheme="https://mingming97.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://mingming97.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>中文分词</title>
    <link href="https://mingming97.github.io/2018/05/17/chinese-split-word/"/>
    <id>https://mingming97.github.io/2018/05/17/chinese-split-word/</id>
    <published>2018-05-17T09:40:01.000Z</published>
    <updated>2019-01-13T08:37:19.853Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p>&emsp;&emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有三个要素：A是状态转移概率分布矩阵，简单说就是在任一时刻从一个隐含状态到另一个隐含状态的转移概率构成的矩阵；B是观测概率分布矩阵，其实就是在任一时刻给定隐含状态s生成观测状态o的条件概率$P(o|s)$构成的矩阵；$\pi$是初始概率矩阵，也就是在初始状态下各隐含状态的概率。而一般的HMM模型有三个基本问题：1. 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，计算$P(O|\lambda)$，这是评估问题。2. 给定观测序列$O = \{o_1, o_2, \dots, o_t\}$，求解模型$\lambda = (A, B, \pi$，使得$P(O|\lambda)$尽可能大，这是学习问题，若给定隐含状态序列S可以考虑用maximum likelihood来解决，若隐含状态序列则可以用Baum-Welch算法解决，不过这并不是本文重点。3. 给定 给定模型$\lambda = (A, B, \pi)$和观测序列$O = \{o_1, o_2, \dots, o_t\}$，求使得$P(S|O)$最大的隐含状态序列$S = \{s_1, s_2, \dots, s_t\}$，这被称为解码问题或预测问题。对于分词这个任务来说，主要涉及到的是第三个问题。<br>&emsp;&emsp;jieba分词的源码就提供了解决这个问题的一个很好的范例。将隐含状态集合定义为$\{S, B, M, E\}$，S的含义是单字，B的含义是词头，M的含义是词中，E的含义是词尾。在 <code>jieba/finaseg/prob_start.py</code>中定义了初始概率$\pi$，在 <code>jieba/finaseg/prob_trans.py</code>中定义了状态转移概率$A$，在<code>jieba/finaseg/prob_emit.py</code>中定义了状态观测概率分布$B$，在用基于统计的方法获得以上这些之后就用Viterbi算法求一条使得$P(S|O)$最大的路径就好。（关于Viterbi算法还是在另篇文章中再说。）<br>&emsp;&emsp;考虑一下，此时如果我们不知道B，该如何定义要求解的函数。可以试着模仿Viterbi的想法，用$\delta_i(s)$表示到第i个字时状态为s时的最优值，则$\delta_{i+1}(s’) = max\{\delta_i(s)a_{ss’}P(s|i),  s\in\{S, B, M, E\}\}$，其中$a_{ss’}$是转移概率，$P(s|i)$表示第i个字状态是s的概率（这样定义是有着一定数学原理的，具体推导也借鉴了Viterbi算法原本的定义，核心思想是极大似然）。转移概率可通过统计的方法得到，那么$P(s|i)$呢？影响到这个概率的因素很多，不妨将这个问题转化为一个seq2seq的问题，输入一个序列，输出各个位置的4-tag标注。中文中通过前后文语境都能作为序列标注的依据，从而考虑使用Bi-directional的LSTM来进行这个任务。只要将输出接一层softmax就可以将结果当作概率使用。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>&emsp;&emsp;有一个经典的亚研院的语料库就是4-tag标注的，大概长这个样子。</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af89cb9124.png" alt="msrtrain.png"></p>
<p>&emsp;&emsp;首先要明确我们训练数据的lstm的输入，应该是batch_size * sentence_len的一个tensor。以标点符号为分隔，汉语中单句话一般没有太长，所以此处统一每个句向量的长度为32。获得句向量的方式很简单，将文章中所有出现的字做成一个字典，将每个字用其在字典中对应的下位置表示，不足的长度用0补齐，就得到了句向量的表示。而对于tag，则可以使用one-hot的编码，注意要为补足句向量的0留一个编码位置，所以一共有5类tag。因此一句话中tag为s的字将被编码为[1, 0, 0, 0, 0]，tag为b的将被编码为[0, 1, 0, 0, 0]，以此类推。<br>&emsp;&emsp;明确了以上过程，就可以开始细节上的处理。注意到每行前面有很多“/s，”/s，‘/s，’/s一类的东西，我们的数据是不需要这类东西的，可以用正则处理掉。处理掉之后单看每行，可以方便的使用python的re.findall提取出来一个字和标签的元组组成的列表。再将二者分别处理，分别得到句子和标签。<br>&emsp;&emsp;通过标签得到独热编码的过程值得记录一下。面对的问题就是给出了一个类别的列表，如何得到独热编码。可以利用numpy的花式索引方便进行。代码如下：（虽然花式索引返回的是数据的拷贝，但是使用花式索引进行赋值却是在原数组上进行操作的）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">data = np.array(data)	<span class="comment"># 获得numpy的数组</span></div><div class="line">one_hot = np.zeros((len(data), <span class="number">5</span>), dtype=np.float64)	<span class="comment"># 获得len(data)个长度为5的0向量</span></div><div class="line">one_hot[range(len(data)), data] = <span class="number">1</span>		<span class="comment"># 二维花式索引 可以实现任意位置的操作</span></div><div class="line">default = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>], dtype=np.float64)	<span class="comment"># 定义默认向量</span></div><div class="line">default = np.tile(default, (<span class="number">32</span> - len(one_hot), <span class="number">1</span>))	<span class="comment"># 将默认向量在列方向上重复32-len(one_hot)次</span></div><div class="line">one_hot = np.concatenate((one_hot, default), axis = <span class="number">0</span>)	<span class="comment"># 将默认部分与有效部分拼接保证长度与句向量相同</span></div></pre></td></tr></table></figure></p>
<h2 id="模型定义及训练"><a href="#模型定义及训练" class="headerlink" title="模型定义及训练"></a>模型定义及训练</h2><p>&emsp;&emsp;接下类就是定义一个双向LSTM的过程，为了简单起见，使用了keras进行，keras中还有embedding层，正好适用于我们生成的句向量。keras封装度比较高，所以代码较短，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">inputs = Input(shape=(sent_len,), dtype=<span class="string">'int32'</span>)</div><div class="line">embedding = Embedding(len(charsets) + <span class="number">1</span>, word_size, input_length=sent_len, mask_zero=<span class="keyword">True</span>)(inputs)</div><div class="line">bilstm_layer = Bidirectional(LSTM(<span class="number">64</span>, return_sequences=<span class="keyword">True</span>), merge_mode=<span class="string">'sum'</span>)(embedding)</div><div class="line">output = TimeDistributed(Dense(<span class="number">5</span>, activation=<span class="string">'softmax'</span>))(bilstm_layer)</div><div class="line">model = Model(input=inputs, output=output) model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">json.dump(model.to_json(), open(os.path.join(filedir, model_name), <span class="string">"w"</span>))</div><div class="line">model.fit(x, y, batch_size=batch_size, epochs=epochs)</div><div class="line">model.save_weights(os.path.join(filedir, model_weights_name))</div></pre></td></tr></table></figure>
<p>&emsp;&emsp;训练好了模型，输入一句话，就能给出各个位置对应各种标签的概率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sentence_embeddings, all_sentences, sentence_len = data.get_sent_embeddings(charsets, test_sentence)</div><div class="line">result = model.predict(sentence_embeddings, verbose=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<h2 id="计算概率最大路径"><a href="#计算概率最大路径" class="headerlink" title="计算概率最大路径"></a>计算概率最大路径</h2><p>&emsp;&emsp;根据之前对$\delta_i(s)$的定义，使用dp就可以计算概率最大的路径了。此处转移概率用的是相等概率，dp算法的具体实现上利用了python字典的性质，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(nodes)</span>:</span></div><div class="line">    path = &#123;<span class="string">'b'</span>: nodes[<span class="number">0</span>][<span class="string">'b'</span>], <span class="string">'s'</span>: nodes[<span class="number">0</span>][<span class="string">'s'</span>]&#125;</div><div class="line">    <span class="keyword">for</span> layer_num <span class="keyword">in</span> range(<span class="number">1</span>, len(nodes)):</div><div class="line">        old_path = path.copy()</div><div class="line">        path = &#123;&#125;</div><div class="line">        <span class="keyword">for</span> new_tag <span class="keyword">in</span> nodes[layer_num].keys():</div><div class="line">            tmp = &#123;&#125;</div><div class="line">            <span class="keyword">if</span> layer_num == len(nodes) - <span class="number">1</span>:</div><div class="line">                <span class="keyword">if</span> new_tag <span class="keyword">in</span> [<span class="string">"m"</span>, <span class="string">"b"</span>]:</div><div class="line">                    <span class="keyword">continue</span></div><div class="line">            <span class="keyword">for</span> old_path_tag <span class="keyword">in</span> old_path.keys():</div><div class="line">                <span class="keyword">if</span> old_path_tag[<span class="number">-1</span>]+new_tag <span class="keyword">in</span> transpose_matrix.keys():</div><div class="line">                    tmp[old_path_tag+new_tag] = old_path[old_path_tag] + \</div><div class="line">                                                nodes[layer_num][new_tag] + \</div><div class="line">                                                transpose_matrix[old_path_tag[<span class="number">-1</span>]+new_tag]</div><div class="line">            k = np.argmax(list(tmp.values()))</div><div class="line">            path[list(tmp.keys())[k]] = list(tmp.values())[k]</div><div class="line">    <span class="keyword">return</span> list(path.keys())[np.argmax(list(path.values()))]</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过以上几个步骤，就完成了对一句话4-tag的标注。自己试了几句话，效果还不错，见图：</p>
<p><img src="https://i.loli.net/2019/01/13/5c3af89cb53f3.png" alt="result1.png"></p>
<p><img src="https://i.loli.net/2019/01/13/5c3af89cb71fb.png" alt="result2.png"></p>
<p>完整代码可以参照我的<a href="https://github.com/mingming97/Chinese-Word-Split" target="_blank" rel="external">Github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本思路&quot;&gt;&lt;a href=&quot;#基本思路&quot; class=&quot;headerlink&quot; title=&quot;基本思路&quot;&gt;&lt;/a&gt;基本思路&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;中文分词其实有很多种思路，大多都是建立在HMM模型的基础上。先简要介绍一下HMM模型，HMM模型中有
    
    </summary>
    
      <category term="NLP" scheme="https://mingming97.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://mingming97.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>hexo配置LaTeX公式</title>
    <link href="https://mingming97.github.io/2018/05/15/hexo-latex/"/>
    <id>https://mingming97.github.io/2018/05/15/hexo-latex/</id>
    <published>2018-05-15T13:12:18.000Z</published>
    <updated>2018-05-18T10:11:23.240Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎 <code>hexo-renderer-marked</code>更换为<code>hexo-renderer-kramed</code>来渲染markdown。<br>&emsp;&emsp;首先要将之前的<code>hexo-renderer-marked</code>卸载，并安装<code>hexo-renderer-kramed</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm uninstall hexo-renderer-marked --save</div><div class="line">npm install hexo-renderer-kramed --save</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;在这之后建议在hexo的根目录下找到<code>package.json</code>文件，用文本编辑器打开它，删除字符串<code>hexo-renderer-marked</code>所在的一行并保存。之所以不直接卸载<code>hexo-renderer-marked</code>，是因为其他重要包极有可能在卸载该包的同时被删除。<br>&emsp;&emsp;不要忘了行内公式的转义字符，打开<code>./node_modules/kramed/lib/rules</code>，并修改<code>inline.js</code>文件的11和20行，分别修改为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</div><div class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;每次在写文章前，要在YAML font-matter中添加<code>mathjax: true</code>，这样便能确保启动mathjax引擎进行渲染了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;正常的hexo框架在默认情况下渲染数学公式会有很多问题，可以通过将hexo默认的引擎 &lt;code&gt;hexo-renderer-marked&lt;/code&gt;更换为&lt;code&gt;hexo-renderer-kramed&lt;/code&gt;来渲染markdown。&lt;
    
    </summary>
    
      <category term="hexo" scheme="https://mingming97.github.io/categories/hexo/"/>
    
    
      <category term="LaTeX" scheme="https://mingming97.github.io/tags/LaTeX/"/>
    
      <category term="hexo" scheme="https://mingming97.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>LIS的O(nlogn)实现原理</title>
    <link href="https://mingming97.github.io/2017/08/30/LIS-O-nlogn/"/>
    <id>https://mingming97.github.io/2017/08/30/LIS-O-nlogn/</id>
    <published>2017-08-30T08:07:39.000Z</published>
    <updated>2018-05-17T16:08:41.703Z</updated>
    
    <content type="html"><![CDATA[<p>　　朴素的LIS的<strong>O(n²)</strong>算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为<strong>d[i] = max{d[j] | j &lt; i &amp;&amp; a[j] &lt; a[i]} + 1</strong>。显然，对于一个i下的两个不同决策<strong>j，k (j,k &lt; i)，若a[j] &lt; a[k]，d[j] &gt;= d[k]，而a[i] &gt; a[k]时，k显然没有j决策更优</strong>。通过这种思想，我们发现决策有一定的单调性，从而其中可以用二分查找来降低时间复杂度。</p>
<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>　　我们可以维护一个决策序列B，用<strong>B[i]</strong>表示子序列长度为i时的最小末尾(a中的值)。可以看出如果<strong>i &gt; j</strong>，一定有<strong>B[i] &gt;= B[j]</strong>。原因很简单，看B的定义就可以知道。由上可知B是单调的序列，对于其中的的值可以进行二分查找。</p>
<p>　　所以可以按顺序枚举<strong>a[i]</strong>，如果<strong>a[i]</strong>的值比B中的最大值要大，则将<strong>a[i]</strong>放入<strong>B[i]</strong>的尾部，最大长度加1; 否则对B进行二分查找以找到<strong>a[i]</strong>该放入的位置，<strong>pos = min{j | B[j] &gt; a[i]}</strong>，用a[i]代替B[pos]，使a[i]成为子序列长度为pos时的最小值。大致过程如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (a[i] &gt; B[len<span class="number">-1</span>]) &#123;</div><div class="line">        B[len] = a[i];</div><div class="line">        len++;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">        pos = binarySearch(len, a[i]);</div><div class="line">        B[pos] = a[i];</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>完整代码请戳：<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/LIS-nlogn.cpp" target="_blank" rel="external">ljm’s github</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　朴素的LIS的&lt;strong&gt;O(n²)&lt;/strong&gt;算法是用dp，用d[i]表示a[i]中以i为结尾的LIS的值，那么状态转移方程可表示为&lt;strong&gt;d[i] = max{d[j] | j &amp;lt; i &amp;amp;&amp;amp; a[j] &amp;lt; a[i]} +
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="dp" scheme="https://mingming97.github.io/tags/dp/"/>
    
  </entry>
  
  <entry>
    <title>并查集</title>
    <link href="https://mingming97.github.io/2017/08/28/Union-Find/"/>
    <id>https://mingming97.github.io/2017/08/28/Union-Find/</id>
    <published>2017-08-28T09:41:39.000Z</published>
    <updated>2018-05-17T16:08:39.771Z</updated>
    
    <content type="html"><![CDATA[<p>　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最快呢。并查集是一种轻量级的数据结构，其本质是一种集合，支持不相交集合的“并”和“查”的操作。由于在特定状况下，可以通过某些特殊处理将这两种操作的时间复杂度降至很低，所以在某些算法，例如tarjan求lca的实现中，也引入了这一数据结构。</p>
<h2 id="基本实现"><a href="#基本实现" class="headerlink" title="基本实现"></a>基本实现</h2><p>　　其实并查集的本质就是一个划分，商集中的每一个元素（集合）内的元素都可以建一棵树，放在一起就变成了森林。对于任意两个个元素，只需要看他们的树根是否相等，就可以判断他们是否属于同一集合；至于两个集合的合并，只需要将一棵树的树根指向另一棵树的树根。那么，可以用<strong>pre[i]</strong>记录i这个结点的父节点，初始化时只需令<strong>pre[i] = i</strong>，意味着每一个结点都属于一个单独的集合，也就是每一个节点都是树根。如果想查找树根怎么办呢？ 按照定义<strong>pre[i]</strong>是i的父亲，<strong>pre[pre[i]]</strong>就是i结点父亲的父亲，按照这个方式不断向上找直至<strong>pre[i]</strong>等于i，就找到了树根，可以使用递归来实现。如果想合并两个集合怎么办呢，只需要找到两个元素所在树的树根，将一个树根指向另一个，就完成了。</p>
<h2 id="时间复杂度优化"><a href="#时间复杂度优化" class="headerlink" title="时间复杂度优化"></a>时间复杂度优化</h2><h3 id="启发式合并"><a href="#启发式合并" class="headerlink" title="启发式合并"></a>启发式合并</h3><p>　　为了解决合并时树退化成链的情况，在合并时我们可以根据两棵树的深度合并，将最大深度小的向最大深度大的合并。如果两棵树的深度一样，则随便选一个作为根，并将根的最大深度+1。这样做的话在n次操作后，任何一棵树（一个集合）的深度最大不会超过<strong>[logn]+1</strong>，从而使查找的时间复杂度降为$O(logn)$。代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> rx = find(x), ry = find(y);</div><div class="line">    <span class="keyword">if</span> (rx != ry) &#123;</div><div class="line">        <span class="keyword">if</span> (rank[rx] == rank[ry]) &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">            rank[rx]++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (rank[rx] &lt; rank[ry]) &#123;</div><div class="line">            pre[rx] = ry;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">            pre[ry] = rx;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="路径压缩"><a href="#路径压缩" class="headerlink" title="路径压缩"></a>路径压缩</h3><p>　　一般情况下我们只需要知道知道一个元素所在的树的根就可以，所以可以在查找元素的过程中，把路径上的所有子节点直接指向根节点，这样就可以将查找的复杂度降至O(1)。代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> x == pre[x] ? x : pre[x] = find(pre[x]);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="应用时的问题"><a href="#应用时的问题" class="headerlink" title="应用时的问题"></a>应用时的问题</h2><p>　　在具体问题中，两棵树是否可以按秩合并，以及是否可以路径压缩是需要按照题的要求来定的，本文只是提出了大体框架及基本实现（可移步<a href="https://github.com/mingming97/Algorithms/blob/master/c%2B%2B/union-find.cpp" target="_blank" rel="external">我的github</a>查看）。具体节点信息的维护或是节点合并顺序等还需要自己判断。
　  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　首先假设一个情景，一个班级上有很多学习小组，假如Alice，Amy等以A为首字母的属于一个小组，Bob，Ben等以B为首字母的属于一个小组，以此类推。 我们如果想快速查询任意两个人是否属于一个集合（不以首字母为依据），或者将两个学习小组合并为一个，用何种数据结构去组织最
    
    </summary>
    
      <category term="算法" scheme="https://mingming97.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="并查集" scheme="https://mingming97.github.io/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/"/>
    
      <category term="数据结构" scheme="https://mingming97.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
